// @bun
var __create = Object.create;
var __defProp = Object.defineProperty;
var __getProtoOf = Object.getPrototypeOf;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __toESM = (mod, isNodeMode, target) => {
  target = mod != null ? __create(__getProtoOf(mod)) : {};
  const to = isNodeMode || !mod || !mod.__esModule ? __defProp(target, "default", { value: mod, enumerable: true }) : target;
  for (let key of __getOwnPropNames(mod))
    if (!__hasOwnProp.call(to, key))
      __defProp(to, key, {
        get: () => mod[key],
        enumerable: true
      });
  return to;
};
var __commonJS = (cb, mod) => () => (mod || cb((mod = { exports: {} }).exports, mod), mod.exports);

// node_modules/pino-std-serializers/lib/err-helpers.js
var require_err_helpers = __commonJS((exports, module) => {
  var isErrorLike = (err) => {
    return err && typeof err.message === "string";
  };
  var getErrorCause = (err) => {
    if (!err)
      return;
    const cause = err.cause;
    if (typeof cause === "function") {
      const causeResult = err.cause();
      return isErrorLike(causeResult) ? causeResult : undefined;
    } else {
      return isErrorLike(cause) ? cause : undefined;
    }
  };
  var _stackWithCauses = (err, seen) => {
    if (!isErrorLike(err))
      return "";
    const stack = err.stack || "";
    if (seen.has(err)) {
      return stack + "\ncauses have become circular...";
    }
    const cause = getErrorCause(err);
    if (cause) {
      seen.add(err);
      return stack + "\ncaused by: " + _stackWithCauses(cause, seen);
    } else {
      return stack;
    }
  };
  var stackWithCauses = (err) => _stackWithCauses(err, new Set);
  var _messageWithCauses = (err, seen, skip) => {
    if (!isErrorLike(err))
      return "";
    const message = skip ? "" : err.message || "";
    if (seen.has(err)) {
      return message + ": ...";
    }
    const cause = getErrorCause(err);
    if (cause) {
      seen.add(err);
      const skipIfVErrorStyleCause = typeof err.cause === "function";
      return message + (skipIfVErrorStyleCause ? "" : ": ") + _messageWithCauses(cause, seen, skipIfVErrorStyleCause);
    } else {
      return message;
    }
  };
  var messageWithCauses = (err) => _messageWithCauses(err, new Set);
  module.exports = {
    isErrorLike,
    getErrorCause,
    stackWithCauses,
    messageWithCauses
  };
});

// node_modules/pino-std-serializers/lib/err-proto.js
var require_err_proto = __commonJS((exports, module) => {
  var seen = Symbol("circular-ref-tag");
  var rawSymbol = Symbol("pino-raw-err-ref");
  var pinoErrProto = Object.create({}, {
    type: {
      enumerable: true,
      writable: true,
      value: undefined
    },
    message: {
      enumerable: true,
      writable: true,
      value: undefined
    },
    stack: {
      enumerable: true,
      writable: true,
      value: undefined
    },
    aggregateErrors: {
      enumerable: true,
      writable: true,
      value: undefined
    },
    raw: {
      enumerable: false,
      get: function() {
        return this[rawSymbol];
      },
      set: function(val) {
        this[rawSymbol] = val;
      }
    }
  });
  Object.defineProperty(pinoErrProto, rawSymbol, {
    writable: true,
    value: {}
  });
  module.exports = {
    pinoErrProto,
    pinoErrorSymbols: {
      seen,
      rawSymbol
    }
  };
});

// node_modules/pino-std-serializers/lib/err.js
var require_err = __commonJS((exports, module) => {
  var errSerializer = function(err) {
    if (!isErrorLike(err)) {
      return err;
    }
    err[seen] = undefined;
    const _err = Object.create(pinoErrProto);
    _err.type = toString.call(err.constructor) === "[object Function]" ? err.constructor.name : err.name;
    _err.message = messageWithCauses(err);
    _err.stack = stackWithCauses(err);
    if (Array.isArray(err.errors)) {
      _err.aggregateErrors = err.errors.map((err2) => errSerializer(err2));
    }
    for (const key in err) {
      if (_err[key] === undefined) {
        const val = err[key];
        if (isErrorLike(val)) {
          if (key !== "cause" && !Object.prototype.hasOwnProperty.call(val, seen)) {
            _err[key] = errSerializer(val);
          }
        } else {
          _err[key] = val;
        }
      }
    }
    delete err[seen];
    _err.raw = err;
    return _err;
  };
  module.exports = errSerializer;
  var { messageWithCauses, stackWithCauses, isErrorLike } = require_err_helpers();
  var { pinoErrProto, pinoErrorSymbols } = require_err_proto();
  var { seen } = pinoErrorSymbols;
  var { toString } = Object.prototype;
});

// node_modules/pino-std-serializers/lib/err-with-cause.js
var require_err_with_cause = __commonJS((exports, module) => {
  var errWithCauseSerializer = function(err) {
    if (!isErrorLike(err)) {
      return err;
    }
    err[seen] = undefined;
    const _err = Object.create(pinoErrProto);
    _err.type = toString.call(err.constructor) === "[object Function]" ? err.constructor.name : err.name;
    _err.message = err.message;
    _err.stack = err.stack;
    if (Array.isArray(err.errors)) {
      _err.aggregateErrors = err.errors.map((err2) => errWithCauseSerializer(err2));
    }
    if (isErrorLike(err.cause) && !Object.prototype.hasOwnProperty.call(err.cause, seen)) {
      _err.cause = errWithCauseSerializer(err.cause);
    }
    for (const key in err) {
      if (_err[key] === undefined) {
        const val = err[key];
        if (isErrorLike(val)) {
          if (!Object.prototype.hasOwnProperty.call(val, seen)) {
            _err[key] = errWithCauseSerializer(val);
          }
        } else {
          _err[key] = val;
        }
      }
    }
    delete err[seen];
    _err.raw = err;
    return _err;
  };
  module.exports = errWithCauseSerializer;
  var { isErrorLike } = require_err_helpers();
  var { pinoErrProto, pinoErrorSymbols } = require_err_proto();
  var { seen } = pinoErrorSymbols;
  var { toString } = Object.prototype;
});

// node_modules/pino-std-serializers/lib/req.js
var require_req = __commonJS((exports, module) => {
  var reqSerializer = function(req) {
    const connection = req.info || req.socket;
    const _req = Object.create(pinoReqProto);
    _req.id = typeof req.id === "function" ? req.id() : req.id || (req.info ? req.info.id : undefined);
    _req.method = req.method;
    if (req.originalUrl) {
      _req.url = req.originalUrl;
    } else {
      const path = req.path;
      _req.url = typeof path === "string" ? path : req.url ? req.url.path || req.url : undefined;
    }
    if (req.query) {
      _req.query = req.query;
    }
    if (req.params) {
      _req.params = req.params;
    }
    _req.headers = req.headers;
    _req.remoteAddress = connection && connection.remoteAddress;
    _req.remotePort = connection && connection.remotePort;
    _req.raw = req.raw || req;
    return _req;
  };
  var mapHttpRequest = function(req) {
    return {
      req: reqSerializer(req)
    };
  };
  module.exports = {
    mapHttpRequest,
    reqSerializer
  };
  var rawSymbol = Symbol("pino-raw-req-ref");
  var pinoReqProto = Object.create({}, {
    id: {
      enumerable: true,
      writable: true,
      value: ""
    },
    method: {
      enumerable: true,
      writable: true,
      value: ""
    },
    url: {
      enumerable: true,
      writable: true,
      value: ""
    },
    query: {
      enumerable: true,
      writable: true,
      value: ""
    },
    params: {
      enumerable: true,
      writable: true,
      value: ""
    },
    headers: {
      enumerable: true,
      writable: true,
      value: {}
    },
    remoteAddress: {
      enumerable: true,
      writable: true,
      value: ""
    },
    remotePort: {
      enumerable: true,
      writable: true,
      value: ""
    },
    raw: {
      enumerable: false,
      get: function() {
        return this[rawSymbol];
      },
      set: function(val) {
        this[rawSymbol] = val;
      }
    }
  });
  Object.defineProperty(pinoReqProto, rawSymbol, {
    writable: true,
    value: {}
  });
});

// node_modules/pino-std-serializers/lib/res.js
var require_res = __commonJS((exports, module) => {
  var resSerializer = function(res) {
    const _res2 = Object.create(pinoResProto);
    _res2.statusCode = res.headersSent ? res.statusCode : null;
    _res2.headers = res.getHeaders ? res.getHeaders() : res._headers;
    _res2.raw = res;
    return _res2;
  };
  var mapHttpResponse = function(res) {
    return {
      res: resSerializer(res)
    };
  };
  module.exports = {
    mapHttpResponse,
    resSerializer
  };
  var rawSymbol = Symbol("pino-raw-res-ref");
  var pinoResProto = Object.create({}, {
    statusCode: {
      enumerable: true,
      writable: true,
      value: 0
    },
    headers: {
      enumerable: true,
      writable: true,
      value: ""
    },
    raw: {
      enumerable: false,
      get: function() {
        return this[rawSymbol];
      },
      set: function(val) {
        this[rawSymbol] = val;
      }
    }
  });
  Object.defineProperty(pinoResProto, rawSymbol, {
    writable: true,
    value: {}
  });
});

// node_modules/pino-std-serializers/index.js
var require_pino_std_serializers = __commonJS((exports, module) => {
  var errSerializer = require_err();
  var errWithCauseSerializer = require_err_with_cause();
  var reqSerializers = require_req();
  var resSerializers = require_res();
  module.exports = {
    err: errSerializer,
    errWithCause: errWithCauseSerializer,
    mapHttpRequest: reqSerializers.mapHttpRequest,
    mapHttpResponse: resSerializers.mapHttpResponse,
    req: reqSerializers.reqSerializer,
    res: resSerializers.resSerializer,
    wrapErrorSerializer: function wrapErrorSerializer(customSerializer) {
      if (customSerializer === errSerializer)
        return customSerializer;
      return function wrapErrSerializer(err) {
        return customSerializer(errSerializer(err));
      };
    },
    wrapRequestSerializer: function wrapRequestSerializer(customSerializer) {
      if (customSerializer === reqSerializers.reqSerializer)
        return customSerializer;
      return function wrappedReqSerializer(req) {
        return customSerializer(reqSerializers.reqSerializer(req));
      };
    },
    wrapResponseSerializer: function wrapResponseSerializer(customSerializer) {
      if (customSerializer === resSerializers.resSerializer)
        return customSerializer;
      return function wrappedResSerializer(res) {
        return customSerializer(resSerializers.resSerializer(res));
      };
    }
  };
});

// node_modules/pino/lib/caller.js
var require_caller = __commonJS((exports, module) => {
  var noOpPrepareStackTrace = function(_, stack) {
    return stack;
  };
  module.exports = function getCallers() {
    const originalPrepare = Error.prepareStackTrace;
    Error.prepareStackTrace = noOpPrepareStackTrace;
    const stack = new Error().stack;
    Error.prepareStackTrace = originalPrepare;
    if (!Array.isArray(stack)) {
      return;
    }
    const entries = stack.slice(2);
    const fileNames = [];
    for (const entry of entries) {
      if (!entry) {
        continue;
      }
      fileNames.push(entry.getFileName());
    }
    return fileNames;
  };
});

// node_modules/fast-redact/lib/validator.js
var require_validator = __commonJS((exports, module) => {
  var validator = function(opts = {}) {
    const {
      ERR_PATHS_MUST_BE_STRINGS = () => "fast-redact - Paths must be (non-empty) strings",
      ERR_INVALID_PATH = (s) => `fast-redact \u2013 Invalid path (${s})`
    } = opts;
    return function validate({ paths }) {
      paths.forEach((s) => {
        if (typeof s !== "string") {
          throw Error(ERR_PATHS_MUST_BE_STRINGS());
        }
        try {
          if (/\u3007/.test(s))
            throw Error();
          const expr = (s[0] === "[" ? "" : ".") + s.replace(/^\*/, "\u3007").replace(/\.\*/g, ".\u3007").replace(/\[\*\]/g, "[\u3007]");
          if (/\n|\r|;/.test(expr))
            throw Error();
          if (/\/\*/.test(expr))
            throw Error();
          Function(`
            'use strict'
            const o = new Proxy({}, { get: () => o, set: () => { throw Error() } });
            const \u3007 = null;
            o${expr}
            if ([o${expr}].length !== 1) throw Error()`)();
        } catch (e) {
          throw Error(ERR_INVALID_PATH(s));
        }
      });
    };
  };
  module.exports = validator;
});

// node_modules/fast-redact/lib/rx.js
var require_rx = __commonJS((exports, module) => {
  module.exports = /[^.[\]]+|\[((?:.)*?)\]/g;
});

// node_modules/fast-redact/lib/parse.js
var require_parse = __commonJS((exports, module) => {
  var parse = function({ paths }) {
    const wildcards = [];
    var wcLen = 0;
    const secret = paths.reduce(function(o, strPath, ix) {
      var path = strPath.match(rx).map((p) => p.replace(/'|"|`/g, ""));
      const leadingBracket = strPath[0] === "[";
      path = path.map((p) => {
        if (p[0] === "[")
          return p.substr(1, p.length - 2);
        else
          return p;
      });
      const star = path.indexOf("*");
      if (star > -1) {
        const before = path.slice(0, star);
        const beforeStr = before.join(".");
        const after = path.slice(star + 1, path.length);
        const nested = after.length > 0;
        wcLen++;
        wildcards.push({
          before,
          beforeStr,
          after,
          nested
        });
      } else {
        o[strPath] = {
          path,
          val: undefined,
          precensored: false,
          circle: "",
          escPath: JSON.stringify(strPath),
          leadingBracket
        };
      }
      return o;
    }, {});
    return { wildcards, wcLen, secret };
  };
  var rx = require_rx();
  module.exports = parse;
});

// node_modules/fast-redact/lib/redactor.js
var require_redactor = __commonJS((exports, module) => {
  var redactor = function({ secret, serialize, wcLen, strict, isCensorFct, censorFctTakesPath }, state) {
    const redact = Function("o", `
    if (typeof o !== 'object' || o == null) {
      ${strictImpl(strict, serialize)}
    }
    const { censor, secret } = this
    const originalSecret = {}
    const secretKeys = Object.keys(secret)
    for (var i = 0; i < secretKeys.length; i++) {
      originalSecret[secretKeys[i]] = secret[secretKeys[i]]
    }

    ${redactTmpl(secret, isCensorFct, censorFctTakesPath)}
    this.compileRestore()
    ${dynamicRedactTmpl(wcLen > 0, isCensorFct, censorFctTakesPath)}
    this.secret = originalSecret
    ${resultTmpl(serialize)}
  `).bind(state);
    redact.state = state;
    if (serialize === false) {
      redact.restore = (o) => state.restore(o);
    }
    return redact;
  };
  var redactTmpl = function(secret, isCensorFct, censorFctTakesPath) {
    return Object.keys(secret).map((path) => {
      const { escPath, leadingBracket, path: arrPath } = secret[path];
      const skip = leadingBracket ? 1 : 0;
      const delim = leadingBracket ? "" : ".";
      const hops = [];
      var match;
      while ((match = rx.exec(path)) !== null) {
        const [, ix] = match;
        const { index, input } = match;
        if (index > skip)
          hops.push(input.substring(0, index - (ix ? 0 : 1)));
      }
      var existence = hops.map((p) => `o${delim}${p}`).join(" && ");
      if (existence.length === 0)
        existence += `o${delim}${path} != null`;
      else
        existence += ` && o${delim}${path} != null`;
      const circularDetection = `
      switch (true) {
        ${hops.reverse().map((p) => `
          case o${delim}${p} === censor:
            secret[${escPath}].circle = ${JSON.stringify(p)}
            break
        `).join("\n")}
      }
    `;
      const censorArgs = censorFctTakesPath ? `val, ${JSON.stringify(arrPath)}` : `val`;
      return `
      if (${existence}) {
        const val = o${delim}${path}
        if (val === censor) {
          secret[${escPath}].precensored = true
        } else {
          secret[${escPath}].val = val
          o${delim}${path} = ${isCensorFct ? `censor(${censorArgs})` : "censor"}
          ${circularDetection}
        }
      }
    `;
    }).join("\n");
  };
  var dynamicRedactTmpl = function(hasWildcards, isCensorFct, censorFctTakesPath) {
    return hasWildcards === true ? `
    {
      const { wildcards, wcLen, groupRedact, nestedRedact } = this
      for (var i = 0; i < wcLen; i++) {
        const { before, beforeStr, after, nested } = wildcards[i]
        if (nested === true) {
          secret[beforeStr] = secret[beforeStr] || []
          nestedRedact(secret[beforeStr], o, before, after, censor, ${isCensorFct}, ${censorFctTakesPath})
        } else secret[beforeStr] = groupRedact(o, before, censor, ${isCensorFct}, ${censorFctTakesPath})
      }
    }
  ` : "";
  };
  var resultTmpl = function(serialize) {
    return serialize === false ? `return o` : `
    var s = this.serialize(o)
    this.restore(o)
    return s
  `;
  };
  var strictImpl = function(strict, serialize) {
    return strict === true ? `throw Error('fast-redact: primitives cannot be redacted')` : serialize === false ? `return o` : `return this.serialize(o)`;
  };
  var rx = require_rx();
  module.exports = redactor;
});

// node_modules/fast-redact/lib/modifiers.js
var require_modifiers = __commonJS((exports, module) => {
  var groupRestore = function({ keys, values, target }) {
    if (target == null || typeof target === "string")
      return;
    const length = keys.length;
    for (var i = 0;i < length; i++) {
      const k = keys[i];
      target[k] = values[i];
    }
  };
  var groupRedact = function(o, path, censor, isCensorFct, censorFctTakesPath) {
    const target = get(o, path);
    if (target == null || typeof target === "string")
      return { keys: null, values: null, target, flat: true };
    const keys = Object.keys(target);
    const keysLength = keys.length;
    const pathLength = path.length;
    const pathWithKey = censorFctTakesPath ? [...path] : undefined;
    const values = new Array(keysLength);
    for (var i = 0;i < keysLength; i++) {
      const key = keys[i];
      values[i] = target[key];
      if (censorFctTakesPath) {
        pathWithKey[pathLength] = key;
        target[key] = censor(target[key], pathWithKey);
      } else if (isCensorFct) {
        target[key] = censor(target[key]);
      } else {
        target[key] = censor;
      }
    }
    return { keys, values, target, flat: true };
  };
  var nestedRestore = function(instructions) {
    for (let i = 0;i < instructions.length; i++) {
      const { target, path, value } = instructions[i];
      let current = target;
      for (let i2 = path.length - 1;i2 > 0; i2--) {
        current = current[path[i2]];
      }
      current[path[0]] = value;
    }
  };
  var nestedRedact = function(store, o, path, ns, censor, isCensorFct, censorFctTakesPath) {
    const target = get(o, path);
    if (target == null)
      return;
    const keys = Object.keys(target);
    const keysLength = keys.length;
    for (var i = 0;i < keysLength; i++) {
      const key = keys[i];
      specialSet(store, target, key, path, ns, censor, isCensorFct, censorFctTakesPath);
    }
    return store;
  };
  var has = function(obj, prop) {
    return obj !== undefined && obj !== null ? "hasOwn" in Object ? Object.hasOwn(obj, prop) : Object.prototype.hasOwnProperty.call(obj, prop) : false;
  };
  var specialSet = function(store, o, k, path, afterPath, censor, isCensorFct, censorFctTakesPath) {
    const afterPathLen = afterPath.length;
    const lastPathIndex = afterPathLen - 1;
    const originalKey = k;
    var i = -1;
    var n;
    var nv;
    var ov;
    var oov = null;
    var wc = null;
    var kIsWc;
    var wcov;
    var consecutive = false;
    var level = 0;
    var depth = 0;
    var redactPathCurrent = tree();
    ov = n = o[k];
    if (typeof n !== "object")
      return;
    while (n != null && ++i < afterPathLen) {
      depth += 1;
      k = afterPath[i];
      oov = ov;
      if (k !== "*" && !wc && !(typeof n === "object" && (k in n))) {
        break;
      }
      if (k === "*") {
        if (wc === "*") {
          consecutive = true;
        }
        wc = k;
        if (i !== lastPathIndex) {
          continue;
        }
      }
      if (wc) {
        const wcKeys = Object.keys(n);
        for (var j = 0;j < wcKeys.length; j++) {
          const wck = wcKeys[j];
          wcov = n[wck];
          kIsWc = k === "*";
          if (consecutive) {
            redactPathCurrent = node4(redactPathCurrent, wck, depth);
            level = i;
            ov = iterateNthLevel(wcov, level - 1, k, path, afterPath, censor, isCensorFct, censorFctTakesPath, originalKey, n, nv, ov, kIsWc, wck, i, lastPathIndex, redactPathCurrent, store, o[originalKey], depth + 1);
          } else {
            if (kIsWc || typeof wcov === "object" && wcov !== null && k in wcov) {
              if (kIsWc) {
                ov = wcov;
              } else {
                ov = wcov[k];
              }
              nv = i !== lastPathIndex ? ov : isCensorFct ? censorFctTakesPath ? censor(ov, [...path, originalKey, ...afterPath]) : censor(ov) : censor;
              if (kIsWc) {
                const rv = restoreInstr(node4(redactPathCurrent, wck, depth), ov, o[originalKey]);
                store.push(rv);
                n[wck] = nv;
              } else {
                if (wcov[k] === nv) {
                } else if (nv === undefined && censor !== undefined || has(wcov, k) && nv === ov) {
                  redactPathCurrent = node4(redactPathCurrent, wck, depth);
                } else {
                  redactPathCurrent = node4(redactPathCurrent, wck, depth);
                  const rv = restoreInstr(node4(redactPathCurrent, k, depth + 1), ov, o[originalKey]);
                  store.push(rv);
                  wcov[k] = nv;
                }
              }
            }
          }
        }
        wc = null;
      } else {
        ov = n[k];
        redactPathCurrent = node4(redactPathCurrent, k, depth);
        nv = i !== lastPathIndex ? ov : isCensorFct ? censorFctTakesPath ? censor(ov, [...path, originalKey, ...afterPath]) : censor(ov) : censor;
        if (has(n, k) && nv === ov || nv === undefined && censor !== undefined) {
        } else {
          const rv = restoreInstr(redactPathCurrent, ov, o[originalKey]);
          store.push(rv);
          n[k] = nv;
        }
        n = n[k];
      }
      if (typeof n !== "object")
        break;
      if (ov === oov || typeof ov === "undefined") {
      }
    }
  };
  var get = function(o, p) {
    var i = -1;
    var l = p.length;
    var n = o;
    while (n != null && ++i < l) {
      n = n[p[i]];
    }
    return n;
  };
  var iterateNthLevel = function(wcov, level, k, path, afterPath, censor, isCensorFct, censorFctTakesPath, originalKey, n, nv, ov, kIsWc, wck, i, lastPathIndex, redactPathCurrent, store, parent, depth) {
    if (level === 0) {
      if (kIsWc || typeof wcov === "object" && wcov !== null && k in wcov) {
        if (kIsWc) {
          ov = wcov;
        } else {
          ov = wcov[k];
        }
        nv = i !== lastPathIndex ? ov : isCensorFct ? censorFctTakesPath ? censor(ov, [...path, originalKey, ...afterPath]) : censor(ov) : censor;
        if (kIsWc) {
          const rv = restoreInstr(redactPathCurrent, ov, parent);
          store.push(rv);
          n[wck] = nv;
        } else {
          if (wcov[k] === nv) {
          } else if (nv === undefined && censor !== undefined || has(wcov, k) && nv === ov) {
          } else {
            const rv = restoreInstr(node4(redactPathCurrent, k, depth + 1), ov, parent);
            store.push(rv);
            wcov[k] = nv;
          }
        }
      }
    }
    for (const key in wcov) {
      if (typeof wcov[key] === "object") {
        redactPathCurrent = node4(redactPathCurrent, key, depth);
        iterateNthLevel(wcov[key], level - 1, k, path, afterPath, censor, isCensorFct, censorFctTakesPath, originalKey, n, nv, ov, kIsWc, wck, i, lastPathIndex, redactPathCurrent, store, parent, depth + 1);
      }
    }
  };
  var tree = function() {
    return { parent: null, key: null, children: [], depth: 0 };
  };
  var node4 = function(parent, key, depth) {
    if (parent.depth === depth) {
      return node4(parent.parent, key, depth);
    }
    var child = {
      parent,
      key,
      depth,
      children: []
    };
    parent.children.push(child);
    return child;
  };
  var restoreInstr = function(node5, value, target) {
    let current = node5;
    const path = [];
    do {
      path.push(current.key);
      current = current.parent;
    } while (current.parent != null);
    return { path, value, target };
  };
  module.exports = {
    groupRedact,
    groupRestore,
    nestedRedact,
    nestedRestore
  };
});

// node_modules/fast-redact/lib/restorer.js
var require_restorer = __commonJS((exports, module) => {
  var restorer = function() {
    return function compileRestore() {
      if (this.restore) {
        this.restore.state.secret = this.secret;
        return;
      }
      const { secret, wcLen } = this;
      const paths = Object.keys(secret);
      const resetters = resetTmpl(secret, paths);
      const hasWildcards = wcLen > 0;
      const state = hasWildcards ? { secret, groupRestore, nestedRestore } : { secret };
      this.restore = Function("o", restoreTmpl(resetters, paths, hasWildcards)).bind(state);
      this.restore.state = state;
    };
  };
  var resetTmpl = function(secret, paths) {
    return paths.map((path) => {
      const { circle, escPath, leadingBracket } = secret[path];
      const delim = leadingBracket ? "" : ".";
      const reset = circle ? `o.${circle} = secret[${escPath}].val` : `o${delim}${path} = secret[${escPath}].val`;
      const clear = `secret[${escPath}].val = undefined`;
      return `
      if (secret[${escPath}].val !== undefined) {
        try { ${reset} } catch (e) {}
        ${clear}
      }
    `;
    }).join("");
  };
  var restoreTmpl = function(resetters, paths, hasWildcards) {
    const dynamicReset = hasWildcards === true ? `
    const keys = Object.keys(secret)
    const len = keys.length
    for (var i = len - 1; i >= ${paths.length}; i--) {
      const k = keys[i]
      const o = secret[k]
      if (o) {
        if (o.flat === true) this.groupRestore(o)
        else this.nestedRestore(o)
        secret[k] = null
      }
    }
  ` : "";
    return `
    const secret = this.secret
    ${dynamicReset}
    ${resetters}
    return o
  `;
  };
  var { groupRestore, nestedRestore } = require_modifiers();
  module.exports = restorer;
});

// node_modules/fast-redact/lib/state.js
var require_state = __commonJS((exports, module) => {
  var state = function(o) {
    const {
      secret,
      censor,
      compileRestore,
      serialize,
      groupRedact,
      nestedRedact,
      wildcards,
      wcLen
    } = o;
    const builder = [{ secret, censor, compileRestore }];
    if (serialize !== false)
      builder.push({ serialize });
    if (wcLen > 0)
      builder.push({ groupRedact, nestedRedact, wildcards, wcLen });
    return Object.assign(...builder);
  };
  module.exports = state;
});

// node_modules/fast-redact/index.js
var require_fast_redact = __commonJS((exports, module) => {
  var fastRedact = function(opts = {}) {
    const paths = Array.from(new Set(opts.paths || []));
    const serialize = "serialize" in opts ? opts.serialize === false ? opts.serialize : typeof opts.serialize === "function" ? opts.serialize : JSON.stringify : JSON.stringify;
    const remove = opts.remove;
    if (remove === true && serialize !== JSON.stringify) {
      throw Error("fast-redact \u2013 remove option may only be set when serializer is JSON.stringify");
    }
    const censor = remove === true ? undefined : ("censor" in opts) ? opts.censor : DEFAULT_CENSOR;
    const isCensorFct = typeof censor === "function";
    const censorFctTakesPath = isCensorFct && censor.length > 1;
    if (paths.length === 0)
      return serialize || noop;
    validate({ paths, serialize, censor });
    const { wildcards, wcLen, secret } = parse({ paths, censor });
    const compileRestore = restorer();
    const strict = "strict" in opts ? opts.strict : true;
    return redactor({ secret, wcLen, serialize, strict, isCensorFct, censorFctTakesPath }, state({
      secret,
      censor,
      compileRestore,
      serialize,
      groupRedact,
      nestedRedact,
      wildcards,
      wcLen
    }));
  };
  var validator = require_validator();
  var parse = require_parse();
  var redactor = require_redactor();
  var restorer = require_restorer();
  var { groupRedact, nestedRedact } = require_modifiers();
  var state = require_state();
  var rx = require_rx();
  var validate = validator();
  var noop = (o) => o;
  noop.restore = noop;
  var DEFAULT_CENSOR = "[REDACTED]";
  fastRedact.rx = rx;
  fastRedact.validator = validator;
  module.exports = fastRedact;
});

// node_modules/pino/lib/symbols.js
var require_symbols = __commonJS((exports, module) => {
  var setLevelSym = Symbol("pino.setLevel");
  var getLevelSym = Symbol("pino.getLevel");
  var levelValSym = Symbol("pino.levelVal");
  var levelCompSym = Symbol("pino.levelComp");
  var useLevelLabelsSym = Symbol("pino.useLevelLabels");
  var useOnlyCustomLevelsSym = Symbol("pino.useOnlyCustomLevels");
  var mixinSym = Symbol("pino.mixin");
  var lsCacheSym = Symbol("pino.lsCache");
  var chindingsSym = Symbol("pino.chindings");
  var asJsonSym = Symbol("pino.asJson");
  var writeSym = Symbol("pino.write");
  var redactFmtSym = Symbol("pino.redactFmt");
  var timeSym = Symbol("pino.time");
  var timeSliceIndexSym = Symbol("pino.timeSliceIndex");
  var streamSym = Symbol("pino.stream");
  var stringifySym = Symbol("pino.stringify");
  var stringifySafeSym = Symbol("pino.stringifySafe");
  var stringifiersSym = Symbol("pino.stringifiers");
  var endSym = Symbol("pino.end");
  var formatOptsSym = Symbol("pino.formatOpts");
  var messageKeySym = Symbol("pino.messageKey");
  var errorKeySym = Symbol("pino.errorKey");
  var nestedKeySym = Symbol("pino.nestedKey");
  var nestedKeyStrSym = Symbol("pino.nestedKeyStr");
  var mixinMergeStrategySym = Symbol("pino.mixinMergeStrategy");
  var msgPrefixSym = Symbol("pino.msgPrefix");
  var wildcardFirstSym = Symbol("pino.wildcardFirst");
  var serializersSym = Symbol.for("pino.serializers");
  var formattersSym = Symbol.for("pino.formatters");
  var hooksSym = Symbol.for("pino.hooks");
  var needsMetadataGsym = Symbol.for("pino.metadata");
  module.exports = {
    setLevelSym,
    getLevelSym,
    levelValSym,
    levelCompSym,
    useLevelLabelsSym,
    mixinSym,
    lsCacheSym,
    chindingsSym,
    asJsonSym,
    writeSym,
    serializersSym,
    redactFmtSym,
    timeSym,
    timeSliceIndexSym,
    streamSym,
    stringifySym,
    stringifySafeSym,
    stringifiersSym,
    endSym,
    formatOptsSym,
    messageKeySym,
    errorKeySym,
    nestedKeySym,
    wildcardFirstSym,
    needsMetadataGsym,
    useOnlyCustomLevelsSym,
    formattersSym,
    hooksSym,
    nestedKeyStrSym,
    mixinMergeStrategySym,
    msgPrefixSym
  };
});

// node_modules/pino/lib/redaction.js
var require_redaction = __commonJS((exports, module) => {
  var redaction = function(opts, serialize) {
    const { paths, censor } = handle(opts);
    const shape = paths.reduce((o, str) => {
      rx.lastIndex = 0;
      const first = rx.exec(str);
      const next = rx.exec(str);
      let ns = first[1] !== undefined ? first[1].replace(/^(?:"|'|`)(.*)(?:"|'|`)$/, "$1") : first[0];
      if (ns === "*") {
        ns = wildcardFirstSym;
      }
      if (next === null) {
        o[ns] = null;
        return o;
      }
      if (o[ns] === null) {
        return o;
      }
      const { index } = next;
      const nextPath = `${str.substr(index, str.length - 1)}`;
      o[ns] = o[ns] || [];
      if (ns !== wildcardFirstSym && o[ns].length === 0) {
        o[ns].push(...o[wildcardFirstSym] || []);
      }
      if (ns === wildcardFirstSym) {
        Object.keys(o).forEach(function(k) {
          if (o[k]) {
            o[k].push(nextPath);
          }
        });
      }
      o[ns].push(nextPath);
      return o;
    }, {});
    const result = {
      [redactFmtSym]: fastRedact({ paths, censor, serialize, strict })
    };
    const topCensor = (...args) => {
      return typeof censor === "function" ? serialize(censor(...args)) : serialize(censor);
    };
    return [...Object.keys(shape), ...Object.getOwnPropertySymbols(shape)].reduce((o, k) => {
      if (shape[k] === null) {
        o[k] = (value) => topCensor(value, [k]);
      } else {
        const wrappedCensor = typeof censor === "function" ? (value, path) => {
          return censor(value, [k, ...path]);
        } : censor;
        o[k] = fastRedact({
          paths: shape[k],
          censor: wrappedCensor,
          serialize,
          strict
        });
      }
      return o;
    }, result);
  };
  var handle = function(opts) {
    if (Array.isArray(opts)) {
      opts = { paths: opts, censor: CENSOR };
      validate(opts);
      return opts;
    }
    let { paths, censor = CENSOR, remove } = opts;
    if (Array.isArray(paths) === false) {
      throw Error("pino \u2013 redact must contain an array of strings");
    }
    if (remove === true)
      censor = undefined;
    validate({ paths, censor });
    return { paths, censor };
  };
  var fastRedact = require_fast_redact();
  var { redactFmtSym, wildcardFirstSym } = require_symbols();
  var { rx, validator } = fastRedact;
  var validate = validator({
    ERR_PATHS_MUST_BE_STRINGS: () => "pino \u2013 redacted paths must be strings",
    ERR_INVALID_PATH: (s) => `pino \u2013 redact paths array contains an invalid path (${s})`
  });
  var CENSOR = "[Redacted]";
  var strict = false;
  module.exports = redaction;
});

// node_modules/pino/lib/time.js
var require_time = __commonJS((exports, module) => {
  var nullTime = () => "";
  var epochTime = () => `,"time":${Date.now()}`;
  var unixTime = () => `,"time":${Math.round(Date.now() / 1000)}`;
  var isoTime = () => `,"time":"${new Date(Date.now()).toISOString()}"`;
  module.exports = { nullTime, epochTime, unixTime, isoTime };
});

// node_modules/quick-format-unescaped/index.js
var require_quick_format_unescaped = __commonJS((exports, module) => {
  var tryStringify = function(o) {
    try {
      return JSON.stringify(o);
    } catch (e) {
      return '"[Circular]"';
    }
  };
  var format = function(f, args, opts) {
    var ss = opts && opts.stringify || tryStringify;
    var offset = 1;
    if (typeof f === "object" && f !== null) {
      var len = args.length + offset;
      if (len === 1)
        return f;
      var objects = new Array(len);
      objects[0] = ss(f);
      for (var index = 1;index < len; index++) {
        objects[index] = ss(args[index]);
      }
      return objects.join(" ");
    }
    if (typeof f !== "string") {
      return f;
    }
    var argLen = args.length;
    if (argLen === 0)
      return f;
    var str = "";
    var a = 1 - offset;
    var lastPos = -1;
    var flen = f && f.length || 0;
    for (var i = 0;i < flen; ) {
      if (f.charCodeAt(i) === 37 && i + 1 < flen) {
        lastPos = lastPos > -1 ? lastPos : 0;
        switch (f.charCodeAt(i + 1)) {
          case 100:
          case 102:
            if (a >= argLen)
              break;
            if (args[a] == null)
              break;
            if (lastPos < i)
              str += f.slice(lastPos, i);
            str += Number(args[a]);
            lastPos = i + 2;
            i++;
            break;
          case 105:
            if (a >= argLen)
              break;
            if (args[a] == null)
              break;
            if (lastPos < i)
              str += f.slice(lastPos, i);
            str += Math.floor(Number(args[a]));
            lastPos = i + 2;
            i++;
            break;
          case 79:
          case 111:
          case 106:
            if (a >= argLen)
              break;
            if (args[a] === undefined)
              break;
            if (lastPos < i)
              str += f.slice(lastPos, i);
            var type = typeof args[a];
            if (type === "string") {
              str += "\'" + args[a] + "\'";
              lastPos = i + 2;
              i++;
              break;
            }
            if (type === "function") {
              str += args[a].name || "<anonymous>";
              lastPos = i + 2;
              i++;
              break;
            }
            str += ss(args[a]);
            lastPos = i + 2;
            i++;
            break;
          case 115:
            if (a >= argLen)
              break;
            if (lastPos < i)
              str += f.slice(lastPos, i);
            str += String(args[a]);
            lastPos = i + 2;
            i++;
            break;
          case 37:
            if (lastPos < i)
              str += f.slice(lastPos, i);
            str += "%";
            lastPos = i + 2;
            i++;
            a--;
            break;
        }
        ++a;
      }
      ++i;
    }
    if (lastPos === -1)
      return f;
    else if (lastPos < flen) {
      str += f.slice(lastPos);
    }
    return str;
  };
  module.exports = format;
});

// node_modules/atomic-sleep/index.js
var require_atomic_sleep = __commonJS((exports, module) => {
  if (typeof SharedArrayBuffer !== "undefined" && typeof Atomics !== "undefined") {
    let sleep = function(ms) {
      const valid = ms > 0 && ms < Infinity;
      if (valid === false) {
        if (typeof ms !== "number" && typeof ms !== "bigint") {
          throw TypeError("sleep: ms must be a number");
        }
        throw RangeError("sleep: ms must be a number that is greater than 0 but less than Infinity");
      }
      Atomics.wait(nil, 0, 0, Number(ms));
    };
    const nil = new Int32Array(new SharedArrayBuffer(4));
    module.exports = sleep;
  } else {
    let sleep = function(ms) {
      const valid = ms > 0 && ms < Infinity;
      if (valid === false) {
        if (typeof ms !== "number" && typeof ms !== "bigint") {
          throw TypeError("sleep: ms must be a number");
        }
        throw RangeError("sleep: ms must be a number that is greater than 0 but less than Infinity");
      }
      const target = Date.now() + Number(ms);
      while (target > Date.now()) {
      }
    };
    module.exports = sleep;
  }
});

// node_modules/sonic-boom/index.js
var require_sonic_boom = __commonJS((exports, module) => {
  var openFile = function(file, sonic) {
    sonic._opening = true;
    sonic._writing = true;
    sonic._asyncDrainScheduled = false;
    function fileOpened(err, fd) {
      if (err) {
        sonic._reopening = false;
        sonic._writing = false;
        sonic._opening = false;
        if (sonic.sync) {
          process.nextTick(() => {
            if (sonic.listenerCount("error") > 0) {
              sonic.emit("error", err);
            }
          });
        } else {
          sonic.emit("error", err);
        }
        return;
      }
      sonic.fd = fd;
      sonic.file = file;
      sonic._reopening = false;
      sonic._opening = false;
      sonic._writing = false;
      if (sonic.sync) {
        process.nextTick(() => sonic.emit("ready"));
      } else {
        sonic.emit("ready");
      }
      if (sonic._reopening || sonic.destroyed) {
        return;
      }
      if (!sonic._writing && sonic._len > sonic.minLength || sonic._flushPending) {
        sonic._actualWrite();
      }
    }
    const flags = sonic.append ? "a" : "w";
    const mode = sonic.mode;
    if (sonic.sync) {
      try {
        if (sonic.mkdir)
          fs.mkdirSync(path.dirname(file), { recursive: true });
        const fd = fs.openSync(file, flags, mode);
        fileOpened(null, fd);
      } catch (err) {
        fileOpened(err);
        throw err;
      }
    } else if (sonic.mkdir) {
      fs.mkdir(path.dirname(file), { recursive: true }, (err) => {
        if (err)
          return fileOpened(err);
        fs.open(file, flags, mode, fileOpened);
      });
    } else {
      fs.open(file, flags, mode, fileOpened);
    }
  };
  var SonicBoom = function(opts) {
    if (!(this instanceof SonicBoom)) {
      return new SonicBoom(opts);
    }
    let { fd, dest, minLength, maxLength, maxWrite, sync, append = true, mkdir, retryEAGAIN, fsync, contentMode, mode } = opts || {};
    fd = fd || dest;
    this._len = 0;
    this.fd = -1;
    this._bufs = [];
    this._lens = [];
    this._writing = false;
    this._ending = false;
    this._reopening = false;
    this._asyncDrainScheduled = false;
    this._flushPending = false;
    this._hwm = Math.max(minLength || 0, 16387);
    this.file = null;
    this.destroyed = false;
    this.minLength = minLength || 0;
    this.maxLength = maxLength || 0;
    this.maxWrite = maxWrite || MAX_WRITE;
    this.sync = sync || false;
    this.writable = true;
    this._fsync = fsync || false;
    this.append = append || false;
    this.mode = mode;
    this.retryEAGAIN = retryEAGAIN || (() => true);
    this.mkdir = mkdir || false;
    let fsWriteSync;
    let fsWrite;
    if (contentMode === kContentModeBuffer) {
      this._writingBuf = kEmptyBuffer;
      this.write = writeBuffer;
      this.flush = flushBuffer;
      this.flushSync = flushBufferSync;
      this._actualWrite = actualWriteBuffer;
      fsWriteSync = () => fs.writeSync(this.fd, this._writingBuf);
      fsWrite = () => fs.write(this.fd, this._writingBuf, this.release);
    } else if (contentMode === undefined || contentMode === kContentModeUtf8) {
      this._writingBuf = "";
      this.write = write;
      this.flush = flush;
      this.flushSync = flushSync;
      this._actualWrite = actualWrite;
      fsWriteSync = () => fs.writeSync(this.fd, this._writingBuf, "utf8");
      fsWrite = () => fs.write(this.fd, this._writingBuf, "utf8", this.release);
    } else {
      throw new Error(`SonicBoom supports "${kContentModeUtf8}" and "${kContentModeBuffer}", but passed ${contentMode}`);
    }
    if (typeof fd === "number") {
      this.fd = fd;
      process.nextTick(() => this.emit("ready"));
    } else if (typeof fd === "string") {
      openFile(fd, this);
    } else {
      throw new Error("SonicBoom supports only file descriptors and files");
    }
    if (this.minLength >= this.maxWrite) {
      throw new Error(`minLength should be smaller than maxWrite (${this.maxWrite})`);
    }
    this.release = (err, n) => {
      if (err) {
        if ((err.code === "EAGAIN" || err.code === "EBUSY") && this.retryEAGAIN(err, this._writingBuf.length, this._len - this._writingBuf.length)) {
          if (this.sync) {
            try {
              sleep(BUSY_WRITE_TIMEOUT);
              this.release(undefined, 0);
            } catch (err2) {
              this.release(err2);
            }
          } else {
            setTimeout(fsWrite, BUSY_WRITE_TIMEOUT);
          }
        } else {
          this._writing = false;
          this.emit("error", err);
        }
        return;
      }
      this.emit("write", n);
      const releasedBufObj = releaseWritingBuf(this._writingBuf, this._len, n);
      this._len = releasedBufObj.len;
      this._writingBuf = releasedBufObj.writingBuf;
      if (this._writingBuf.length) {
        if (!this.sync) {
          fsWrite();
          return;
        }
        try {
          do {
            const n2 = fsWriteSync();
            const releasedBufObj2 = releaseWritingBuf(this._writingBuf, this._len, n2);
            this._len = releasedBufObj2.len;
            this._writingBuf = releasedBufObj2.writingBuf;
          } while (this._writingBuf.length);
        } catch (err2) {
          this.release(err2);
          return;
        }
      }
      if (this._fsync) {
        fs.fsyncSync(this.fd);
      }
      const len = this._len;
      if (this._reopening) {
        this._writing = false;
        this._reopening = false;
        this.reopen();
      } else if (len > this.minLength) {
        this._actualWrite();
      } else if (this._ending) {
        if (len > 0) {
          this._actualWrite();
        } else {
          this._writing = false;
          actualClose(this);
        }
      } else {
        this._writing = false;
        if (this.sync) {
          if (!this._asyncDrainScheduled) {
            this._asyncDrainScheduled = true;
            process.nextTick(emitDrain, this);
          }
        } else {
          this.emit("drain");
        }
      }
    };
    this.on("newListener", function(name) {
      if (name === "drain") {
        this._asyncDrainScheduled = false;
      }
    });
  };
  var releaseWritingBuf = function(writingBuf, len, n) {
    if (typeof writingBuf === "string" && Buffer.byteLength(writingBuf) !== n) {
      n = Buffer.from(writingBuf).subarray(0, n).toString().length;
    }
    len = Math.max(len - n, 0);
    writingBuf = writingBuf.slice(n);
    return { writingBuf, len };
  };
  var emitDrain = function(sonic) {
    const hasListeners = sonic.listenerCount("drain") > 0;
    if (!hasListeners)
      return;
    sonic._asyncDrainScheduled = false;
    sonic.emit("drain");
  };
  var mergeBuf = function(bufs, len) {
    if (bufs.length === 0) {
      return kEmptyBuffer;
    }
    if (bufs.length === 1) {
      return bufs[0];
    }
    return Buffer.concat(bufs, len);
  };
  var write = function(data) {
    if (this.destroyed) {
      throw new Error("SonicBoom destroyed");
    }
    const len = this._len + data.length;
    const bufs = this._bufs;
    if (this.maxLength && len > this.maxLength) {
      this.emit("drop", data);
      return this._len < this._hwm;
    }
    if (bufs.length === 0 || bufs[bufs.length - 1].length + data.length > this.maxWrite) {
      bufs.push("" + data);
    } else {
      bufs[bufs.length - 1] += data;
    }
    this._len = len;
    if (!this._writing && this._len >= this.minLength) {
      this._actualWrite();
    }
    return this._len < this._hwm;
  };
  var writeBuffer = function(data) {
    if (this.destroyed) {
      throw new Error("SonicBoom destroyed");
    }
    const len = this._len + data.length;
    const bufs = this._bufs;
    const lens = this._lens;
    if (this.maxLength && len > this.maxLength) {
      this.emit("drop", data);
      return this._len < this._hwm;
    }
    if (bufs.length === 0 || lens[lens.length - 1] + data.length > this.maxWrite) {
      bufs.push([data]);
      lens.push(data.length);
    } else {
      bufs[bufs.length - 1].push(data);
      lens[lens.length - 1] += data.length;
    }
    this._len = len;
    if (!this._writing && this._len >= this.minLength) {
      this._actualWrite();
    }
    return this._len < this._hwm;
  };
  var callFlushCallbackOnDrain = function(cb) {
    this._flushPending = true;
    const onDrain = () => {
      if (!this._fsync) {
        fs.fsync(this.fd, (err) => {
          this._flushPending = false;
          cb(err);
        });
      } else {
        this._flushPending = false;
        cb();
      }
      this.off("error", onError);
    };
    const onError = (err) => {
      this._flushPending = false;
      cb(err);
      this.off("drain", onDrain);
    };
    this.once("drain", onDrain);
    this.once("error", onError);
  };
  var flush = function(cb) {
    if (cb != null && typeof cb !== "function") {
      throw new Error("flush cb must be a function");
    }
    if (this.destroyed) {
      const error = new Error("SonicBoom destroyed");
      if (cb) {
        cb(error);
        return;
      }
      throw error;
    }
    if (this.minLength <= 0) {
      cb?.();
      return;
    }
    if (cb) {
      callFlushCallbackOnDrain.call(this, cb);
    }
    if (this._writing) {
      return;
    }
    if (this._bufs.length === 0) {
      this._bufs.push("");
    }
    this._actualWrite();
  };
  var flushBuffer = function(cb) {
    if (cb != null && typeof cb !== "function") {
      throw new Error("flush cb must be a function");
    }
    if (this.destroyed) {
      const error = new Error("SonicBoom destroyed");
      if (cb) {
        cb(error);
        return;
      }
      throw error;
    }
    if (this.minLength <= 0) {
      cb?.();
      return;
    }
    if (cb) {
      callFlushCallbackOnDrain.call(this, cb);
    }
    if (this._writing) {
      return;
    }
    if (this._bufs.length === 0) {
      this._bufs.push([]);
      this._lens.push(0);
    }
    this._actualWrite();
  };
  var flushSync = function() {
    if (this.destroyed) {
      throw new Error("SonicBoom destroyed");
    }
    if (this.fd < 0) {
      throw new Error("sonic boom is not ready yet");
    }
    if (!this._writing && this._writingBuf.length > 0) {
      this._bufs.unshift(this._writingBuf);
      this._writingBuf = "";
    }
    let buf = "";
    while (this._bufs.length || buf) {
      if (buf.length <= 0) {
        buf = this._bufs[0];
      }
      try {
        const n = fs.writeSync(this.fd, buf, "utf8");
        const releasedBufObj = releaseWritingBuf(buf, this._len, n);
        buf = releasedBufObj.writingBuf;
        this._len = releasedBufObj.len;
        if (buf.length <= 0) {
          this._bufs.shift();
        }
      } catch (err) {
        const shouldRetry = err.code === "EAGAIN" || err.code === "EBUSY";
        if (shouldRetry && !this.retryEAGAIN(err, buf.length, this._len - buf.length)) {
          throw err;
        }
        sleep(BUSY_WRITE_TIMEOUT);
      }
    }
    try {
      fs.fsyncSync(this.fd);
    } catch {
    }
  };
  var flushBufferSync = function() {
    if (this.destroyed) {
      throw new Error("SonicBoom destroyed");
    }
    if (this.fd < 0) {
      throw new Error("sonic boom is not ready yet");
    }
    if (!this._writing && this._writingBuf.length > 0) {
      this._bufs.unshift([this._writingBuf]);
      this._writingBuf = kEmptyBuffer;
    }
    let buf = kEmptyBuffer;
    while (this._bufs.length || buf.length) {
      if (buf.length <= 0) {
        buf = mergeBuf(this._bufs[0], this._lens[0]);
      }
      try {
        const n = fs.writeSync(this.fd, buf);
        buf = buf.subarray(n);
        this._len = Math.max(this._len - n, 0);
        if (buf.length <= 0) {
          this._bufs.shift();
          this._lens.shift();
        }
      } catch (err) {
        const shouldRetry = err.code === "EAGAIN" || err.code === "EBUSY";
        if (shouldRetry && !this.retryEAGAIN(err, buf.length, this._len - buf.length)) {
          throw err;
        }
        sleep(BUSY_WRITE_TIMEOUT);
      }
    }
  };
  var actualWrite = function() {
    const release = this.release;
    this._writing = true;
    this._writingBuf = this._writingBuf || this._bufs.shift() || "";
    if (this.sync) {
      try {
        const written = fs.writeSync(this.fd, this._writingBuf, "utf8");
        release(null, written);
      } catch (err) {
        release(err);
      }
    } else {
      fs.write(this.fd, this._writingBuf, "utf8", release);
    }
  };
  var actualWriteBuffer = function() {
    const release = this.release;
    this._writing = true;
    this._writingBuf = this._writingBuf.length ? this._writingBuf : mergeBuf(this._bufs.shift(), this._lens.shift());
    if (this.sync) {
      try {
        const written = fs.writeSync(this.fd, this._writingBuf);
        release(null, written);
      } catch (err) {
        release(err);
      }
    } else {
      fs.write(this.fd, this._writingBuf, release);
    }
  };
  var actualClose = function(sonic) {
    if (sonic.fd === -1) {
      sonic.once("ready", actualClose.bind(null, sonic));
      return;
    }
    sonic.destroyed = true;
    sonic._bufs = [];
    sonic._lens = [];
    fs.fsync(sonic.fd, closeWrapped);
    function closeWrapped() {
      if (sonic.fd !== 1 && sonic.fd !== 2) {
        fs.close(sonic.fd, done);
      } else {
        done();
      }
    }
    function done(err) {
      if (err) {
        sonic.emit("error", err);
        return;
      }
      if (sonic._ending && !sonic._writing) {
        sonic.emit("finish");
      }
      sonic.emit("close");
    }
  };
  var fs = import.meta.require("fs");
  var EventEmitter = import.meta.require("events");
  var inherits = import.meta.require("util").inherits;
  var path = import.meta.require("path");
  var sleep = require_atomic_sleep();
  var BUSY_WRITE_TIMEOUT = 100;
  var kEmptyBuffer = Buffer.allocUnsafe(0);
  var MAX_WRITE = 16 * 1024;
  var kContentModeBuffer = "buffer";
  var kContentModeUtf8 = "utf8";
  inherits(SonicBoom, EventEmitter);
  SonicBoom.prototype.reopen = function(file) {
    if (this.destroyed) {
      throw new Error("SonicBoom destroyed");
    }
    if (this._opening) {
      this.once("ready", () => {
        this.reopen(file);
      });
      return;
    }
    if (this._ending) {
      return;
    }
    if (!this.file) {
      throw new Error("Unable to reopen a file descriptor, you must pass a file to SonicBoom");
    }
    this._reopening = true;
    if (this._writing) {
      return;
    }
    const fd = this.fd;
    this.once("ready", () => {
      if (fd !== this.fd) {
        fs.close(fd, (err) => {
          if (err) {
            return this.emit("error", err);
          }
        });
      }
    });
    openFile(file || this.file, this);
  };
  SonicBoom.prototype.end = function() {
    if (this.destroyed) {
      throw new Error("SonicBoom destroyed");
    }
    if (this._opening) {
      this.once("ready", () => {
        this.end();
      });
      return;
    }
    if (this._ending) {
      return;
    }
    this._ending = true;
    if (this._writing) {
      return;
    }
    if (this._len > 0 && this.fd >= 0) {
      this._actualWrite();
    } else {
      actualClose(this);
    }
  };
  SonicBoom.prototype.destroy = function() {
    if (this.destroyed) {
      return;
    }
    actualClose(this);
  };
  SonicBoom.SonicBoom = SonicBoom;
  SonicBoom.default = SonicBoom;
  module.exports = SonicBoom;
});

// node_modules/on-exit-leak-free/index.js
var require_on_exit_leak_free = __commonJS((exports, module) => {
  var ensureRegistry = function() {
    if (registry === undefined) {
      registry = new FinalizationRegistry(clear);
    }
  };
  var install = function(event) {
    if (refs[event].length > 0) {
      return;
    }
    process.on(event, functions[event]);
  };
  var uninstall = function(event) {
    if (refs[event].length > 0) {
      return;
    }
    process.removeListener(event, functions[event]);
    if (refs.exit.length === 0 && refs.beforeExit.length === 0) {
      registry = undefined;
    }
  };
  var onExit = function() {
    callRefs("exit");
  };
  var onBeforeExit = function() {
    callRefs("beforeExit");
  };
  var callRefs = function(event) {
    for (const ref of refs[event]) {
      const obj = ref.deref();
      const fn = ref.fn;
      if (obj !== undefined) {
        fn(obj, event);
      }
    }
    refs[event] = [];
  };
  var clear = function(ref) {
    for (const event of ["exit", "beforeExit"]) {
      const index = refs[event].indexOf(ref);
      refs[event].splice(index, index + 1);
      uninstall(event);
    }
  };
  var _register = function(event, obj, fn) {
    if (obj === undefined) {
      throw new Error("the object can\'t be undefined");
    }
    install(event);
    const ref = new WeakRef(obj);
    ref.fn = fn;
    ensureRegistry();
    registry.register(obj, ref);
    refs[event].push(ref);
  };
  var register = function(obj, fn) {
    _register("exit", obj, fn);
  };
  var registerBeforeExit = function(obj, fn) {
    _register("beforeExit", obj, fn);
  };
  var unregister = function(obj) {
    if (registry === undefined) {
      return;
    }
    registry.unregister(obj);
    for (const event of ["exit", "beforeExit"]) {
      refs[event] = refs[event].filter((ref) => {
        const _obj = ref.deref();
        return _obj && _obj !== obj;
      });
      uninstall(event);
    }
  };
  var refs = {
    exit: [],
    beforeExit: []
  };
  var functions = {
    exit: onExit,
    beforeExit: onBeforeExit
  };
  var registry;
  module.exports = {
    register,
    registerBeforeExit,
    unregister
  };
});

// node_modules/thread-stream/package.json
var require_package = __commonJS((exports, module) => {
  module.exports = {
    name: "thread-stream",
    version: "2.4.1",
    description: "A streaming way to send data to a Node.js Worker Thread",
    main: "index.js",
    types: "index.d.ts",
    dependencies: {
      "real-require": "^0.2.0"
    },
    devDependencies: {
      "@types/node": "^20.1.0",
      "@types/tap": "^15.0.0",
      desm: "^1.3.0",
      fastbench: "^1.0.1",
      husky: "^8.0.1",
      "pino-elasticsearch": "^6.0.0",
      "sonic-boom": "^3.0.0",
      standard: "^17.0.0",
      tap: "^16.2.0",
      "ts-node": "^10.8.0",
      typescript: "^4.7.2",
      "why-is-node-running": "^2.2.2"
    },
    scripts: {
      test: "standard && npm run transpile && tap test/*.test.*js && tap --ts test/*.test.*ts",
      "test:ci": "standard && npm run transpile && npm run test:ci:js && npm run test:ci:ts",
      "test:ci:js": "tap --no-check-coverage --coverage-report=lcovonly \"test/**/*.test.*js\"",
      "test:ci:ts": "tap --ts --no-check-coverage --coverage-report=lcovonly \"test/**/*.test.*ts\"",
      "test:yarn": "npm run transpile && tap \"test/**/*.test.js\" --no-check-coverage",
      transpile: "sh ./test/ts/transpile.sh",
      prepare: "husky install"
    },
    standard: {
      ignore: [
        "test/ts/**/*"
      ]
    },
    repository: {
      type: "git",
      url: "git+https://github.com/mcollina/thread-stream.git"
    },
    keywords: [
      "worker",
      "thread",
      "threads",
      "stream"
    ],
    author: "Matteo Collina <hello@matteocollina.com>",
    license: "MIT",
    bugs: {
      url: "https://github.com/mcollina/thread-stream/issues"
    },
    homepage: "https://github.com/mcollina/thread-stream#readme"
  };
});

// node_modules/thread-stream/lib/wait.js
var require_wait = __commonJS((exports, module) => {
  var wait = function(state, index, expected, timeout, done) {
    const max = Date.now() + timeout;
    let current = Atomics.load(state, index);
    if (current === expected) {
      done(null, "ok");
      return;
    }
    let prior = current;
    const check = (backoff) => {
      if (Date.now() > max) {
        done(null, "timed-out");
      } else {
        setTimeout(() => {
          prior = current;
          current = Atomics.load(state, index);
          if (current === prior) {
            check(backoff >= MAX_TIMEOUT ? MAX_TIMEOUT : backoff * 2);
          } else {
            if (current === expected)
              done(null, "ok");
            else
              done(null, "not-equal");
          }
        }, backoff);
      }
    };
    check(1);
  };
  var waitDiff = function(state, index, expected, timeout, done) {
    const max = Date.now() + timeout;
    let current = Atomics.load(state, index);
    if (current !== expected) {
      done(null, "ok");
      return;
    }
    const check = (backoff) => {
      if (Date.now() > max) {
        done(null, "timed-out");
      } else {
        setTimeout(() => {
          current = Atomics.load(state, index);
          if (current !== expected) {
            done(null, "ok");
          } else {
            check(backoff >= MAX_TIMEOUT ? MAX_TIMEOUT : backoff * 2);
          }
        }, backoff);
      }
    };
    check(1);
  };
  var MAX_TIMEOUT = 1000;
  module.exports = { wait, waitDiff };
});

// node_modules/thread-stream/lib/indexes.js
var require_indexes = __commonJS((exports, module) => {
  var WRITE_INDEX = 4;
  var READ_INDEX = 8;
  module.exports = {
    WRITE_INDEX,
    READ_INDEX
  };
});

// node_modules/thread-stream/index.js
var require_thread_stream = __commonJS((exports, module) => {
  var createWorker = function(stream, opts) {
    const { filename, workerData } = opts;
    const bundlerOverrides = "__bundlerPathsOverrides" in globalThis ? globalThis.__bundlerPathsOverrides : {};
    const toExecute = bundlerOverrides["thread-stream-worker"] || join2(__dirname, "lib", "worker.js");
    const worker = new Worker(toExecute, {
      ...opts.workerOpts,
      trackUnmanagedFds: false,
      workerData: {
        filename: filename.indexOf("file://") === 0 ? filename : pathToFileURL(filename).href,
        dataBuf: stream[kImpl].dataBuf,
        stateBuf: stream[kImpl].stateBuf,
        workerData: {
          $context: {
            threadStreamVersion: version
          },
          ...workerData
        }
      }
    });
    worker.stream = new FakeWeakRef(stream);
    worker.on("message", onWorkerMessage);
    worker.on("exit", onWorkerExit);
    registry.register(stream, worker);
    return worker;
  };
  var drain = function(stream) {
    assert(!stream[kImpl].sync);
    if (stream[kImpl].needDrain) {
      stream[kImpl].needDrain = false;
      stream.emit("drain");
    }
  };
  var nextFlush = function(stream) {
    const writeIndex = Atomics.load(stream[kImpl].state, WRITE_INDEX);
    let leftover = stream[kImpl].data.length - writeIndex;
    if (leftover > 0) {
      if (stream[kImpl].buf.length === 0) {
        stream[kImpl].flushing = false;
        if (stream[kImpl].ending) {
          end(stream);
        } else if (stream[kImpl].needDrain) {
          process.nextTick(drain, stream);
        }
        return;
      }
      let toWrite = stream[kImpl].buf.slice(0, leftover);
      let toWriteBytes = Buffer.byteLength(toWrite);
      if (toWriteBytes <= leftover) {
        stream[kImpl].buf = stream[kImpl].buf.slice(leftover);
        write(stream, toWrite, nextFlush.bind(null, stream));
      } else {
        stream.flush(() => {
          if (stream.destroyed) {
            return;
          }
          Atomics.store(stream[kImpl].state, READ_INDEX, 0);
          Atomics.store(stream[kImpl].state, WRITE_INDEX, 0);
          while (toWriteBytes > stream[kImpl].data.length) {
            leftover = leftover / 2;
            toWrite = stream[kImpl].buf.slice(0, leftover);
            toWriteBytes = Buffer.byteLength(toWrite);
          }
          stream[kImpl].buf = stream[kImpl].buf.slice(leftover);
          write(stream, toWrite, nextFlush.bind(null, stream));
        });
      }
    } else if (leftover === 0) {
      if (writeIndex === 0 && stream[kImpl].buf.length === 0) {
        return;
      }
      stream.flush(() => {
        Atomics.store(stream[kImpl].state, READ_INDEX, 0);
        Atomics.store(stream[kImpl].state, WRITE_INDEX, 0);
        nextFlush(stream);
      });
    } else {
      destroy(stream, new Error("overwritten"));
    }
  };
  var onWorkerMessage = function(msg) {
    const stream = this.stream.deref();
    if (stream === undefined) {
      this.exited = true;
      this.terminate();
      return;
    }
    switch (msg.code) {
      case "READY":
        this.stream = new WeakRef2(stream);
        stream.flush(() => {
          stream[kImpl].ready = true;
          stream.emit("ready");
        });
        break;
      case "ERROR":
        destroy(stream, msg.err);
        break;
      case "EVENT":
        if (Array.isArray(msg.args)) {
          stream.emit(msg.name, ...msg.args);
        } else {
          stream.emit(msg.name, msg.args);
        }
        break;
      case "WARNING":
        process.emitWarning(msg.err);
        break;
      default:
        destroy(stream, new Error("this should not happen: " + msg.code));
    }
  };
  var onWorkerExit = function(code) {
    const stream = this.stream.deref();
    if (stream === undefined) {
      return;
    }
    registry.unregister(stream);
    stream.worker.exited = true;
    stream.worker.off("exit", onWorkerExit);
    destroy(stream, code !== 0 ? new Error("the worker thread exited") : null);
  };
  var error = function(stream, err) {
    setImmediate(() => {
      stream.emit("error", err);
    });
  };
  var destroy = function(stream, err) {
    if (stream[kImpl].destroyed) {
      return;
    }
    stream[kImpl].destroyed = true;
    if (err) {
      stream[kImpl].errored = err;
      error(stream, err);
    }
    if (!stream.worker.exited) {
      stream.worker.terminate().catch(() => {
      }).then(() => {
        stream[kImpl].closed = true;
        stream.emit("close");
      });
    } else {
      setImmediate(() => {
        stream[kImpl].closed = true;
        stream.emit("close");
      });
    }
  };
  var write = function(stream, data, cb) {
    const current = Atomics.load(stream[kImpl].state, WRITE_INDEX);
    const length = Buffer.byteLength(data);
    stream[kImpl].data.write(data, current);
    Atomics.store(stream[kImpl].state, WRITE_INDEX, current + length);
    Atomics.notify(stream[kImpl].state, WRITE_INDEX);
    cb();
    return true;
  };
  var end = function(stream) {
    if (stream[kImpl].ended || !stream[kImpl].ending || stream[kImpl].flushing) {
      return;
    }
    stream[kImpl].ended = true;
    try {
      stream.flushSync();
      let readIndex = Atomics.load(stream[kImpl].state, READ_INDEX);
      Atomics.store(stream[kImpl].state, WRITE_INDEX, -1);
      Atomics.notify(stream[kImpl].state, WRITE_INDEX);
      let spins = 0;
      while (readIndex !== -1) {
        Atomics.wait(stream[kImpl].state, READ_INDEX, readIndex, 1000);
        readIndex = Atomics.load(stream[kImpl].state, READ_INDEX);
        if (readIndex === -2) {
          destroy(stream, new Error("end() failed"));
          return;
        }
        if (++spins === 10) {
          destroy(stream, new Error("end() took too long (10s)"));
          return;
        }
      }
      process.nextTick(() => {
        stream[kImpl].finished = true;
        stream.emit("finish");
      });
    } catch (err) {
      destroy(stream, err);
    }
  };
  var writeSync = function(stream) {
    const cb = () => {
      if (stream[kImpl].ending) {
        end(stream);
      } else if (stream[kImpl].needDrain) {
        process.nextTick(drain, stream);
      }
    };
    stream[kImpl].flushing = false;
    while (stream[kImpl].buf.length !== 0) {
      const writeIndex = Atomics.load(stream[kImpl].state, WRITE_INDEX);
      let leftover = stream[kImpl].data.length - writeIndex;
      if (leftover === 0) {
        flushSync(stream);
        Atomics.store(stream[kImpl].state, READ_INDEX, 0);
        Atomics.store(stream[kImpl].state, WRITE_INDEX, 0);
        continue;
      } else if (leftover < 0) {
        throw new Error("overwritten");
      }
      let toWrite = stream[kImpl].buf.slice(0, leftover);
      let toWriteBytes = Buffer.byteLength(toWrite);
      if (toWriteBytes <= leftover) {
        stream[kImpl].buf = stream[kImpl].buf.slice(leftover);
        write(stream, toWrite, cb);
      } else {
        flushSync(stream);
        Atomics.store(stream[kImpl].state, READ_INDEX, 0);
        Atomics.store(stream[kImpl].state, WRITE_INDEX, 0);
        while (toWriteBytes > stream[kImpl].buf.length) {
          leftover = leftover / 2;
          toWrite = stream[kImpl].buf.slice(0, leftover);
          toWriteBytes = Buffer.byteLength(toWrite);
        }
        stream[kImpl].buf = stream[kImpl].buf.slice(leftover);
        write(stream, toWrite, cb);
      }
    }
  };
  var flushSync = function(stream) {
    if (stream[kImpl].flushing) {
      throw new Error("unable to flush while flushing");
    }
    const writeIndex = Atomics.load(stream[kImpl].state, WRITE_INDEX);
    let spins = 0;
    while (true) {
      const readIndex = Atomics.load(stream[kImpl].state, READ_INDEX);
      if (readIndex === -2) {
        throw Error("_flushSync failed");
      }
      if (readIndex !== writeIndex) {
        Atomics.wait(stream[kImpl].state, READ_INDEX, readIndex, 1000);
      } else {
        break;
      }
      if (++spins === 10) {
        throw new Error("_flushSync took too long (10s)");
      }
    }
  };
  var __dirname = "/Users/ambassador4ik/WebstormProjects/The Brainiest/AuthService/Backend/node_modules/thread-stream";
  var { version } = require_package();
  var { EventEmitter } = import.meta.require("events");
  var { Worker } = import.meta.require("worker_threads");
  var { join: join2 } = import.meta.require("path");
  var { pathToFileURL } = import.meta.require("url");
  var { wait } = require_wait();
  var {
    WRITE_INDEX,
    READ_INDEX
  } = require_indexes();
  var buffer = import.meta.require("buffer");
  var assert = import.meta.require("assert");
  var kImpl = Symbol("kImpl");
  var MAX_STRING = buffer.constants.MAX_STRING_LENGTH;

  class FakeWeakRef {
    constructor(value) {
      this._value = value;
    }
    deref() {
      return this._value;
    }
  }

  class FakeFinalizationRegistry {
    register() {
    }
    unregister() {
    }
  }
  var FinalizationRegistry2 = process.env.NODE_V8_COVERAGE ? FakeFinalizationRegistry : global.FinalizationRegistry || FakeFinalizationRegistry;
  var WeakRef2 = process.env.NODE_V8_COVERAGE ? FakeWeakRef : global.WeakRef || FakeWeakRef;
  var registry = new FinalizationRegistry2((worker) => {
    if (worker.exited) {
      return;
    }
    worker.terminate();
  });

  class ThreadStream extends EventEmitter {
    constructor(opts = {}) {
      super();
      if (opts.bufferSize < 4) {
        throw new Error("bufferSize must at least fit a 4-byte utf-8 char");
      }
      this[kImpl] = {};
      this[kImpl].stateBuf = new SharedArrayBuffer(128);
      this[kImpl].state = new Int32Array(this[kImpl].stateBuf);
      this[kImpl].dataBuf = new SharedArrayBuffer(opts.bufferSize || 4 * 1024 * 1024);
      this[kImpl].data = Buffer.from(this[kImpl].dataBuf);
      this[kImpl].sync = opts.sync || false;
      this[kImpl].ending = false;
      this[kImpl].ended = false;
      this[kImpl].needDrain = false;
      this[kImpl].destroyed = false;
      this[kImpl].flushing = false;
      this[kImpl].ready = false;
      this[kImpl].finished = false;
      this[kImpl].errored = null;
      this[kImpl].closed = false;
      this[kImpl].buf = "";
      this.worker = createWorker(this, opts);
    }
    write(data) {
      if (this[kImpl].destroyed) {
        error(this, new Error("the worker has exited"));
        return false;
      }
      if (this[kImpl].ending) {
        error(this, new Error("the worker is ending"));
        return false;
      }
      if (this[kImpl].flushing && this[kImpl].buf.length + data.length >= MAX_STRING) {
        try {
          writeSync(this);
          this[kImpl].flushing = true;
        } catch (err) {
          destroy(this, err);
          return false;
        }
      }
      this[kImpl].buf += data;
      if (this[kImpl].sync) {
        try {
          writeSync(this);
          return true;
        } catch (err) {
          destroy(this, err);
          return false;
        }
      }
      if (!this[kImpl].flushing) {
        this[kImpl].flushing = true;
        setImmediate(nextFlush, this);
      }
      this[kImpl].needDrain = this[kImpl].data.length - this[kImpl].buf.length - Atomics.load(this[kImpl].state, WRITE_INDEX) <= 0;
      return !this[kImpl].needDrain;
    }
    end() {
      if (this[kImpl].destroyed) {
        return;
      }
      this[kImpl].ending = true;
      end(this);
    }
    flush(cb) {
      if (this[kImpl].destroyed) {
        if (typeof cb === "function") {
          process.nextTick(cb, new Error("the worker has exited"));
        }
        return;
      }
      const writeIndex = Atomics.load(this[kImpl].state, WRITE_INDEX);
      wait(this[kImpl].state, READ_INDEX, writeIndex, Infinity, (err, res) => {
        if (err) {
          destroy(this, err);
          process.nextTick(cb, err);
          return;
        }
        if (res === "not-equal") {
          this.flush(cb);
          return;
        }
        process.nextTick(cb);
      });
    }
    flushSync() {
      if (this[kImpl].destroyed) {
        return;
      }
      writeSync(this);
      flushSync(this);
    }
    unref() {
      this.worker.unref();
    }
    ref() {
      this.worker.ref();
    }
    get ready() {
      return this[kImpl].ready;
    }
    get destroyed() {
      return this[kImpl].destroyed;
    }
    get closed() {
      return this[kImpl].closed;
    }
    get writable() {
      return !this[kImpl].destroyed && !this[kImpl].ending;
    }
    get writableEnded() {
      return this[kImpl].ending;
    }
    get writableFinished() {
      return this[kImpl].finished;
    }
    get writableNeedDrain() {
      return this[kImpl].needDrain;
    }
    get writableObjectMode() {
      return false;
    }
    get writableErrored() {
      return this[kImpl].errored;
    }
  }
  module.exports = ThreadStream;
});

// node_modules/pino/lib/transport.js
var require_transport = __commonJS((exports, module) => {
  var setupOnExit = function(stream) {
    onExit.register(stream, autoEnd);
    onExit.registerBeforeExit(stream, flush);
    stream.on("close", function() {
      onExit.unregister(stream);
    });
  };
  var buildStream = function(filename, workerData, workerOpts) {
    const stream = new ThreadStream({
      filename,
      workerData,
      workerOpts
    });
    stream.on("ready", onReady);
    stream.on("close", function() {
      process.removeListener("exit", onExit2);
    });
    process.on("exit", onExit2);
    function onReady() {
      process.removeListener("exit", onExit2);
      stream.unref();
      if (workerOpts.autoEnd !== false) {
        setupOnExit(stream);
      }
    }
    function onExit2() {
      if (stream.closed) {
        return;
      }
      stream.flushSync();
      sleep(100);
      stream.end();
    }
    return stream;
  };
  var autoEnd = function(stream) {
    stream.ref();
    stream.flushSync();
    stream.end();
    stream.once("close", function() {
      stream.unref();
    });
  };
  var flush = function(stream) {
    stream.flushSync();
  };
  var transport = function(fullOptions) {
    const { pipeline, targets, levels, dedupe, options = {}, worker = {}, caller = getCallers() } = fullOptions;
    const callers = typeof caller === "string" ? [caller] : caller;
    const bundlerOverrides = "__bundlerPathsOverrides" in globalThis ? globalThis.__bundlerPathsOverrides : {};
    let target = fullOptions.target;
    if (target && targets) {
      throw new Error("only one of target or targets can be specified");
    }
    if (targets) {
      target = bundlerOverrides["pino-worker"] || join2(__dirname, "worker.js");
      options.targets = targets.map((dest) => {
        return {
          ...dest,
          target: fixTarget(dest.target)
        };
      });
    } else if (pipeline) {
      target = bundlerOverrides["pino-pipeline-worker"] || join2(__dirname, "worker-pipeline.js");
      options.targets = pipeline.map((dest) => {
        return {
          ...dest,
          target: fixTarget(dest.target)
        };
      });
    }
    if (levels) {
      options.levels = levels;
    }
    if (dedupe) {
      options.dedupe = dedupe;
    }
    return buildStream(fixTarget(target), options, worker);
    function fixTarget(origin) {
      origin = bundlerOverrides[origin] || origin;
      if (isAbsolute(origin) || origin.indexOf("file://") === 0) {
        return origin;
      }
      if (origin === "pino/file") {
        return join2(__dirname, "..", "file.js");
      }
      let fixTarget2;
      for (const filePath of callers) {
        try {
          const context3 = filePath === "node:repl" ? process.cwd() + sep : filePath;
          fixTarget2 = createRequire(context3).resolve(origin);
          break;
        } catch (err) {
          continue;
        }
      }
      if (!fixTarget2) {
        throw new Error(`unable to determine transport target for "${origin}"`);
      }
      return fixTarget2;
    }
  };
  var __dirname = "/Users/ambassador4ik/WebstormProjects/The Brainiest/AuthService/Backend/node_modules/pino/lib";
  var { createRequire } = import.meta.require("module");
  var getCallers = require_caller();
  var { join: join2, isAbsolute, sep } = import.meta.require("path");
  var sleep = require_atomic_sleep();
  var onExit = require_on_exit_leak_free();
  var ThreadStream = require_thread_stream();
  module.exports = transport;
});

// node_modules/pino/lib/tools.js
var require_tools = __commonJS((exports, module) => {
  var noop = function() {
  };
  var genLog = function(level, hook) {
    if (!hook)
      return LOG;
    return function hookWrappedLog(...args) {
      hook.call(this, args, LOG, level);
    };
    function LOG(o, ...n) {
      if (typeof o === "object") {
        let msg = o;
        if (o !== null) {
          if (o.method && o.headers && o.socket) {
            o = mapHttpRequest(o);
          } else if (typeof o.setHeader === "function") {
            o = mapHttpResponse(o);
          }
        }
        let formatParams;
        if (msg === null && n.length === 0) {
          formatParams = [null];
        } else {
          msg = n.shift();
          formatParams = n;
        }
        if (typeof this[msgPrefixSym] === "string" && msg !== undefined && msg !== null) {
          msg = this[msgPrefixSym] + msg;
        }
        this[writeSym](o, format(msg, formatParams, this[formatOptsSym]), level);
      } else {
        let msg = o === undefined ? n.shift() : o;
        if (typeof this[msgPrefixSym] === "string" && msg !== undefined && msg !== null) {
          msg = this[msgPrefixSym] + msg;
        }
        this[writeSym](null, format(msg, n, this[formatOptsSym]), level);
      }
    }
  };
  var asString = function(str) {
    let result = "";
    let last = 0;
    let found = false;
    let point = 255;
    const l = str.length;
    if (l > 100) {
      return JSON.stringify(str);
    }
    for (var i = 0;i < l && point >= 32; i++) {
      point = str.charCodeAt(i);
      if (point === 34 || point === 92) {
        result += str.slice(last, i) + "\\";
        last = i;
        found = true;
      }
    }
    if (!found) {
      result = str;
    } else {
      result += str.slice(last);
    }
    return point < 32 ? JSON.stringify(str) : '"' + result + '"';
  };
  var asJson = function(obj, msg, num, time) {
    const stringify2 = this[stringifySym];
    const stringifySafe = this[stringifySafeSym];
    const stringifiers = this[stringifiersSym];
    const end = this[endSym];
    const chindings = this[chindingsSym];
    const serializers = this[serializersSym];
    const formatters = this[formattersSym];
    const messageKey = this[messageKeySym];
    const errorKey = this[errorKeySym];
    let data = this[lsCacheSym][num] + time;
    data = data + chindings;
    let value;
    if (formatters.log) {
      obj = formatters.log(obj);
    }
    const wildcardStringifier = stringifiers[wildcardFirstSym];
    let propStr = "";
    for (const key in obj) {
      value = obj[key];
      if (Object.prototype.hasOwnProperty.call(obj, key) && value !== undefined) {
        if (serializers[key]) {
          value = serializers[key](value);
        } else if (key === errorKey && serializers.err) {
          value = serializers.err(value);
        }
        const stringifier = stringifiers[key] || wildcardStringifier;
        switch (typeof value) {
          case "undefined":
          case "function":
            continue;
          case "number":
            if (Number.isFinite(value) === false) {
              value = null;
            }
          case "boolean":
            if (stringifier)
              value = stringifier(value);
            break;
          case "string":
            value = (stringifier || asString)(value);
            break;
          default:
            value = (stringifier || stringify2)(value, stringifySafe);
        }
        if (value === undefined)
          continue;
        const strKey = asString(key);
        propStr += "," + strKey + ":" + value;
      }
    }
    let msgStr = "";
    if (msg !== undefined) {
      value = serializers[messageKey] ? serializers[messageKey](msg) : msg;
      const stringifier = stringifiers[messageKey] || wildcardStringifier;
      switch (typeof value) {
        case "function":
          break;
        case "number":
          if (Number.isFinite(value) === false) {
            value = null;
          }
        case "boolean":
          if (stringifier)
            value = stringifier(value);
          msgStr = ',"' + messageKey + '":' + value;
          break;
        case "string":
          value = (stringifier || asString)(value);
          msgStr = ',"' + messageKey + '":' + value;
          break;
        default:
          value = (stringifier || stringify2)(value, stringifySafe);
          msgStr = ',"' + messageKey + '":' + value;
      }
    }
    if (this[nestedKeySym] && propStr) {
      return data + this[nestedKeyStrSym] + propStr.slice(1) + "}" + msgStr + end;
    } else {
      return data + propStr + msgStr + end;
    }
  };
  var asChindings = function(instance, bindings) {
    let value;
    let data = instance[chindingsSym];
    const stringify2 = instance[stringifySym];
    const stringifySafe = instance[stringifySafeSym];
    const stringifiers = instance[stringifiersSym];
    const wildcardStringifier = stringifiers[wildcardFirstSym];
    const serializers = instance[serializersSym];
    const formatter = instance[formattersSym].bindings;
    bindings = formatter(bindings);
    for (const key in bindings) {
      value = bindings[key];
      const valid = key !== "level" && key !== "serializers" && key !== "formatters" && key !== "customLevels" && bindings.hasOwnProperty(key) && value !== undefined;
      if (valid === true) {
        value = serializers[key] ? serializers[key](value) : value;
        value = (stringifiers[key] || wildcardStringifier || stringify2)(value, stringifySafe);
        if (value === undefined)
          continue;
        data += ',"' + key + '":' + value;
      }
    }
    return data;
  };
  var hasBeenTampered = function(stream) {
    return stream.write !== stream.constructor.prototype.write;
  };
  var buildSafeSonicBoom = function(opts) {
    const stream = new SonicBoom(opts);
    stream.on("error", filterBrokenPipe);
    if (!hasNodeCodeCoverage && !opts.sync && isMainThread) {
      onExit.register(stream, autoEnd);
      stream.on("close", function() {
        onExit.unregister(stream);
      });
    }
    return stream;
    function filterBrokenPipe(err) {
      if (err.code === "EPIPE") {
        stream.write = noop;
        stream.end = noop;
        stream.flushSync = noop;
        stream.destroy = noop;
        return;
      }
      stream.removeListener("error", filterBrokenPipe);
      stream.emit("error", err);
    }
  };
  var autoEnd = function(stream, eventName) {
    if (stream.destroyed) {
      return;
    }
    if (eventName === "beforeExit") {
      stream.flush();
      stream.on("drain", function() {
        stream.end();
      });
    } else {
      stream.flushSync();
    }
  };
  var createArgsNormalizer = function(defaultOptions) {
    return function normalizeArgs(instance, caller, opts = {}, stream) {
      if (typeof opts === "string") {
        stream = buildSafeSonicBoom({ dest: opts });
        opts = {};
      } else if (typeof stream === "string") {
        if (opts && opts.transport) {
          throw Error("only one of option.transport or stream can be specified");
        }
        stream = buildSafeSonicBoom({ dest: stream });
      } else if (opts instanceof SonicBoom || opts.writable || opts._writableState) {
        stream = opts;
        opts = {};
      } else if (opts.transport) {
        if (opts.transport instanceof SonicBoom || opts.transport.writable || opts.transport._writableState) {
          throw Error("option.transport do not allow stream, please pass to option directly. e.g. pino(transport)");
        }
        if (opts.transport.targets && opts.transport.targets.length && opts.formatters && typeof opts.formatters.level === "function") {
          throw Error("option.transport.targets do not allow custom level formatters");
        }
        let customLevels;
        if (opts.customLevels) {
          customLevels = opts.useOnlyCustomLevels ? opts.customLevels : Object.assign({}, opts.levels, opts.customLevels);
        }
        stream = transport({ caller, ...opts.transport, levels: customLevels });
      }
      opts = Object.assign({}, defaultOptions, opts);
      opts.serializers = Object.assign({}, defaultOptions.serializers, opts.serializers);
      opts.formatters = Object.assign({}, defaultOptions.formatters, opts.formatters);
      if (opts.prettyPrint) {
        throw new Error("prettyPrint option is no longer supported, see the pino-pretty package (https://github.com/pinojs/pino-pretty)");
      }
      const { enabled, onChild } = opts;
      if (enabled === false)
        opts.level = "silent";
      if (!onChild)
        opts.onChild = noop;
      if (!stream) {
        if (!hasBeenTampered(process.stdout)) {
          stream = buildSafeSonicBoom({ fd: process.stdout.fd || 1 });
        } else {
          stream = process.stdout;
        }
      }
      return { opts, stream };
    };
  };
  var stringify = function(obj, stringifySafeFn) {
    try {
      return JSON.stringify(obj);
    } catch (_) {
      try {
        const stringify2 = stringifySafeFn || this[stringifySafeSym];
        return stringify2(obj);
      } catch (_2) {
        return '"[unable to serialize, circular reference is too complex to analyze]"';
      }
    }
  };
  var buildFormatters = function(level, bindings, log) {
    return {
      level,
      bindings,
      log
    };
  };
  var normalizeDestFileDescriptor = function(destination) {
    const fd = Number(destination);
    if (typeof destination === "string" && Number.isFinite(fd)) {
      return fd;
    }
    if (destination === undefined) {
      return 1;
    }
    return destination;
  };
  var format = require_quick_format_unescaped();
  var { mapHttpRequest, mapHttpResponse } = require_pino_std_serializers();
  var SonicBoom = require_sonic_boom();
  var onExit = require_on_exit_leak_free();
  var {
    lsCacheSym,
    chindingsSym,
    writeSym,
    serializersSym,
    formatOptsSym,
    endSym,
    stringifiersSym,
    stringifySym,
    stringifySafeSym,
    wildcardFirstSym,
    nestedKeySym,
    formattersSym,
    messageKeySym,
    errorKeySym,
    nestedKeyStrSym,
    msgPrefixSym
  } = require_symbols();
  var { isMainThread } = import.meta.require("worker_threads");
  var transport = require_transport();
  var hasNodeCodeCoverage = process.env.NODE_V8_COVERAGE || process.env.V8_COVERAGE;
  module.exports = {
    noop,
    buildSafeSonicBoom,
    asChindings,
    asJson,
    genLog,
    createArgsNormalizer,
    stringify,
    buildFormatters,
    normalizeDestFileDescriptor
  };
});

// node_modules/pino/lib/constants.js
var require_constants = __commonJS((exports, module) => {
  var DEFAULT_LEVELS = {
    trace: 10,
    debug: 20,
    info: 30,
    warn: 40,
    error: 50,
    fatal: 60
  };
  var SORTING_ORDER = {
    ASC: "ASC",
    DESC: "DESC"
  };
  module.exports = {
    DEFAULT_LEVELS,
    SORTING_ORDER
  };
});

// node_modules/pino/lib/levels.js
var require_levels = __commonJS((exports, module) => {
  var genLsCache = function(instance) {
    const formatter = instance[formattersSym].level;
    const { labels } = instance.levels;
    const cache = {};
    for (const label in labels) {
      const level = formatter(labels[label], Number(label));
      cache[label] = JSON.stringify(level).slice(0, -1);
    }
    instance[lsCacheSym] = cache;
    return instance;
  };
  var isStandardLevel = function(level, useOnlyCustomLevels) {
    if (useOnlyCustomLevels) {
      return false;
    }
    switch (level) {
      case "fatal":
      case "error":
      case "warn":
      case "info":
      case "debug":
      case "trace":
        return true;
      default:
        return false;
    }
  };
  var setLevel = function(level) {
    const { labels, values } = this.levels;
    if (typeof level === "number") {
      if (labels[level] === undefined)
        throw Error("unknown level value" + level);
      level = labels[level];
    }
    if (values[level] === undefined)
      throw Error("unknown level " + level);
    const preLevelVal = this[levelValSym];
    const levelVal = this[levelValSym] = values[level];
    const useOnlyCustomLevelsVal = this[useOnlyCustomLevelsSym];
    const levelComparison = this[levelCompSym];
    const hook = this[hooksSym].logMethod;
    for (const key in values) {
      if (levelComparison(values[key], levelVal) === false) {
        this[key] = noop;
        continue;
      }
      this[key] = isStandardLevel(key, useOnlyCustomLevelsVal) ? levelMethods[key](hook) : genLog(values[key], hook);
    }
    this.emit("level-change", level, levelVal, labels[preLevelVal], preLevelVal, this);
  };
  var getLevel = function(level) {
    const { levels, levelVal } = this;
    return levels && levels.labels ? levels.labels[levelVal] : "";
  };
  var isLevelEnabled = function(logLevel) {
    const { values } = this.levels;
    const logLevelVal = values[logLevel];
    return logLevelVal !== undefined && this[levelCompSym](logLevelVal, this[levelValSym]);
  };
  var compareLevel = function(direction, current, expected) {
    if (direction === SORTING_ORDER.DESC) {
      return current <= expected;
    }
    return current >= expected;
  };
  var genLevelComparison = function(levelComparison) {
    if (typeof levelComparison === "string") {
      return compareLevel.bind(null, levelComparison);
    }
    return levelComparison;
  };
  var mappings = function(customLevels = null, useOnlyCustomLevels = false) {
    const customNums = customLevels ? Object.keys(customLevels).reduce((o, k) => {
      o[customLevels[k]] = k;
      return o;
    }, {}) : null;
    const labels = Object.assign(Object.create(Object.prototype, { Infinity: { value: "silent" } }), useOnlyCustomLevels ? null : nums, customNums);
    const values = Object.assign(Object.create(Object.prototype, { silent: { value: Infinity } }), useOnlyCustomLevels ? null : DEFAULT_LEVELS, customLevels);
    return { labels, values };
  };
  var assertDefaultLevelFound = function(defaultLevel, customLevels, useOnlyCustomLevels) {
    if (typeof defaultLevel === "number") {
      const values = [].concat(Object.keys(customLevels || {}).map((key) => customLevels[key]), useOnlyCustomLevels ? [] : Object.keys(nums).map((level) => +level), Infinity);
      if (!values.includes(defaultLevel)) {
        throw Error(`default level:${defaultLevel} must be included in custom levels`);
      }
      return;
    }
    const labels = Object.assign(Object.create(Object.prototype, { silent: { value: Infinity } }), useOnlyCustomLevels ? null : DEFAULT_LEVELS, customLevels);
    if (!(defaultLevel in labels)) {
      throw Error(`default level:${defaultLevel} must be included in custom levels`);
    }
  };
  var assertNoLevelCollisions = function(levels, customLevels) {
    const { labels, values } = levels;
    for (const k in customLevels) {
      if (k in values) {
        throw Error("levels cannot be overridden");
      }
      if (customLevels[k] in labels) {
        throw Error("pre-existing level values cannot be used for new levels");
      }
    }
  };
  var assertLevelComparison = function(levelComparison) {
    if (typeof levelComparison === "function") {
      return;
    }
    if (typeof levelComparison === "string" && Object.values(SORTING_ORDER).includes(levelComparison)) {
      return;
    }
    throw new Error('Levels comparison should be one of "ASC", "DESC" or "function" type');
  };
  var {
    lsCacheSym,
    levelValSym,
    useOnlyCustomLevelsSym,
    streamSym,
    formattersSym,
    hooksSym,
    levelCompSym
  } = require_symbols();
  var { noop, genLog } = require_tools();
  var { DEFAULT_LEVELS, SORTING_ORDER } = require_constants();
  var levelMethods = {
    fatal: (hook) => {
      const logFatal = genLog(DEFAULT_LEVELS.fatal, hook);
      return function(...args) {
        const stream = this[streamSym];
        logFatal.call(this, ...args);
        if (typeof stream.flushSync === "function") {
          try {
            stream.flushSync();
          } catch (e) {
          }
        }
      };
    },
    error: (hook) => genLog(DEFAULT_LEVELS.error, hook),
    warn: (hook) => genLog(DEFAULT_LEVELS.warn, hook),
    info: (hook) => genLog(DEFAULT_LEVELS.info, hook),
    debug: (hook) => genLog(DEFAULT_LEVELS.debug, hook),
    trace: (hook) => genLog(DEFAULT_LEVELS.trace, hook)
  };
  var nums = Object.keys(DEFAULT_LEVELS).reduce((o, k) => {
    o[DEFAULT_LEVELS[k]] = k;
    return o;
  }, {});
  var initialLsCache = Object.keys(nums).reduce((o, k) => {
    o[k] = '{"level":' + Number(k);
    return o;
  }, {});
  module.exports = {
    initialLsCache,
    genLsCache,
    levelMethods,
    getLevel,
    setLevel,
    isLevelEnabled,
    mappings,
    assertNoLevelCollisions,
    assertDefaultLevelFound,
    genLevelComparison,
    assertLevelComparison
  };
});

// node_modules/pino/lib/meta.js
var require_meta = __commonJS((exports, module) => {
  module.exports = { version: "8.19.0" };
});

// node_modules/pino/lib/proto.js
var require_proto = __commonJS((exports, module) => {
  var child = function(bindings2, options) {
    if (!bindings2) {
      throw Error("missing bindings for child Pino");
    }
    options = options || {};
    const serializers = this[serializersSym];
    const formatters = this[formattersSym];
    const instance = Object.create(this);
    if (options.hasOwnProperty("serializers") === true) {
      instance[serializersSym] = Object.create(null);
      for (const k in serializers) {
        instance[serializersSym][k] = serializers[k];
      }
      const parentSymbols = Object.getOwnPropertySymbols(serializers);
      for (var i = 0;i < parentSymbols.length; i++) {
        const ks = parentSymbols[i];
        instance[serializersSym][ks] = serializers[ks];
      }
      for (const bk in options.serializers) {
        instance[serializersSym][bk] = options.serializers[bk];
      }
      const bindingsSymbols = Object.getOwnPropertySymbols(options.serializers);
      for (var bi = 0;bi < bindingsSymbols.length; bi++) {
        const bks = bindingsSymbols[bi];
        instance[serializersSym][bks] = options.serializers[bks];
      }
    } else
      instance[serializersSym] = serializers;
    if (options.hasOwnProperty("formatters")) {
      const { level, bindings: chindings, log } = options.formatters;
      instance[formattersSym] = buildFormatters(level || formatters.level, chindings || resetChildingsFormatter, log || formatters.log);
    } else {
      instance[formattersSym] = buildFormatters(formatters.level, resetChildingsFormatter, formatters.log);
    }
    if (options.hasOwnProperty("customLevels") === true) {
      assertNoLevelCollisions(this.levels, options.customLevels);
      instance.levels = mappings(options.customLevels, instance[useOnlyCustomLevelsSym]);
      genLsCache(instance);
    }
    if (typeof options.redact === "object" && options.redact !== null || Array.isArray(options.redact)) {
      instance.redact = options.redact;
      const stringifiers = redaction(instance.redact, stringify);
      const formatOpts = { stringify: stringifiers[redactFmtSym] };
      instance[stringifySym] = stringify;
      instance[stringifiersSym] = stringifiers;
      instance[formatOptsSym] = formatOpts;
    }
    if (typeof options.msgPrefix === "string") {
      instance[msgPrefixSym] = (this[msgPrefixSym] || "") + options.msgPrefix;
    }
    instance[chindingsSym] = asChindings(instance, bindings2);
    const childLevel = options.level || this.level;
    instance[setLevelSym](childLevel);
    this.onChild(instance);
    return instance;
  };
  var bindings = function() {
    const chindings = this[chindingsSym];
    const chindingsJson = `{${chindings.substr(1)}}`;
    const bindingsFromJson = JSON.parse(chindingsJson);
    delete bindingsFromJson.pid;
    delete bindingsFromJson.hostname;
    return bindingsFromJson;
  };
  var setBindings = function(newBindings) {
    const chindings = asChindings(this, newBindings);
    this[chindingsSym] = chindings;
    delete this[parsedChindingsSym];
  };
  var defaultMixinMergeStrategy = function(mergeObject, mixinObject) {
    return Object.assign(mixinObject, mergeObject);
  };
  var write = function(_obj, msg, num) {
    const t = this[timeSym]();
    const mixin = this[mixinSym];
    const errorKey = this[errorKeySym];
    const messageKey = this[messageKeySym];
    const mixinMergeStrategy = this[mixinMergeStrategySym] || defaultMixinMergeStrategy;
    let obj;
    if (_obj === undefined || _obj === null) {
      obj = {};
    } else if (_obj instanceof Error) {
      obj = { [errorKey]: _obj };
      if (msg === undefined) {
        msg = _obj.message;
      }
    } else {
      obj = _obj;
      if (msg === undefined && _obj[messageKey] === undefined && _obj[errorKey]) {
        msg = _obj[errorKey].message;
      }
    }
    if (mixin) {
      obj = mixinMergeStrategy(obj, mixin(obj, num, this));
    }
    const s = this[asJsonSym](obj, msg, num, t);
    const stream = this[streamSym];
    if (stream[needsMetadataGsym] === true) {
      stream.lastLevel = num;
      stream.lastObj = obj;
      stream.lastMsg = msg;
      stream.lastTime = t.slice(this[timeSliceIndexSym]);
      stream.lastLogger = this;
    }
    stream.write(s);
  };
  var noop = function() {
  };
  var flush = function(cb) {
    if (cb != null && typeof cb !== "function") {
      throw Error("callback must be a function");
    }
    const stream = this[streamSym];
    if (typeof stream.flush === "function") {
      stream.flush(cb || noop);
    } else if (cb)
      cb();
  };
  var { EventEmitter } = import.meta.require("events");
  var {
    lsCacheSym,
    levelValSym,
    setLevelSym,
    getLevelSym,
    chindingsSym,
    parsedChindingsSym,
    mixinSym,
    asJsonSym,
    writeSym,
    mixinMergeStrategySym,
    timeSym,
    timeSliceIndexSym,
    streamSym,
    serializersSym,
    formattersSym,
    errorKeySym,
    messageKeySym,
    useOnlyCustomLevelsSym,
    needsMetadataGsym,
    redactFmtSym,
    stringifySym,
    formatOptsSym,
    stringifiersSym,
    msgPrefixSym
  } = require_symbols();
  var {
    getLevel,
    setLevel,
    isLevelEnabled,
    mappings,
    initialLsCache,
    genLsCache,
    assertNoLevelCollisions
  } = require_levels();
  var {
    asChindings,
    asJson,
    buildFormatters,
    stringify
  } = require_tools();
  var {
    version
  } = require_meta();
  var redaction = require_redaction();
  var constructor = class Pino {
  };
  var prototype = {
    constructor,
    child,
    bindings,
    setBindings,
    flush,
    isLevelEnabled,
    version,
    get level() {
      return this[getLevelSym]();
    },
    set level(lvl) {
      this[setLevelSym](lvl);
    },
    get levelVal() {
      return this[levelValSym];
    },
    set levelVal(n) {
      throw Error("levelVal is read-only");
    },
    [lsCacheSym]: initialLsCache,
    [writeSym]: write,
    [asJsonSym]: asJson,
    [getLevelSym]: getLevel,
    [setLevelSym]: setLevel
  };
  Object.setPrototypeOf(prototype, EventEmitter.prototype);
  module.exports = function() {
    return Object.create(prototype);
  };
  var resetChildingsFormatter = (bindings2) => bindings2;
});

// node_modules/safe-stable-stringify/index.js
var require_safe_stable_stringify = __commonJS((exports, module) => {
  var strEscape = function(str) {
    if (str.length < 5000 && !strEscapeSequencesRegExp.test(str)) {
      return `"${str}"`;
    }
    return JSON.stringify(str);
  };
  var insertSort = function(array) {
    if (array.length > 200) {
      return array.sort();
    }
    for (let i = 1;i < array.length; i++) {
      const currentValue = array[i];
      let position = i;
      while (position !== 0 && array[position - 1] > currentValue) {
        array[position] = array[position - 1];
        position--;
      }
      array[position] = currentValue;
    }
    return array;
  };
  var isTypedArrayWithEntries = function(value) {
    return typedArrayPrototypeGetSymbolToStringTag.call(value) !== undefined && value.length !== 0;
  };
  var stringifyTypedArray = function(array, separator, maximumBreadth) {
    if (array.length < maximumBreadth) {
      maximumBreadth = array.length;
    }
    const whitespace = separator === "," ? "" : " ";
    let res = `"0":${whitespace}${array[0]}`;
    for (let i = 1;i < maximumBreadth; i++) {
      res += `${separator}"${i}":${whitespace}${array[i]}`;
    }
    return res;
  };
  var getCircularValueOption = function(options) {
    if (hasOwnProperty.call(options, "circularValue")) {
      const circularValue = options.circularValue;
      if (typeof circularValue === "string") {
        return `"${circularValue}"`;
      }
      if (circularValue == null) {
        return circularValue;
      }
      if (circularValue === Error || circularValue === TypeError) {
        return {
          toString() {
            throw new TypeError("Converting circular structure to JSON");
          }
        };
      }
      throw new TypeError('The "circularValue" argument must be of type string or the value null or undefined');
    }
    return '"[Circular]"';
  };
  var getBooleanOption = function(options, key) {
    let value;
    if (hasOwnProperty.call(options, key)) {
      value = options[key];
      if (typeof value !== "boolean") {
        throw new TypeError(`The "${key}" argument must be of type boolean`);
      }
    }
    return value === undefined ? true : value;
  };
  var getPositiveIntegerOption = function(options, key) {
    let value;
    if (hasOwnProperty.call(options, key)) {
      value = options[key];
      if (typeof value !== "number") {
        throw new TypeError(`The "${key}" argument must be of type number`);
      }
      if (!Number.isInteger(value)) {
        throw new TypeError(`The "${key}" argument must be an integer`);
      }
      if (value < 1) {
        throw new RangeError(`The "${key}" argument must be >= 1`);
      }
    }
    return value === undefined ? Infinity : value;
  };
  var getItemCount = function(number) {
    if (number === 1) {
      return "1 item";
    }
    return `${number} items`;
  };
  var getUniqueReplacerSet = function(replacerArray) {
    const replacerSet = new Set;
    for (const value of replacerArray) {
      if (typeof value === "string" || typeof value === "number") {
        replacerSet.add(String(value));
      }
    }
    return replacerSet;
  };
  var getStrictOption = function(options) {
    if (hasOwnProperty.call(options, "strict")) {
      const value = options.strict;
      if (typeof value !== "boolean") {
        throw new TypeError('The "strict" argument must be of type boolean');
      }
      if (value) {
        return (value2) => {
          let message = `Object can not safely be stringified. Received type ${typeof value2}`;
          if (typeof value2 !== "function")
            message += ` (${value2.toString()})`;
          throw new Error(message);
        };
      }
    }
  };
  var configure = function(options) {
    options = { ...options };
    const fail = getStrictOption(options);
    if (fail) {
      if (options.bigint === undefined) {
        options.bigint = false;
      }
      if (!("circularValue" in options)) {
        options.circularValue = Error;
      }
    }
    const circularValue = getCircularValueOption(options);
    const bigint = getBooleanOption(options, "bigint");
    const deterministic = getBooleanOption(options, "deterministic");
    const maximumDepth = getPositiveIntegerOption(options, "maximumDepth");
    const maximumBreadth = getPositiveIntegerOption(options, "maximumBreadth");
    function stringifyFnReplacer(key, parent, stack, replacer, spacer, indentation) {
      let value = parent[key];
      if (typeof value === "object" && value !== null && typeof value.toJSON === "function") {
        value = value.toJSON(key);
      }
      value = replacer.call(parent, key, value);
      switch (typeof value) {
        case "string":
          return strEscape(value);
        case "object": {
          if (value === null) {
            return "null";
          }
          if (stack.indexOf(value) !== -1) {
            return circularValue;
          }
          let res = "";
          let join2 = ",";
          const originalIndentation = indentation;
          if (Array.isArray(value)) {
            if (value.length === 0) {
              return "[]";
            }
            if (maximumDepth < stack.length + 1) {
              return '"[Array]"';
            }
            stack.push(value);
            if (spacer !== "") {
              indentation += spacer;
              res += `\n${indentation}`;
              join2 = `,\n${indentation}`;
            }
            const maximumValuesToStringify = Math.min(value.length, maximumBreadth);
            let i = 0;
            for (;i < maximumValuesToStringify - 1; i++) {
              const tmp2 = stringifyFnReplacer(String(i), value, stack, replacer, spacer, indentation);
              res += tmp2 !== undefined ? tmp2 : "null";
              res += join2;
            }
            const tmp = stringifyFnReplacer(String(i), value, stack, replacer, spacer, indentation);
            res += tmp !== undefined ? tmp : "null";
            if (value.length - 1 > maximumBreadth) {
              const removedKeys = value.length - maximumBreadth - 1;
              res += `${join2}"... ${getItemCount(removedKeys)} not stringified"`;
            }
            if (spacer !== "") {
              res += `\n${originalIndentation}`;
            }
            stack.pop();
            return `[${res}]`;
          }
          let keys = Object.keys(value);
          const keyLength = keys.length;
          if (keyLength === 0) {
            return "{}";
          }
          if (maximumDepth < stack.length + 1) {
            return '"[Object]"';
          }
          let whitespace = "";
          let separator = "";
          if (spacer !== "") {
            indentation += spacer;
            join2 = `,\n${indentation}`;
            whitespace = " ";
          }
          const maximumPropertiesToStringify = Math.min(keyLength, maximumBreadth);
          if (deterministic && !isTypedArrayWithEntries(value)) {
            keys = insertSort(keys);
          }
          stack.push(value);
          for (let i = 0;i < maximumPropertiesToStringify; i++) {
            const key2 = keys[i];
            const tmp = stringifyFnReplacer(key2, value, stack, replacer, spacer, indentation);
            if (tmp !== undefined) {
              res += `${separator}${strEscape(key2)}:${whitespace}${tmp}`;
              separator = join2;
            }
          }
          if (keyLength > maximumBreadth) {
            const removedKeys = keyLength - maximumBreadth;
            res += `${separator}"...":${whitespace}"${getItemCount(removedKeys)} not stringified"`;
            separator = join2;
          }
          if (spacer !== "" && separator.length > 1) {
            res = `\n${indentation}${res}\n${originalIndentation}`;
          }
          stack.pop();
          return `{${res}}`;
        }
        case "number":
          return isFinite(value) ? String(value) : fail ? fail(value) : "null";
        case "boolean":
          return value === true ? "true" : "false";
        case "undefined":
          return;
        case "bigint":
          if (bigint) {
            return String(value);
          }
        default:
          return fail ? fail(value) : undefined;
      }
    }
    function stringifyArrayReplacer(key, value, stack, replacer, spacer, indentation) {
      if (typeof value === "object" && value !== null && typeof value.toJSON === "function") {
        value = value.toJSON(key);
      }
      switch (typeof value) {
        case "string":
          return strEscape(value);
        case "object": {
          if (value === null) {
            return "null";
          }
          if (stack.indexOf(value) !== -1) {
            return circularValue;
          }
          const originalIndentation = indentation;
          let res = "";
          let join2 = ",";
          if (Array.isArray(value)) {
            if (value.length === 0) {
              return "[]";
            }
            if (maximumDepth < stack.length + 1) {
              return '"[Array]"';
            }
            stack.push(value);
            if (spacer !== "") {
              indentation += spacer;
              res += `\n${indentation}`;
              join2 = `,\n${indentation}`;
            }
            const maximumValuesToStringify = Math.min(value.length, maximumBreadth);
            let i = 0;
            for (;i < maximumValuesToStringify - 1; i++) {
              const tmp2 = stringifyArrayReplacer(String(i), value[i], stack, replacer, spacer, indentation);
              res += tmp2 !== undefined ? tmp2 : "null";
              res += join2;
            }
            const tmp = stringifyArrayReplacer(String(i), value[i], stack, replacer, spacer, indentation);
            res += tmp !== undefined ? tmp : "null";
            if (value.length - 1 > maximumBreadth) {
              const removedKeys = value.length - maximumBreadth - 1;
              res += `${join2}"... ${getItemCount(removedKeys)} not stringified"`;
            }
            if (spacer !== "") {
              res += `\n${originalIndentation}`;
            }
            stack.pop();
            return `[${res}]`;
          }
          stack.push(value);
          let whitespace = "";
          if (spacer !== "") {
            indentation += spacer;
            join2 = `,\n${indentation}`;
            whitespace = " ";
          }
          let separator = "";
          for (const key2 of replacer) {
            const tmp = stringifyArrayReplacer(key2, value[key2], stack, replacer, spacer, indentation);
            if (tmp !== undefined) {
              res += `${separator}${strEscape(key2)}:${whitespace}${tmp}`;
              separator = join2;
            }
          }
          if (spacer !== "" && separator.length > 1) {
            res = `\n${indentation}${res}\n${originalIndentation}`;
          }
          stack.pop();
          return `{${res}}`;
        }
        case "number":
          return isFinite(value) ? String(value) : fail ? fail(value) : "null";
        case "boolean":
          return value === true ? "true" : "false";
        case "undefined":
          return;
        case "bigint":
          if (bigint) {
            return String(value);
          }
        default:
          return fail ? fail(value) : undefined;
      }
    }
    function stringifyIndent(key, value, stack, spacer, indentation) {
      switch (typeof value) {
        case "string":
          return strEscape(value);
        case "object": {
          if (value === null) {
            return "null";
          }
          if (typeof value.toJSON === "function") {
            value = value.toJSON(key);
            if (typeof value !== "object") {
              return stringifyIndent(key, value, stack, spacer, indentation);
            }
            if (value === null) {
              return "null";
            }
          }
          if (stack.indexOf(value) !== -1) {
            return circularValue;
          }
          const originalIndentation = indentation;
          if (Array.isArray(value)) {
            if (value.length === 0) {
              return "[]";
            }
            if (maximumDepth < stack.length + 1) {
              return '"[Array]"';
            }
            stack.push(value);
            indentation += spacer;
            let res2 = `\n${indentation}`;
            const join3 = `,\n${indentation}`;
            const maximumValuesToStringify = Math.min(value.length, maximumBreadth);
            let i = 0;
            for (;i < maximumValuesToStringify - 1; i++) {
              const tmp2 = stringifyIndent(String(i), value[i], stack, spacer, indentation);
              res2 += tmp2 !== undefined ? tmp2 : "null";
              res2 += join3;
            }
            const tmp = stringifyIndent(String(i), value[i], stack, spacer, indentation);
            res2 += tmp !== undefined ? tmp : "null";
            if (value.length - 1 > maximumBreadth) {
              const removedKeys = value.length - maximumBreadth - 1;
              res2 += `${join3}"... ${getItemCount(removedKeys)} not stringified"`;
            }
            res2 += `\n${originalIndentation}`;
            stack.pop();
            return `[${res2}]`;
          }
          let keys = Object.keys(value);
          const keyLength = keys.length;
          if (keyLength === 0) {
            return "{}";
          }
          if (maximumDepth < stack.length + 1) {
            return '"[Object]"';
          }
          indentation += spacer;
          const join2 = `,\n${indentation}`;
          let res = "";
          let separator = "";
          let maximumPropertiesToStringify = Math.min(keyLength, maximumBreadth);
          if (isTypedArrayWithEntries(value)) {
            res += stringifyTypedArray(value, join2, maximumBreadth);
            keys = keys.slice(value.length);
            maximumPropertiesToStringify -= value.length;
            separator = join2;
          }
          if (deterministic) {
            keys = insertSort(keys);
          }
          stack.push(value);
          for (let i = 0;i < maximumPropertiesToStringify; i++) {
            const key2 = keys[i];
            const tmp = stringifyIndent(key2, value[key2], stack, spacer, indentation);
            if (tmp !== undefined) {
              res += `${separator}${strEscape(key2)}: ${tmp}`;
              separator = join2;
            }
          }
          if (keyLength > maximumBreadth) {
            const removedKeys = keyLength - maximumBreadth;
            res += `${separator}"...": "${getItemCount(removedKeys)} not stringified"`;
            separator = join2;
          }
          if (separator !== "") {
            res = `\n${indentation}${res}\n${originalIndentation}`;
          }
          stack.pop();
          return `{${res}}`;
        }
        case "number":
          return isFinite(value) ? String(value) : fail ? fail(value) : "null";
        case "boolean":
          return value === true ? "true" : "false";
        case "undefined":
          return;
        case "bigint":
          if (bigint) {
            return String(value);
          }
        default:
          return fail ? fail(value) : undefined;
      }
    }
    function stringifySimple(key, value, stack) {
      switch (typeof value) {
        case "string":
          return strEscape(value);
        case "object": {
          if (value === null) {
            return "null";
          }
          if (typeof value.toJSON === "function") {
            value = value.toJSON(key);
            if (typeof value !== "object") {
              return stringifySimple(key, value, stack);
            }
            if (value === null) {
              return "null";
            }
          }
          if (stack.indexOf(value) !== -1) {
            return circularValue;
          }
          let res = "";
          if (Array.isArray(value)) {
            if (value.length === 0) {
              return "[]";
            }
            if (maximumDepth < stack.length + 1) {
              return '"[Array]"';
            }
            stack.push(value);
            const maximumValuesToStringify = Math.min(value.length, maximumBreadth);
            let i = 0;
            for (;i < maximumValuesToStringify - 1; i++) {
              const tmp2 = stringifySimple(String(i), value[i], stack);
              res += tmp2 !== undefined ? tmp2 : "null";
              res += ",";
            }
            const tmp = stringifySimple(String(i), value[i], stack);
            res += tmp !== undefined ? tmp : "null";
            if (value.length - 1 > maximumBreadth) {
              const removedKeys = value.length - maximumBreadth - 1;
              res += `,"... ${getItemCount(removedKeys)} not stringified"`;
            }
            stack.pop();
            return `[${res}]`;
          }
          let keys = Object.keys(value);
          const keyLength = keys.length;
          if (keyLength === 0) {
            return "{}";
          }
          if (maximumDepth < stack.length + 1) {
            return '"[Object]"';
          }
          let separator = "";
          let maximumPropertiesToStringify = Math.min(keyLength, maximumBreadth);
          if (isTypedArrayWithEntries(value)) {
            res += stringifyTypedArray(value, ",", maximumBreadth);
            keys = keys.slice(value.length);
            maximumPropertiesToStringify -= value.length;
            separator = ",";
          }
          if (deterministic) {
            keys = insertSort(keys);
          }
          stack.push(value);
          for (let i = 0;i < maximumPropertiesToStringify; i++) {
            const key2 = keys[i];
            const tmp = stringifySimple(key2, value[key2], stack);
            if (tmp !== undefined) {
              res += `${separator}${strEscape(key2)}:${tmp}`;
              separator = ",";
            }
          }
          if (keyLength > maximumBreadth) {
            const removedKeys = keyLength - maximumBreadth;
            res += `${separator}"...":"${getItemCount(removedKeys)} not stringified"`;
          }
          stack.pop();
          return `{${res}}`;
        }
        case "number":
          return isFinite(value) ? String(value) : fail ? fail(value) : "null";
        case "boolean":
          return value === true ? "true" : "false";
        case "undefined":
          return;
        case "bigint":
          if (bigint) {
            return String(value);
          }
        default:
          return fail ? fail(value) : undefined;
      }
    }
    function stringify2(value, replacer, space) {
      if (arguments.length > 1) {
        let spacer = "";
        if (typeof space === "number") {
          spacer = " ".repeat(Math.min(space, 10));
        } else if (typeof space === "string") {
          spacer = space.slice(0, 10);
        }
        if (replacer != null) {
          if (typeof replacer === "function") {
            return stringifyFnReplacer("", { "": value }, [], replacer, spacer, "");
          }
          if (Array.isArray(replacer)) {
            return stringifyArrayReplacer("", value, [], getUniqueReplacerSet(replacer), spacer, "");
          }
        }
        if (spacer.length !== 0) {
          return stringifyIndent("", value, [], spacer, "");
        }
      }
      return stringifySimple("", value, []);
    }
    return stringify2;
  };
  var { hasOwnProperty } = Object.prototype;
  var stringify = configure();
  stringify.configure = configure;
  stringify.stringify = stringify;
  stringify.default = stringify;
  exports.stringify = stringify;
  exports.configure = configure;
  module.exports = stringify;
  var strEscapeSequencesRegExp = /[\u0000-\u001f\u0022\u005c\ud800-\udfff]|[\ud800-\udbff](?![\udc00-\udfff])|(?:[^\ud800-\udbff]|^)[\udc00-\udfff]/;
  var typedArrayPrototypeGetSymbolToStringTag = Object.getOwnPropertyDescriptor(Object.getPrototypeOf(Object.getPrototypeOf(new Int8Array)), Symbol.toStringTag).get;
});

// node_modules/pino/lib/multistream.js
var require_multistream = __commonJS((exports, module) => {
  var multistream = function(streamsArray, opts) {
    let counter = 0;
    streamsArray = streamsArray || [];
    opts = opts || { dedupe: false };
    const streamLevels = Object.create(DEFAULT_LEVELS);
    streamLevels.silent = Infinity;
    if (opts.levels && typeof opts.levels === "object") {
      Object.keys(opts.levels).forEach((i) => {
        streamLevels[i] = opts.levels[i];
      });
    }
    const res = {
      write,
      add,
      flushSync,
      end,
      minLevel: 0,
      streams: [],
      clone,
      [metadata]: true,
      streamLevels
    };
    if (Array.isArray(streamsArray)) {
      streamsArray.forEach(add, res);
    } else {
      add.call(res, streamsArray);
    }
    streamsArray = null;
    return res;
    function write(data) {
      let dest;
      const level = this.lastLevel;
      const { streams } = this;
      let recordedLevel = 0;
      let stream;
      for (let i = initLoopVar(streams.length, opts.dedupe);checkLoopVar(i, streams.length, opts.dedupe); i = adjustLoopVar(i, opts.dedupe)) {
        dest = streams[i];
        if (dest.level <= level) {
          if (recordedLevel !== 0 && recordedLevel !== dest.level) {
            break;
          }
          stream = dest.stream;
          if (stream[metadata]) {
            const { lastTime, lastMsg, lastObj, lastLogger } = this;
            stream.lastLevel = level;
            stream.lastTime = lastTime;
            stream.lastMsg = lastMsg;
            stream.lastObj = lastObj;
            stream.lastLogger = lastLogger;
          }
          stream.write(data);
          if (opts.dedupe) {
            recordedLevel = dest.level;
          }
        } else if (!opts.dedupe) {
          break;
        }
      }
    }
    function flushSync() {
      for (const { stream } of this.streams) {
        if (typeof stream.flushSync === "function") {
          stream.flushSync();
        }
      }
    }
    function add(dest) {
      if (!dest) {
        return res;
      }
      const isStream = typeof dest.write === "function" || dest.stream;
      const stream_ = dest.write ? dest : dest.stream;
      if (!isStream) {
        throw Error("stream object needs to implement either StreamEntry or DestinationStream interface");
      }
      const { streams, streamLevels: streamLevels2 } = this;
      let level;
      if (typeof dest.levelVal === "number") {
        level = dest.levelVal;
      } else if (typeof dest.level === "string") {
        level = streamLevels2[dest.level];
      } else if (typeof dest.level === "number") {
        level = dest.level;
      } else {
        level = DEFAULT_INFO_LEVEL;
      }
      const dest_ = {
        stream: stream_,
        level,
        levelVal: undefined,
        id: counter++
      };
      streams.unshift(dest_);
      streams.sort(compareByLevel);
      this.minLevel = streams[0].level;
      return res;
    }
    function end() {
      for (const { stream } of this.streams) {
        if (typeof stream.flushSync === "function") {
          stream.flushSync();
        }
        stream.end();
      }
    }
    function clone(level) {
      const streams = new Array(this.streams.length);
      for (let i = 0;i < streams.length; i++) {
        streams[i] = {
          level,
          stream: this.streams[i].stream
        };
      }
      return {
        write,
        add,
        minLevel: level,
        streams,
        clone,
        flushSync,
        [metadata]: true
      };
    }
  };
  var compareByLevel = function(a, b) {
    return a.level - b.level;
  };
  var initLoopVar = function(length, dedupe) {
    return dedupe ? length - 1 : 0;
  };
  var adjustLoopVar = function(i, dedupe) {
    return dedupe ? i - 1 : i + 1;
  };
  var checkLoopVar = function(i, length, dedupe) {
    return dedupe ? i >= 0 : i < length;
  };
  var metadata = Symbol.for("pino.metadata");
  var { DEFAULT_LEVELS } = require_constants();
  var DEFAULT_INFO_LEVEL = DEFAULT_LEVELS.info;
  module.exports = multistream;
});

// node_modules/pino/pino.js
var require_pino = __commonJS((exports, module) => {
  var pino = function(...args) {
    const instance = {};
    const { opts, stream } = normalize(instance, caller(), ...args);
    const {
      redact,
      crlf,
      serializers: serializers2,
      timestamp,
      messageKey,
      errorKey,
      nestedKey,
      base,
      name,
      level,
      customLevels,
      levelComparison,
      mixin,
      mixinMergeStrategy,
      useOnlyCustomLevels,
      formatters,
      hooks,
      depthLimit,
      edgeLimit,
      onChild,
      msgPrefix
    } = opts;
    const stringifySafe = configure({
      maximumDepth: depthLimit,
      maximumBreadth: edgeLimit
    });
    const allFormatters = buildFormatters(formatters.level, formatters.bindings, formatters.log);
    const stringifyFn = stringify.bind({
      [stringifySafeSym]: stringifySafe
    });
    const stringifiers = redact ? redaction(redact, stringifyFn) : {};
    const formatOpts = redact ? { stringify: stringifiers[redactFmtSym] } : { stringify: stringifyFn };
    const end = "}" + (crlf ? "\r\n" : "\n");
    const coreChindings = asChindings.bind(null, {
      [chindingsSym]: "",
      [serializersSym]: serializers2,
      [stringifiersSym]: stringifiers,
      [stringifySym]: stringify,
      [stringifySafeSym]: stringifySafe,
      [formattersSym]: allFormatters
    });
    let chindings = "";
    if (base !== null) {
      if (name === undefined) {
        chindings = coreChindings(base);
      } else {
        chindings = coreChindings(Object.assign({}, base, { name }));
      }
    }
    const time2 = timestamp instanceof Function ? timestamp : timestamp ? epochTime : nullTime;
    const timeSliceIndex = time2().indexOf(":") + 1;
    if (useOnlyCustomLevels && !customLevels)
      throw Error("customLevels is required if useOnlyCustomLevels is set true");
    if (mixin && typeof mixin !== "function")
      throw Error(`Unknown mixin type "${typeof mixin}" - expected "function"`);
    if (msgPrefix && typeof msgPrefix !== "string")
      throw Error(`Unknown msgPrefix type "${typeof msgPrefix}" - expected "string"`);
    assertDefaultLevelFound(level, customLevels, useOnlyCustomLevels);
    const levels = mappings(customLevels, useOnlyCustomLevels);
    assertLevelComparison(levelComparison);
    const levelCompFunc = genLevelComparison(levelComparison);
    Object.assign(instance, {
      levels,
      [levelCompSym]: levelCompFunc,
      [useOnlyCustomLevelsSym]: useOnlyCustomLevels,
      [streamSym]: stream,
      [timeSym]: time2,
      [timeSliceIndexSym]: timeSliceIndex,
      [stringifySym]: stringify,
      [stringifySafeSym]: stringifySafe,
      [stringifiersSym]: stringifiers,
      [endSym]: end,
      [formatOptsSym]: formatOpts,
      [messageKeySym]: messageKey,
      [errorKeySym]: errorKey,
      [nestedKeySym]: nestedKey,
      [nestedKeyStrSym]: nestedKey ? `,${JSON.stringify(nestedKey)}:{` : "",
      [serializersSym]: serializers2,
      [mixinSym]: mixin,
      [mixinMergeStrategySym]: mixinMergeStrategy,
      [chindingsSym]: chindings,
      [formattersSym]: allFormatters,
      [hooksSym]: hooks,
      silent: noop,
      onChild,
      [msgPrefixSym]: msgPrefix
    });
    Object.setPrototypeOf(instance, proto());
    genLsCache(instance);
    instance[setLevelSym](level);
    return instance;
  };
  var os = import.meta.require("os");
  var stdSerializers = require_pino_std_serializers();
  var caller = require_caller();
  var redaction = require_redaction();
  var time = require_time();
  var proto = require_proto();
  var symbols = require_symbols();
  var { configure } = require_safe_stable_stringify();
  var { assertDefaultLevelFound, mappings, genLsCache, genLevelComparison, assertLevelComparison } = require_levels();
  var { DEFAULT_LEVELS, SORTING_ORDER } = require_constants();
  var {
    createArgsNormalizer,
    asChindings,
    buildSafeSonicBoom,
    buildFormatters,
    stringify,
    normalizeDestFileDescriptor,
    noop
  } = require_tools();
  var { version } = require_meta();
  var {
    chindingsSym,
    redactFmtSym,
    serializersSym,
    timeSym,
    timeSliceIndexSym,
    streamSym,
    stringifySym,
    stringifySafeSym,
    stringifiersSym,
    setLevelSym,
    endSym,
    formatOptsSym,
    messageKeySym,
    errorKeySym,
    nestedKeySym,
    mixinSym,
    levelCompSym,
    useOnlyCustomLevelsSym,
    formattersSym,
    hooksSym,
    nestedKeyStrSym,
    mixinMergeStrategySym,
    msgPrefixSym
  } = symbols;
  var { epochTime, nullTime } = time;
  var { pid } = process;
  var hostname = os.hostname();
  var defaultErrorSerializer = stdSerializers.err;
  var defaultOptions = {
    level: "info",
    levelComparison: SORTING_ORDER.ASC,
    levels: DEFAULT_LEVELS,
    messageKey: "msg",
    errorKey: "err",
    nestedKey: null,
    enabled: true,
    base: { pid, hostname },
    serializers: Object.assign(Object.create(null), {
      err: defaultErrorSerializer
    }),
    formatters: Object.assign(Object.create(null), {
      bindings(bindings) {
        return bindings;
      },
      level(label, number) {
        return { level: number };
      }
    }),
    hooks: {
      logMethod: undefined
    },
    timestamp: epochTime,
    name: undefined,
    redact: null,
    customLevels: null,
    useOnlyCustomLevels: false,
    depthLimit: 5,
    edgeLimit: 100
  };
  var normalize = createArgsNormalizer(defaultOptions);
  var serializers = Object.assign(Object.create(null), stdSerializers);
  module.exports = pino;
  module.exports.destination = (dest = process.stdout.fd) => {
    if (typeof dest === "object") {
      dest.dest = normalizeDestFileDescriptor(dest.dest || process.stdout.fd);
      return buildSafeSonicBoom(dest);
    } else {
      return buildSafeSonicBoom({ dest: normalizeDestFileDescriptor(dest), minLength: 0 });
    }
  };
  module.exports.transport = require_transport();
  module.exports.multistream = require_multistream();
  module.exports.levels = mappings();
  module.exports.stdSerializers = serializers;
  module.exports.stdTimeFunctions = Object.assign({}, time);
  module.exports.symbols = symbols;
  module.exports.version = version;
  module.exports.default = pino;
  module.exports.pino = pino;
});

// node_modules/kafkajs/src/loggers/index.js
var require_loggers = __commonJS((exports, module) => {
  var { assign } = Object;
  var LEVELS = {
    NOTHING: 0,
    ERROR: 1,
    WARN: 2,
    INFO: 4,
    DEBUG: 5
  };
  var createLevel = (label, level, currentLevel, namespace, logFunction) => (message, extra = {}) => {
    if (level > currentLevel())
      return;
    logFunction({
      namespace,
      level,
      label,
      log: assign({
        timestamp: new Date().toISOString(),
        logger: "kafkajs",
        message
      }, extra)
    });
  };
  var evaluateLogLevel = (logLevel) => {
    const envLogLevel = (process.env.KAFKAJS_LOG_LEVEL || "").toUpperCase();
    return LEVELS[envLogLevel] == null ? logLevel : LEVELS[envLogLevel];
  };
  var createLogger = ({ level = LEVELS.INFO, logCreator } = {}) => {
    let logLevel = evaluateLogLevel(level);
    const logFunction = logCreator(logLevel);
    const createNamespace = (namespace, logLevel2 = null) => {
      const namespaceLogLevel = evaluateLogLevel(logLevel2);
      return createLogFunctions(namespace, namespaceLogLevel);
    };
    const createLogFunctions = (namespace, namespaceLogLevel = null) => {
      const currentLogLevel = () => namespaceLogLevel == null ? logLevel : namespaceLogLevel;
      const logger = {
        info: createLevel("INFO", LEVELS.INFO, currentLogLevel, namespace, logFunction),
        error: createLevel("ERROR", LEVELS.ERROR, currentLogLevel, namespace, logFunction),
        warn: createLevel("WARN", LEVELS.WARN, currentLogLevel, namespace, logFunction),
        debug: createLevel("DEBUG", LEVELS.DEBUG, currentLogLevel, namespace, logFunction)
      };
      return assign(logger, {
        namespace: createNamespace,
        setLogLevel: (newLevel) => {
          logLevel = newLevel;
        }
      });
    };
    return createLogFunctions();
  };
  module.exports = {
    LEVELS,
    createLogger
  };
});

// node_modules/kafkajs/src/instrumentation/event.js
var require_event = __commonJS((exports, module) => {
  var id = 0;
  var nextId = () => {
    if (id === Number.MAX_VALUE) {
      id = 0;
    }
    return id++;
  };

  class InstrumentationEvent {
    constructor(type, payload) {
      this.id = nextId();
      this.type = type;
      this.timestamp = Date.now();
      this.payload = payload;
    }
  }
  module.exports = InstrumentationEvent;
});

// node_modules/kafkajs/package.json
var require_package2 = __commonJS((exports, module) => {
  module.exports = {
    name: "kafkajs",
    version: "2.2.4",
    description: "A modern Apache Kafka client for node.js",
    author: "Tulio Ornelas <ornelas.tulio@gmail.com>",
    main: "index.js",
    types: "types/index.d.ts",
    license: "MIT",
    keywords: [
      "kafka",
      "sasl",
      "scram"
    ],
    engines: {
      node: ">=14.0.0"
    },
    repository: {
      type: "git",
      url: "https://github.com/tulios/kafkajs.git"
    },
    bugs: {
      url: "https://github.com/tulios/kafkajs/issues"
    },
    homepage: "https://kafka.js.org",
    scripts: {
      jest: "export KAFKA_VERSION=${KAFKA_VERSION:='2.4'} && NODE_ENV=test echo \"KAFKA_VERSION: ${KAFKA_VERSION}\" && KAFKAJS_DEBUG_PROTOCOL_BUFFERS=1 jest",
      "test:local": "yarn jest --detectOpenHandles",
      "test:debug": "NODE_ENV=test KAFKAJS_DEBUG_PROTOCOL_BUFFERS=1 node --inspect-brk $(yarn bin 2>/dev/null)/jest --detectOpenHandles --runInBand --watch",
      "test:local:watch": "yarn test:local --watch",
      test: "yarn lint && JEST_JUNIT_OUTPUT_NAME=test-report.xml ./scripts/testWithKafka.sh 'yarn jest --ci --maxWorkers=4 --no-watchman --forceExit'",
      lint: "find . -path ./node_modules -prune -o -path ./coverage -prune -o -path ./website -prune -o -name '*.js' -print0 | xargs -0 eslint",
      format: "find . -path ./node_modules -prune -o -path ./coverage -prune -o -path ./website -prune -o -name '*.js' -print0 | xargs -0 prettier --write",
      precommit: "lint-staged",
      "test:group:broker": "yarn jest --forceExit --testPathPattern 'src/broker/.*'",
      "test:group:admin": "yarn jest --forceExit --testPathPattern 'src/admin/.*'",
      "test:group:producer": "yarn jest --forceExit --testPathPattern 'src/producer/.*'",
      "test:group:consumer": "yarn jest --forceExit --testPathPattern 'src/consumer/.*.spec.js'",
      "test:group:others": "yarn jest --forceExit --testPathPattern 'src/(?!(broker|admin|producer|consumer)/).*'",
      "test:group:oauthbearer": "OAUTHBEARER_ENABLED=1 yarn jest --forceExit src/producer/index.spec.js src/broker/__tests__/connect.spec.js src/consumer/__tests__/connection.spec.js src/broker/__tests__/disconnect.spec.js src/admin/__tests__/connection.spec.js src/broker/__tests__/reauthenticate.spec.js",
      "test:group:broker:ci": "JEST_JUNIT_OUTPUT_NAME=test-report.xml ./scripts/testWithKafka.sh \"yarn test:group:broker --ci --maxWorkers=4 --no-watchman\"",
      "test:group:admin:ci": "JEST_JUNIT_OUTPUT_NAME=test-report.xml ./scripts/testWithKafka.sh \"yarn test:group:admin --ci --maxWorkers=4 --no-watchman\"",
      "test:group:producer:ci": "JEST_JUNIT_OUTPUT_NAME=test-report.xml ./scripts/testWithKafka.sh \"yarn test:group:producer --ci --maxWorkers=4 --no-watchman\"",
      "test:group:consumer:ci": "JEST_JUNIT_OUTPUT_NAME=test-report.xml ./scripts/testWithKafka.sh \"yarn test:group:consumer --ci --maxWorkers=4 --no-watchman\"",
      "test:group:others:ci": "JEST_JUNIT_OUTPUT_NAME=test-report.xml ./scripts/testWithKafka.sh \"yarn test:group:others --ci --maxWorkers=4 --no-watchman\"",
      "test:group:oauthbearer:ci": "JEST_JUNIT_OUTPUT_NAME=test-report.xml COMPOSE_FILE='docker-compose.2_4_oauthbearer.yml' ./scripts/testWithKafka.sh \"yarn test:group:oauthbearer --ci --maxWorkers=4 --no-watchman\"",
      "test:types": "tsc -p types/"
    },
    devDependencies: {
      "@types/jest": "^27.4.0",
      "@types/node": "^12.0.8",
      "@typescript-eslint/typescript-estree": "^1.10.2",
      eslint: "^6.8.0",
      "eslint-config-prettier": "^6.0.0",
      "eslint-config-standard": "^13.0.1",
      "eslint-plugin-import": "^2.18.2",
      "eslint-plugin-jest": "^26.1.0",
      "eslint-plugin-node": "^11.0.0",
      "eslint-plugin-prettier": "^3.1.0",
      "eslint-plugin-promise": "^4.2.1",
      "eslint-plugin-standard": "^4.0.0",
      execa: "^2.0.3",
      glob: "^7.1.4",
      husky: "^3.0.1",
      ip: "^1.1.5",
      jest: "^25.1.0",
      "jest-circus": "^25.1.0",
      "jest-extended": "^0.11.2",
      "jest-junit": "^10.0.0",
      jsonwebtoken: "^9.0.0",
      "lint-staged": "^9.2.0",
      mockdate: "^2.0.5",
      prettier: "^1.18.2",
      semver: "^6.2.0",
      typescript: "^3.8.3",
      uuid: "^3.3.2"
    },
    dependencies: {},
    "lint-staged": {
      "*.js": [
        "prettier --write",
        "git add"
      ]
    }
  };
});

// node_modules/kafkajs/src/errors.js
var require_errors = __commonJS((exports, module) => {
  var pkgJson = require_package2();
  var { bugs } = pkgJson;

  class KafkaJSError extends Error {
    constructor(e, { retriable = true, cause } = {}) {
      super(e, { cause });
      Error.captureStackTrace(this, this.constructor);
      this.message = e.message || e;
      this.name = "KafkaJSError";
      this.retriable = retriable;
      this.helpUrl = e.helpUrl;
      this.cause = cause;
    }
  }

  class KafkaJSNonRetriableError extends KafkaJSError {
    constructor(e, { cause } = {}) {
      super(e, { retriable: false, cause });
      this.name = "KafkaJSNonRetriableError";
    }
  }

  class KafkaJSProtocolError extends KafkaJSError {
    constructor(e, { retriable = e.retriable } = {}) {
      super(e, { retriable });
      this.type = e.type;
      this.code = e.code;
      this.name = "KafkaJSProtocolError";
    }
  }

  class KafkaJSOffsetOutOfRange extends KafkaJSProtocolError {
    constructor(e, { topic, partition }) {
      super(e);
      this.topic = topic;
      this.partition = partition;
      this.name = "KafkaJSOffsetOutOfRange";
    }
  }

  class KafkaJSMemberIdRequired extends KafkaJSProtocolError {
    constructor(e, { memberId }) {
      super(e);
      this.memberId = memberId;
      this.name = "KafkaJSMemberIdRequired";
    }
  }

  class KafkaJSNumberOfRetriesExceeded extends KafkaJSNonRetriableError {
    constructor(e, { retryCount, retryTime }) {
      super(e, { cause: e });
      this.stack = `${this.name}\n  Caused by: ${e.stack}`;
      this.retryCount = retryCount;
      this.retryTime = retryTime;
      this.name = "KafkaJSNumberOfRetriesExceeded";
    }
  }

  class KafkaJSConnectionError extends KafkaJSError {
    constructor(e, { broker, code } = {}) {
      super(e);
      this.broker = broker;
      this.code = code;
      this.name = "KafkaJSConnectionError";
    }
  }

  class KafkaJSConnectionClosedError extends KafkaJSConnectionError {
    constructor(e, { host, port } = {}) {
      super(e, { broker: `${host}:${port}` });
      this.host = host;
      this.port = port;
      this.name = "KafkaJSConnectionClosedError";
    }
  }

  class KafkaJSRequestTimeoutError extends KafkaJSError {
    constructor(e, { broker, correlationId, createdAt, sentAt, pendingDuration } = {}) {
      super(e);
      this.broker = broker;
      this.correlationId = correlationId;
      this.createdAt = createdAt;
      this.sentAt = sentAt;
      this.pendingDuration = pendingDuration;
      this.name = "KafkaJSRequestTimeoutError";
    }
  }

  class KafkaJSMetadataNotLoaded extends KafkaJSError {
    constructor() {
      super(...arguments);
      this.name = "KafkaJSMetadataNotLoaded";
    }
  }

  class KafkaJSTopicMetadataNotLoaded extends KafkaJSMetadataNotLoaded {
    constructor(e, { topic } = {}) {
      super(e);
      this.topic = topic;
      this.name = "KafkaJSTopicMetadataNotLoaded";
    }
  }

  class KafkaJSStaleTopicMetadataAssignment extends KafkaJSError {
    constructor(e, { topic, unknownPartitions } = {}) {
      super(e);
      this.topic = topic;
      this.unknownPartitions = unknownPartitions;
      this.name = "KafkaJSStaleTopicMetadataAssignment";
    }
  }

  class KafkaJSDeleteGroupsError extends KafkaJSError {
    constructor(e, groups = []) {
      super(e);
      this.groups = groups;
      this.name = "KafkaJSDeleteGroupsError";
    }
  }

  class KafkaJSServerDoesNotSupportApiKey extends KafkaJSNonRetriableError {
    constructor(e, { apiKey, apiName } = {}) {
      super(e);
      this.apiKey = apiKey;
      this.apiName = apiName;
      this.name = "KafkaJSServerDoesNotSupportApiKey";
    }
  }

  class KafkaJSBrokerNotFound extends KafkaJSError {
    constructor() {
      super(...arguments);
      this.name = "KafkaJSBrokerNotFound";
    }
  }

  class KafkaJSPartialMessageError extends KafkaJSNonRetriableError {
    constructor() {
      super(...arguments);
      this.name = "KafkaJSPartialMessageError";
    }
  }

  class KafkaJSSASLAuthenticationError extends KafkaJSNonRetriableError {
    constructor() {
      super(...arguments);
      this.name = "KafkaJSSASLAuthenticationError";
    }
  }

  class KafkaJSGroupCoordinatorNotFound extends KafkaJSNonRetriableError {
    constructor() {
      super(...arguments);
      this.name = "KafkaJSGroupCoordinatorNotFound";
    }
  }

  class KafkaJSNotImplemented extends KafkaJSNonRetriableError {
    constructor() {
      super(...arguments);
      this.name = "KafkaJSNotImplemented";
    }
  }

  class KafkaJSTimeout extends KafkaJSNonRetriableError {
    constructor() {
      super(...arguments);
      this.name = "KafkaJSTimeout";
    }
  }

  class KafkaJSLockTimeout extends KafkaJSTimeout {
    constructor() {
      super(...arguments);
      this.name = "KafkaJSLockTimeout";
    }
  }

  class KafkaJSUnsupportedMagicByteInMessageSet extends KafkaJSNonRetriableError {
    constructor() {
      super(...arguments);
      this.name = "KafkaJSUnsupportedMagicByteInMessageSet";
    }
  }

  class KafkaJSDeleteTopicRecordsError extends KafkaJSError {
    constructor({ partitions }) {
      const retriable = partitions.filter(({ error }) => error != null).every(({ error }) => error.retriable === true);
      super("Error while deleting records", { retriable });
      this.name = "KafkaJSDeleteTopicRecordsError";
      this.partitions = partitions;
    }
  }
  var issueUrl = bugs ? bugs.url : null;

  class KafkaJSInvariantViolation extends KafkaJSNonRetriableError {
    constructor(e) {
      const message = e.message || e;
      super(`Invariant violated: ${message}. This is likely a bug and should be reported.`);
      this.name = "KafkaJSInvariantViolation";
      if (issueUrl !== null) {
        const issueTitle = encodeURIComponent(`Invariant violation: ${message}`);
        this.helpUrl = `${issueUrl}/new?assignees=&labels=bug&template=bug_report.md&title=${issueTitle}`;
      }
    }
  }

  class KafkaJSInvalidVarIntError extends KafkaJSNonRetriableError {
    constructor() {
      super(...arguments);
      this.name = "KafkaJSNonRetriableError";
    }
  }

  class KafkaJSInvalidLongError extends KafkaJSNonRetriableError {
    constructor() {
      super(...arguments);
      this.name = "KafkaJSNonRetriableError";
    }
  }

  class KafkaJSCreateTopicError extends KafkaJSProtocolError {
    constructor(e, topicName) {
      super(e);
      this.topic = topicName;
      this.name = "KafkaJSCreateTopicError";
    }
  }

  class KafkaJSAlterPartitionReassignmentsError extends KafkaJSProtocolError {
    constructor(e, topicName, partition) {
      super(e);
      this.topic = topicName;
      this.partition = partition;
      this.name = "KafkaJSAlterPartitionReassignmentsError";
    }
  }

  class KafkaJSAggregateError extends Error {
    constructor(message, errors) {
      super(message);
      this.errors = errors;
      this.name = "KafkaJSAggregateError";
    }
  }

  class KafkaJSFetcherRebalanceError extends Error {
  }

  class KafkaJSNoBrokerAvailableError extends KafkaJSError {
    constructor() {
      super("No broker available");
      this.name = "KafkaJSNoBrokerAvailableError";
    }
  }
  var isRebalancing = (e) => e.type === "REBALANCE_IN_PROGRESS" || e.type === "NOT_COORDINATOR_FOR_GROUP" || e.type === "ILLEGAL_GENERATION";
  var isKafkaJSError = (e) => e instanceof KafkaJSError;
  module.exports = {
    KafkaJSError,
    KafkaJSNonRetriableError,
    KafkaJSPartialMessageError,
    KafkaJSBrokerNotFound,
    KafkaJSProtocolError,
    KafkaJSConnectionError,
    KafkaJSConnectionClosedError,
    KafkaJSRequestTimeoutError,
    KafkaJSSASLAuthenticationError,
    KafkaJSNumberOfRetriesExceeded,
    KafkaJSOffsetOutOfRange,
    KafkaJSMemberIdRequired,
    KafkaJSGroupCoordinatorNotFound,
    KafkaJSNotImplemented,
    KafkaJSMetadataNotLoaded,
    KafkaJSTopicMetadataNotLoaded,
    KafkaJSStaleTopicMetadataAssignment,
    KafkaJSDeleteGroupsError,
    KafkaJSTimeout,
    KafkaJSLockTimeout,
    KafkaJSServerDoesNotSupportApiKey,
    KafkaJSUnsupportedMagicByteInMessageSet,
    KafkaJSDeleteTopicRecordsError,
    KafkaJSInvariantViolation,
    KafkaJSInvalidVarIntError,
    KafkaJSInvalidLongError,
    KafkaJSCreateTopicError,
    KafkaJSAggregateError,
    KafkaJSFetcherRebalanceError,
    KafkaJSNoBrokerAvailableError,
    KafkaJSAlterPartitionReassignmentsError,
    isRebalancing,
    isKafkaJSError
  };
});

// node_modules/kafkajs/src/instrumentation/emitter.js
var require_emitter = __commonJS((exports, module) => {
  var { EventEmitter } = import.meta.require("events");
  var InstrumentationEvent = require_event();
  var { KafkaJSError } = require_errors();
  module.exports = class InstrumentationEventEmitter {
    constructor() {
      this.emitter = new EventEmitter;
    }
    emit(eventName, payload) {
      if (!eventName) {
        throw new KafkaJSError("Invalid event name", { retriable: false });
      }
      if (this.emitter.listenerCount(eventName) > 0) {
        const event = new InstrumentationEvent(eventName, payload);
        this.emitter.emit(eventName, event);
      }
    }
    addListener(eventName, listener) {
      this.emitter.addListener(eventName, listener);
      return () => this.emitter.removeListener(eventName, listener);
    }
  };
});

// node_modules/kafkajs/src/loggers/console.js
var require_console = __commonJS((exports, module) => {
  var { LEVELS: logLevel } = require_loggers();
  module.exports = () => ({ namespace, level, label, log }) => {
    const prefix = namespace ? `[${namespace}] ` : "";
    const message = JSON.stringify(Object.assign({ level: label }, log, {
      message: `${prefix}${log.message}`
    }));
    switch (level) {
      case logLevel.INFO:
        return console.info(message);
      case logLevel.ERROR:
        return console.error(message);
      case logLevel.WARN:
        return console.warn(message);
      case logLevel.DEBUG:
        return console.log(message);
    }
  };
});

// node_modules/kafkajs/src/utils/lock.js
var require_lock = __commonJS((exports, module) => {
  var { format } = import.meta.require("util");
  var { KafkaJSLockTimeout } = require_errors();
  var PRIVATE = {
    LOCKED: Symbol("private:Lock:locked"),
    TIMEOUT: Symbol("private:Lock:timeout"),
    WAITING: Symbol("private:Lock:waiting"),
    TIMEOUT_ERROR_MESSAGE: Symbol("private:Lock:timeoutErrorMessage")
  };
  var TIMEOUT_MESSAGE = "Timeout while acquiring lock (%d waiting locks)";
  module.exports = class Lock {
    constructor({ timeout, description = null } = {}) {
      if (typeof timeout !== "number") {
        throw new TypeError(`'timeout' is not a number, received '${typeof timeout}'`);
      }
      this[PRIVATE.LOCKED] = false;
      this[PRIVATE.TIMEOUT] = timeout;
      this[PRIVATE.WAITING] = new Set;
      this[PRIVATE.TIMEOUT_ERROR_MESSAGE] = () => {
        const timeoutMessage = format(TIMEOUT_MESSAGE, this[PRIVATE.WAITING].size);
        return description ? `${timeoutMessage}: "${description}"` : timeoutMessage;
      };
    }
    async acquire() {
      return new Promise((resolve, reject) => {
        if (!this[PRIVATE.LOCKED]) {
          this[PRIVATE.LOCKED] = true;
          return resolve();
        }
        let timeoutId = null;
        const tryToAcquire = async () => {
          if (!this[PRIVATE.LOCKED]) {
            this[PRIVATE.LOCKED] = true;
            clearTimeout(timeoutId);
            this[PRIVATE.WAITING].delete(tryToAcquire);
            return resolve();
          }
        };
        this[PRIVATE.WAITING].add(tryToAcquire);
        timeoutId = setTimeout(() => {
          const error = new KafkaJSLockTimeout(this[PRIVATE.TIMEOUT_ERROR_MESSAGE]());
          this[PRIVATE.WAITING].delete(tryToAcquire);
          reject(error);
        }, this[PRIVATE.TIMEOUT]);
      });
    }
    async release() {
      this[PRIVATE.LOCKED] = false;
      const waitingLock = this[PRIVATE.WAITING].values().next().value;
      if (waitingLock) {
        return waitingLock();
      }
    }
  };
});

// node_modules/kafkajs/src/protocol/message/compression/gzip.js
var require_gzip = __commonJS((exports, module) => {
  var { promisify } = import.meta.require("util");
  var zlib = import.meta.require("zlib");
  var gzip = promisify(zlib.gzip);
  var unzip = promisify(zlib.unzip);
  module.exports = {
    async compress(encoder) {
      return await gzip(encoder.buffer);
    },
    async decompress(buffer) {
      return await unzip(buffer);
    }
  };
});

// node_modules/kafkajs/src/protocol/message/compression/index.js
var require_compression = __commonJS((exports, module) => {
  var { KafkaJSNotImplemented } = require_errors();
  var COMPRESSION_CODEC_MASK = 7;
  var Types = {
    None: 0,
    GZIP: 1,
    Snappy: 2,
    LZ4: 3,
    ZSTD: 4
  };
  var Codecs = {
    [Types.GZIP]: () => require_gzip(),
    [Types.Snappy]: () => {
      throw new KafkaJSNotImplemented("Snappy compression not implemented");
    },
    [Types.LZ4]: () => {
      throw new KafkaJSNotImplemented("LZ4 compression not implemented");
    },
    [Types.ZSTD]: () => {
      throw new KafkaJSNotImplemented("ZSTD compression not implemented");
    }
  };
  var lookupCodec = (type) => Codecs[type] ? Codecs[type]() : null;
  var lookupCodecByAttributes = (attributes) => {
    const codec = Codecs[attributes & COMPRESSION_CODEC_MASK];
    return codec ? codec() : null;
  };
  module.exports = {
    Types,
    Codecs,
    lookupCodec,
    lookupCodecByAttributes,
    COMPRESSION_CODEC_MASK
  };
});

// node_modules/kafkajs/src/protocol/requests/apiKeys.js
var require_apiKeys = __commonJS((exports, module) => {
  module.exports = {
    Produce: 0,
    Fetch: 1,
    ListOffsets: 2,
    Metadata: 3,
    LeaderAndIsr: 4,
    StopReplica: 5,
    UpdateMetadata: 6,
    ControlledShutdown: 7,
    OffsetCommit: 8,
    OffsetFetch: 9,
    GroupCoordinator: 10,
    JoinGroup: 11,
    Heartbeat: 12,
    LeaveGroup: 13,
    SyncGroup: 14,
    DescribeGroups: 15,
    ListGroups: 16,
    SaslHandshake: 17,
    ApiVersions: 18,
    CreateTopics: 19,
    DeleteTopics: 20,
    DeleteRecords: 21,
    InitProducerId: 22,
    OffsetForLeaderEpoch: 23,
    AddPartitionsToTxn: 24,
    AddOffsetsToTxn: 25,
    EndTxn: 26,
    WriteTxnMarkers: 27,
    TxnOffsetCommit: 28,
    DescribeAcls: 29,
    CreateAcls: 30,
    DeleteAcls: 31,
    DescribeConfigs: 32,
    AlterConfigs: 33,
    AlterReplicaLogDirs: 34,
    DescribeLogDirs: 35,
    SaslAuthenticate: 36,
    CreatePartitions: 37,
    CreateDelegationToken: 38,
    RenewDelegationToken: 39,
    ExpireDelegationToken: 40,
    DescribeDelegationToken: 41,
    DeleteGroups: 42,
    ElectPreferredLeaders: 43,
    IncrementalAlterConfigs: 44,
    AlterPartitionReassignments: 45,
    ListPartitionReassignments: 46
  };
});

// node_modules/kafkajs/src/utils/long.js
var require_long = __commonJS((exports, module) => {
  class Long {
    constructor(value) {
      this.value = value;
    }
    static isLong(obj) {
      return typeof obj.value === "bigint";
    }
    static fromBits(value) {
      return new Long(BigInt(value));
    }
    static fromInt(value) {
      if (isNaN(value))
        return Long.ZERO;
      return new Long(BigInt.asIntN(64, BigInt(value)));
    }
    static fromNumber(value) {
      if (isNaN(value))
        return Long.ZERO;
      return new Long(BigInt(value));
    }
    static fromValue(val) {
      if (typeof val === "number")
        return this.fromNumber(val);
      if (typeof val === "string")
        return this.fromString(val);
      if (typeof val === "bigint")
        return new Long(val);
      if (this.isLong(val))
        return new Long(BigInt(val.value));
      return new Long(BigInt(val));
    }
    static fromString(str) {
      if (str.length === 0)
        throw Error("empty string");
      if (str === "NaN" || str === "Infinity" || str === "+Infinity" || str === "-Infinity")
        return Long.ZERO;
      return new Long(BigInt(str));
    }
    isZero() {
      return this.value === BigInt(0);
    }
    isNegative() {
      return this.value < BigInt(0);
    }
    toString() {
      return String(this.value);
    }
    toNumber() {
      return Number(this.value);
    }
    toInt() {
      return Number(BigInt.asIntN(32, this.value));
    }
    toJSON() {
      return this.toString();
    }
    shiftLeft(numBits) {
      return new Long(this.value << BigInt(numBits));
    }
    shiftRight(numBits) {
      return new Long(this.value >> BigInt(numBits));
    }
    or(other) {
      if (!Long.isLong(other))
        other = Long.fromValue(other);
      return Long.fromBits(this.value | other.value);
    }
    xor(other) {
      if (!Long.isLong(other))
        other = Long.fromValue(other);
      return new Long(this.value ^ other.value);
    }
    and(other) {
      if (!Long.isLong(other))
        other = Long.fromValue(other);
      return new Long(this.value & other.value);
    }
    not() {
      return new Long(~this.value);
    }
    shiftRightUnsigned(numBits) {
      return new Long(this.value >> BigInt.asUintN(64, BigInt(numBits)));
    }
    equals(other) {
      if (!Long.isLong(other))
        other = Long.fromValue(other);
      return this.value === other.value;
    }
    greaterThanOrEqual(other) {
      if (!Long.isLong(other))
        other = Long.fromValue(other);
      return this.value >= other.value;
    }
    gte(other) {
      return this.greaterThanOrEqual(other);
    }
    notEquals(other) {
      if (!Long.isLong(other))
        other = Long.fromValue(other);
      return !this.equals(other);
    }
    add(addend) {
      if (!Long.isLong(addend))
        addend = Long.fromValue(addend);
      return new Long(this.value + addend.value);
    }
    subtract(subtrahend) {
      if (!Long.isLong(subtrahend))
        subtrahend = Long.fromValue(subtrahend);
      return this.add(subtrahend.negate());
    }
    multiply(multiplier) {
      if (this.isZero())
        return Long.ZERO;
      if (!Long.isLong(multiplier))
        multiplier = Long.fromValue(multiplier);
      return new Long(this.value * multiplier.value);
    }
    divide(divisor) {
      if (!Long.isLong(divisor))
        divisor = Long.fromValue(divisor);
      if (divisor.isZero())
        throw Error("division by zero");
      return new Long(this.value / divisor.value);
    }
    compare(other) {
      if (!Long.isLong(other))
        other = Long.fromValue(other);
      if (this.value === other.value)
        return 0;
      if (this.value > other.value)
        return 1;
      if (other.value > this.value)
        return -1;
    }
    lessThan(other) {
      if (!Long.isLong(other))
        other = Long.fromValue(other);
      return this.value < other.value;
    }
    negate() {
      if (this.equals(Long.MIN_VALUE)) {
        return Long.MIN_VALUE;
      }
      return this.not().add(Long.ONE);
    }
    getHighBits() {
      return Number(BigInt.asIntN(32, this.value >> BigInt(32)));
    }
    getLowBits() {
      return Number(BigInt.asIntN(32, this.value));
    }
  }
  Long.MIN_VALUE = new Long(BigInt("-9223372036854775808"));
  Long.MAX_VALUE = new Long(BigInt("9223372036854775807"));
  Long.ZERO = Long.fromInt(0);
  Long.ONE = Long.fromInt(1);
  module.exports = Long;
});

// node_modules/kafkajs/src/protocol/encoder.js
var require_encoder = __commonJS((exports, module) => {
  var Long = require_long();
  var INT8_SIZE = 1;
  var INT16_SIZE = 2;
  var INT32_SIZE = 4;
  var INT64_SIZE = 8;
  var DOUBLE_SIZE = 8;
  var MOST_SIGNIFICANT_BIT = 128;
  var OTHER_BITS = 127;
  var UNSIGNED_INT32_MAX_NUMBER = 4294967168;
  var UNSIGNED_INT64_MAX_NUMBER = 0xffffffffffffff80n;
  module.exports = class Encoder {
    static encodeZigZag(value) {
      return value << 1 ^ value >> 31;
    }
    static encodeZigZag64(value) {
      const longValue = Long.fromValue(value);
      return longValue.shiftLeft(1).xor(longValue.shiftRight(63));
    }
    static sizeOfVarInt(value) {
      let encodedValue = this.encodeZigZag(value);
      let bytes = 1;
      while ((encodedValue & UNSIGNED_INT32_MAX_NUMBER) !== 0) {
        bytes += 1;
        encodedValue >>>= 7;
      }
      return bytes;
    }
    static sizeOfVarLong(value) {
      let longValue = Encoder.encodeZigZag64(value);
      let bytes = 1;
      while (longValue.and(UNSIGNED_INT64_MAX_NUMBER).notEquals(Long.fromInt(0))) {
        bytes += 1;
        longValue = longValue.shiftRightUnsigned(7);
      }
      return bytes;
    }
    static sizeOfVarIntBytes(value) {
      const size = value == null ? -1 : Buffer.byteLength(value);
      if (size < 0) {
        return Encoder.sizeOfVarInt(-1);
      }
      return Encoder.sizeOfVarInt(size) + size;
    }
    static nextPowerOfTwo(value) {
      return 1 << 31 - Math.clz32(value) + 1;
    }
    constructor(initialSize = 511) {
      this.buf = Buffer.alloc(Encoder.nextPowerOfTwo(initialSize));
      this.offset = 0;
    }
    writeBufferInternal(buffer) {
      const bufferLength = buffer.length;
      this.ensureAvailable(bufferLength);
      buffer.copy(this.buf, this.offset, 0);
      this.offset += bufferLength;
    }
    ensureAvailable(length) {
      if (this.offset + length > this.buf.length) {
        const newLength = Encoder.nextPowerOfTwo(this.offset + length);
        const newBuffer = Buffer.alloc(newLength);
        this.buf.copy(newBuffer, 0, 0, this.offset);
        this.buf = newBuffer;
      }
    }
    get buffer() {
      return this.buf.slice(0, this.offset);
    }
    writeInt8(value) {
      this.ensureAvailable(INT8_SIZE);
      this.buf.writeInt8(value, this.offset);
      this.offset += INT8_SIZE;
      return this;
    }
    writeInt16(value) {
      this.ensureAvailable(INT16_SIZE);
      this.buf.writeInt16BE(value, this.offset);
      this.offset += INT16_SIZE;
      return this;
    }
    writeInt32(value) {
      this.ensureAvailable(INT32_SIZE);
      this.buf.writeInt32BE(value, this.offset);
      this.offset += INT32_SIZE;
      return this;
    }
    writeUInt32(value) {
      this.ensureAvailable(INT32_SIZE);
      this.buf.writeUInt32BE(value, this.offset);
      this.offset += INT32_SIZE;
      return this;
    }
    writeInt64(value) {
      this.ensureAvailable(INT64_SIZE);
      const longValue = Long.fromValue(value);
      this.buf.writeInt32BE(longValue.getHighBits(), this.offset);
      this.buf.writeInt32BE(longValue.getLowBits(), this.offset + INT32_SIZE);
      this.offset += INT64_SIZE;
      return this;
    }
    writeDouble(value) {
      this.ensureAvailable(DOUBLE_SIZE);
      this.buf.writeDoubleBE(value, this.offset);
      this.offset += DOUBLE_SIZE;
      return this;
    }
    writeBoolean(value) {
      value ? this.writeInt8(1) : this.writeInt8(0);
      return this;
    }
    writeString(value) {
      if (value == null) {
        this.writeInt16(-1);
        return this;
      }
      const byteLength = Buffer.byteLength(value, "utf8");
      this.ensureAvailable(INT16_SIZE + byteLength);
      this.writeInt16(byteLength);
      this.buf.write(value, this.offset, byteLength, "utf8");
      this.offset += byteLength;
      return this;
    }
    writeVarIntString(value) {
      if (value == null) {
        this.writeVarInt(-1);
        return this;
      }
      const byteLength = Buffer.byteLength(value, "utf8");
      this.writeVarInt(byteLength);
      this.ensureAvailable(byteLength);
      this.buf.write(value, this.offset, byteLength, "utf8");
      this.offset += byteLength;
      return this;
    }
    writeUVarIntString(value) {
      if (value == null) {
        this.writeUVarInt(0);
        return this;
      }
      const byteLength = Buffer.byteLength(value, "utf8");
      this.writeUVarInt(byteLength + 1);
      this.ensureAvailable(byteLength);
      this.buf.write(value, this.offset, byteLength, "utf8");
      this.offset += byteLength;
      return this;
    }
    writeBytes(value) {
      if (value == null) {
        this.writeInt32(-1);
        return this;
      }
      if (Buffer.isBuffer(value)) {
        this.ensureAvailable(INT32_SIZE + value.length);
        this.writeInt32(value.length);
        this.writeBufferInternal(value);
      } else {
        const valueToWrite = String(value);
        const byteLength = Buffer.byteLength(valueToWrite, "utf8");
        this.ensureAvailable(INT32_SIZE + byteLength);
        this.writeInt32(byteLength);
        this.buf.write(valueToWrite, this.offset, byteLength, "utf8");
        this.offset += byteLength;
      }
      return this;
    }
    writeVarIntBytes(value) {
      if (value == null) {
        this.writeVarInt(-1);
        return this;
      }
      if (Buffer.isBuffer(value)) {
        this.writeVarInt(value.length);
        this.writeBufferInternal(value);
      } else {
        const valueToWrite = String(value);
        const byteLength = Buffer.byteLength(valueToWrite, "utf8");
        this.writeVarInt(byteLength);
        this.ensureAvailable(byteLength);
        this.buf.write(valueToWrite, this.offset, byteLength, "utf8");
        this.offset += byteLength;
      }
      return this;
    }
    writeUVarIntBytes(value) {
      if (value == null) {
        this.writeVarInt(0);
        return this;
      }
      if (Buffer.isBuffer(value)) {
        this.writeUVarInt(value.length + 1);
        this.writeBufferInternal(value);
      } else {
        const valueToWrite = String(value);
        const byteLength = Buffer.byteLength(valueToWrite, "utf8");
        this.writeUVarInt(byteLength + 1);
        this.ensureAvailable(byteLength);
        this.buf.write(valueToWrite, this.offset, byteLength, "utf8");
        this.offset += byteLength;
      }
      return this;
    }
    writeEncoder(value) {
      if (value == null || !Buffer.isBuffer(value.buf)) {
        throw new Error("value should be an instance of Encoder");
      }
      this.writeBufferInternal(value.buffer);
      return this;
    }
    writeEncoderArray(value) {
      if (!Array.isArray(value) || value.some((v) => v == null || !Buffer.isBuffer(v.buf))) {
        throw new Error("all values should be an instance of Encoder[]");
      }
      value.forEach((v) => {
        this.writeBufferInternal(v.buffer);
      });
      return this;
    }
    writeBuffer(value) {
      if (!Buffer.isBuffer(value)) {
        throw new Error("value should be an instance of Buffer");
      }
      this.writeBufferInternal(value);
      return this;
    }
    writeNullableArray(array, type) {
      const length = array.length !== 0 ? array.length : -1;
      this.writeArray(array, type, length);
      return this;
    }
    writeArray(array, type, length) {
      const arrayLength = length == null ? array.length : length;
      this.writeInt32(arrayLength);
      if (type !== undefined) {
        switch (type) {
          case "int32":
          case "number":
            array.forEach((value) => this.writeInt32(value));
            break;
          case "string":
            array.forEach((value) => this.writeString(value));
            break;
          case "object":
            this.writeEncoderArray(array);
            break;
        }
      } else {
        array.forEach((value) => {
          switch (typeof value) {
            case "number":
              this.writeInt32(value);
              break;
            case "string":
              this.writeString(value);
              break;
            case "object":
              this.writeEncoder(value);
              break;
          }
        });
      }
      return this;
    }
    writeVarIntArray(array, type) {
      if (type === "object") {
        this.writeVarInt(array.length);
        this.writeEncoderArray(array);
      } else {
        const objectArray = array.filter((v) => typeof v === "object");
        this.writeVarInt(objectArray.length);
        this.writeEncoderArray(objectArray);
      }
      return this;
    }
    writeUVarIntArray(array, type) {
      if (type === "object") {
        this.writeUVarInt(array.length + 1);
        this.writeEncoderArray(array);
      } else if (array === null) {
        this.writeUVarInt(0);
      } else {
        const objectArray = array.filter((v) => typeof v === "object");
        this.writeUVarInt(objectArray.length + 1);
        this.writeEncoderArray(objectArray);
      }
      return this;
    }
    writeVarInt(value) {
      return this.writeUVarInt(Encoder.encodeZigZag(value));
    }
    writeUVarInt(value) {
      const byteArray = [];
      while ((value & UNSIGNED_INT32_MAX_NUMBER) !== 0) {
        byteArray.push(value & OTHER_BITS | MOST_SIGNIFICANT_BIT);
        value >>>= 7;
      }
      byteArray.push(value & OTHER_BITS);
      this.writeBufferInternal(Buffer.from(byteArray));
      return this;
    }
    writeVarLong(value) {
      const byteArray = [];
      let longValue = Encoder.encodeZigZag64(value);
      while (longValue.and(UNSIGNED_INT64_MAX_NUMBER).notEquals(Long.fromInt(0))) {
        byteArray.push(longValue.and(OTHER_BITS).or(MOST_SIGNIFICANT_BIT).toInt());
        longValue = longValue.shiftRightUnsigned(7);
      }
      byteArray.push(longValue.toInt());
      this.writeBufferInternal(Buffer.from(byteArray));
      return this;
    }
    size() {
      return this.offset;
    }
    toJSON() {
      return this.buffer.toJSON();
    }
  };
});

// node_modules/kafkajs/src/protocol/crc32.js
var require_crc32 = __commonJS((exports, module) => {
  var CRC_TABLE = new Int32Array([
    0,
    1996959894,
    3993919788,
    2567524794,
    124634137,
    1886057615,
    3915621685,
    2657392035,
    249268274,
    2044508324,
    3772115230,
    2547177864,
    162941995,
    2125561021,
    3887607047,
    2428444049,
    498536548,
    1789927666,
    4089016648,
    2227061214,
    450548861,
    1843258603,
    4107580753,
    2211677639,
    325883990,
    1684777152,
    4251122042,
    2321926636,
    335633487,
    1661365465,
    4195302755,
    2366115317,
    997073096,
    1281953886,
    3579855332,
    2724688242,
    1006888145,
    1258607687,
    3524101629,
    2768942443,
    901097722,
    1119000684,
    3686517206,
    2898065728,
    853044451,
    1172266101,
    3705015759,
    2882616665,
    651767980,
    1373503546,
    3369554304,
    3218104598,
    565507253,
    1454621731,
    3485111705,
    3099436303,
    671266974,
    1594198024,
    3322730930,
    2970347812,
    795835527,
    1483230225,
    3244367275,
    3060149565,
    1994146192,
    31158534,
    2563907772,
    4023717930,
    1907459465,
    112637215,
    2680153253,
    3904427059,
    2013776290,
    251722036,
    2517215374,
    3775830040,
    2137656763,
    141376813,
    2439277719,
    3865271297,
    1802195444,
    476864866,
    2238001368,
    4066508878,
    1812370925,
    453092731,
    2181625025,
    4111451223,
    1706088902,
    314042704,
    2344532202,
    4240017532,
    1658658271,
    366619977,
    2362670323,
    4224994405,
    1303535960,
    984961486,
    2747007092,
    3569037538,
    1256170817,
    1037604311,
    2765210733,
    3554079995,
    1131014506,
    879679996,
    2909243462,
    3663771856,
    1141124467,
    855842277,
    2852801631,
    3708648649,
    1342533948,
    654459306,
    3188396048,
    3373015174,
    1466479909,
    544179635,
    3110523913,
    3462522015,
    1591671054,
    702138776,
    2966460450,
    3352799412,
    1504918807,
    783551873,
    3082640443,
    3233442989,
    3988292384,
    2596254646,
    62317068,
    1957810842,
    3939845945,
    2647816111,
    81470997,
    1943803523,
    3814918930,
    2489596804,
    225274430,
    2053790376,
    3826175755,
    2466906013,
    167816743,
    2097651377,
    4027552580,
    2265490386,
    503444072,
    1762050814,
    4150417245,
    2154129355,
    426522225,
    1852507879,
    4275313526,
    2312317920,
    282753626,
    1742555852,
    4189708143,
    2394877945,
    397917763,
    1622183637,
    3604390888,
    2714866558,
    953729732,
    1340076626,
    3518719985,
    2797360999,
    1068828381,
    1219638859,
    3624741850,
    2936675148,
    906185462,
    1090812512,
    3747672003,
    2825379669,
    829329135,
    1181335161,
    3412177804,
    3160834842,
    628085408,
    1382605366,
    3423369109,
    3138078467,
    570562233,
    1426400815,
    3317316542,
    2998733608,
    733239954,
    1555261956,
    3268935591,
    3050360625,
    752459403,
    1541320221,
    2607071920,
    3965973030,
    1969922972,
    40735498,
    2617837225,
    3943577151,
    1913087877,
    83908371,
    2512341634,
    3803740692,
    2075208622,
    213261112,
    2463272603,
    3855990285,
    2094854071,
    198958881,
    2262029012,
    4057260610,
    1759359992,
    534414190,
    2176718541,
    4139329115,
    1873836001,
    414664567,
    2282248934,
    4279200368,
    1711684554,
    285281116,
    2405801727,
    4167216745,
    1634467795,
    376229701,
    2685067896,
    3608007406,
    1308918612,
    956543938,
    2808555105,
    3495958263,
    1231636301,
    1047427035,
    2932959818,
    3654703836,
    1088359270,
    936918000,
    2847714899,
    3736837829,
    1202900863,
    817233897,
    3183342108,
    3401237130,
    1404277552,
    615818150,
    3134207493,
    3453421203,
    1423857449,
    601450431,
    3009837614,
    3294710456,
    1567103746,
    711928724,
    3020668471,
    3272380065,
    1510334235,
    755167117
  ]);
  module.exports = (encoder) => {
    const { buffer } = encoder;
    const l = buffer.length;
    let crc = -1;
    for (let n = 0;n < l; n++) {
      crc = CRC_TABLE[(crc ^ buffer[n]) & 255] ^ crc >>> 8;
    }
    return crc ^ -1;
  };
});

// node_modules/kafkajs/src/protocol/message/v0/index.js
var require_v0 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var crc32 = require_crc32();
  var { Types: Compression, COMPRESSION_CODEC_MASK } = require_compression();
  module.exports = ({ compression = Compression.None, key, value }) => {
    const content = new Encoder().writeInt8(0).writeInt8(compression & COMPRESSION_CODEC_MASK).writeBytes(key).writeBytes(value);
    const crc = crc32(content);
    return new Encoder().writeInt32(crc).writeEncoder(content);
  };
});

// node_modules/kafkajs/src/protocol/message/v1/index.js
var require_v1 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var crc32 = require_crc32();
  var { Types: Compression, COMPRESSION_CODEC_MASK } = require_compression();
  module.exports = ({ compression = Compression.None, timestamp = Date.now(), key, value }) => {
    const content = new Encoder().writeInt8(1).writeInt8(compression & COMPRESSION_CODEC_MASK).writeInt64(timestamp).writeBytes(key).writeBytes(value);
    const crc = crc32(content);
    return new Encoder().writeInt32(crc).writeEncoder(content);
  };
});

// node_modules/kafkajs/src/protocol/message/index.js
var require_message = __commonJS((exports, module) => {
  var versions = {
    0: require_v0(),
    1: require_v1()
  };
  module.exports = ({ version = 0 }) => versions[version];
});

// node_modules/kafkajs/src/protocol/messageSet/index.js
var require_messageSet = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var MessageProtocol = require_message();
  var { Types } = require_compression();
  module.exports = ({ messageVersion = 0, compression, entries }) => {
    const isCompressed = compression !== Types.None;
    const Message = MessageProtocol({ version: messageVersion });
    const encoder = new Encoder;
    entries.forEach((entry, i) => {
      const message = Message(entry);
      encoder.writeInt64(isCompressed ? i : -1);
      encoder.writeInt32(message.size());
      encoder.writeEncoder(message);
    });
    return encoder;
  };
});

// node_modules/kafkajs/src/protocol/requests/produce/v0/request.js
var require_request = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { Produce: apiKey } = require_apiKeys();
  var MessageSet = require_messageSet();
  module.exports = ({ acks, timeout, topicData }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "Produce",
    expectResponse: () => acks !== 0,
    encode: async () => {
      return new Encoder().writeInt16(acks).writeInt32(timeout).writeArray(topicData.map(encodeTopic));
    }
  });
  var encodeTopic = ({ topic, partitions }) => {
    return new Encoder().writeString(topic).writeArray(partitions.map(encodePartitions));
  };
  var encodePartitions = ({ partition, messages }) => {
    const messageSet = MessageSet({ messageVersion: 0, entries: messages });
    return new Encoder().writeInt32(partition).writeInt32(messageSet.size()).writeEncoder(messageSet);
  };
});

// node_modules/kafkajs/src/protocol/decoder.js
var require_decoder = __commonJS((exports, module) => {
  var { KafkaJSInvalidVarIntError, KafkaJSInvalidLongError } = require_errors();
  var Long = require_long();
  var INT8_SIZE = 1;
  var INT16_SIZE = 2;
  var INT32_SIZE = 4;
  var INT64_SIZE = 8;
  var DOUBLE_SIZE = 8;
  var MOST_SIGNIFICANT_BIT = 128;
  var OTHER_BITS = 127;
  module.exports = class Decoder {
    static int32Size() {
      return INT32_SIZE;
    }
    static decodeZigZag(value) {
      return value >>> 1 ^ -(value & 1);
    }
    static decodeZigZag64(longValue) {
      return longValue.shiftRightUnsigned(1).xor(longValue.and(Long.fromInt(1)).negate());
    }
    constructor(buffer) {
      this.buffer = buffer;
      this.offset = 0;
    }
    readInt8() {
      const value = this.buffer.readInt8(this.offset);
      this.offset += INT8_SIZE;
      return value;
    }
    canReadInt16() {
      return this.canReadBytes(INT16_SIZE);
    }
    readInt16() {
      const value = this.buffer.readInt16BE(this.offset);
      this.offset += INT16_SIZE;
      return value;
    }
    canReadInt32() {
      return this.canReadBytes(INT32_SIZE);
    }
    readInt32() {
      const value = this.buffer.readInt32BE(this.offset);
      this.offset += INT32_SIZE;
      return value;
    }
    canReadInt64() {
      return this.canReadBytes(INT64_SIZE);
    }
    readInt64() {
      const first = this.buffer[this.offset];
      const last = this.buffer[this.offset + 7];
      const low = (first << 24) + this.buffer[this.offset + 1] * 2 ** 16 + this.buffer[this.offset + 2] * 2 ** 8 + this.buffer[this.offset + 3];
      const high = this.buffer[this.offset + 4] * 2 ** 24 + this.buffer[this.offset + 5] * 2 ** 16 + this.buffer[this.offset + 6] * 2 ** 8 + last;
      this.offset += INT64_SIZE;
      return (BigInt(low) << 32n) + BigInt(high);
    }
    readDouble() {
      const value = this.buffer.readDoubleBE(this.offset);
      this.offset += DOUBLE_SIZE;
      return value;
    }
    readString() {
      const byteLength = this.readInt16();
      if (byteLength === -1) {
        return null;
      }
      const stringBuffer = this.buffer.slice(this.offset, this.offset + byteLength);
      const value = stringBuffer.toString("utf8");
      this.offset += byteLength;
      return value;
    }
    readVarIntString() {
      const byteLength = this.readVarInt();
      if (byteLength === -1) {
        return null;
      }
      const stringBuffer = this.buffer.slice(this.offset, this.offset + byteLength);
      const value = stringBuffer.toString("utf8");
      this.offset += byteLength;
      return value;
    }
    readUVarIntString() {
      const byteLength = this.readUVarInt();
      if (byteLength === 0) {
        return null;
      }
      const stringBuffer = this.buffer.slice(this.offset, this.offset + byteLength - 1);
      const value = stringBuffer.toString("utf8");
      this.offset += byteLength - 1;
      return value;
    }
    canReadBytes(length) {
      return Buffer.byteLength(this.buffer) - this.offset >= length;
    }
    readBytes(byteLength = this.readInt32()) {
      if (byteLength === -1) {
        return null;
      }
      const stringBuffer = this.buffer.slice(this.offset, this.offset + byteLength);
      this.offset += byteLength;
      return stringBuffer;
    }
    readVarIntBytes() {
      const byteLength = this.readVarInt();
      if (byteLength === -1) {
        return null;
      }
      const stringBuffer = this.buffer.slice(this.offset, this.offset + byteLength);
      this.offset += byteLength;
      return stringBuffer;
    }
    readUVarIntBytes() {
      const byteLength = this.readUVarInt();
      if (byteLength === 0) {
        return null;
      }
      const stringBuffer = this.buffer.slice(this.offset, this.offset + byteLength);
      this.offset += byteLength - 1;
      return stringBuffer;
    }
    readBoolean() {
      return this.readInt8() === 1;
    }
    readAll() {
      const result = this.buffer.slice(this.offset);
      this.offset += Buffer.byteLength(this.buffer);
      return result;
    }
    readArray(reader) {
      const length = this.readInt32();
      if (length === -1) {
        return [];
      }
      const array = new Array(length);
      for (let i = 0;i < length; i++) {
        array[i] = reader(this);
      }
      return array;
    }
    readVarIntArray(reader) {
      const length = this.readVarInt();
      if (length === -1) {
        return [];
      }
      const array = new Array(length);
      for (let i = 0;i < length; i++) {
        array[i] = reader(this);
      }
      return array;
    }
    readUVarIntArray(reader) {
      const length = this.readUVarInt();
      if (length === 0) {
        return null;
      }
      const array = new Array(length - 1);
      for (let i = 0;i < length - 1; i++) {
        array[i] = reader(this);
      }
      return array;
    }
    async readArrayAsync(reader) {
      const length = this.readInt32();
      if (length === -1) {
        return [];
      }
      const array = new Array(length);
      for (let i = 0;i < length; i++) {
        array[i] = await reader(this);
      }
      return array;
    }
    readVarInt() {
      let currentByte;
      let result = 0;
      let i = 0;
      do {
        currentByte = this.buffer[this.offset++];
        result += (currentByte & OTHER_BITS) << i;
        i += 7;
      } while (currentByte >= MOST_SIGNIFICANT_BIT);
      return Decoder.decodeZigZag(result);
    }
    readUVarInt() {
      let currentByte;
      let result = 0;
      let i = 0;
      while (((currentByte = this.buffer[this.offset++]) & MOST_SIGNIFICANT_BIT) !== 0) {
        result |= (currentByte & OTHER_BITS) << i;
        i += 7;
        if (i > 28) {
          throw new KafkaJSInvalidVarIntError("Invalid VarInt, must contain 5 bytes or less");
        }
      }
      result |= currentByte << i;
      return result >>> 0;
    }
    readTaggedFields() {
      const numberOfTaggedFields = this.readUVarInt();
      if (numberOfTaggedFields === 0) {
        return null;
      }
      const taggedFields = {};
      for (let i = 0;i < numberOfTaggedFields; i++) {
        this.readUVarInt();
        this.readUVarIntBytes();
      }
      return taggedFields;
    }
    readVarLong() {
      let currentByte;
      let result = Long.fromInt(0);
      let i = 0;
      do {
        if (i > 63) {
          throw new KafkaJSInvalidLongError("Invalid Long, must contain 9 bytes or less");
        }
        currentByte = this.buffer[this.offset++];
        result = result.add(Long.fromInt(currentByte & OTHER_BITS).shiftLeft(i));
        i += 7;
      } while (currentByte >= MOST_SIGNIFICANT_BIT);
      return Decoder.decodeZigZag64(result);
    }
    slice(size) {
      return new Decoder(this.buffer.slice(this.offset, this.offset + size));
    }
    forward(size) {
      this.offset += size;
    }
  };
});

// node_modules/kafkajs/src/utils/websiteUrl.js
var require_websiteUrl = __commonJS((exports, module) => {
  var BASE_URL = "https://kafka.js.org";
  var stripLeading = (char) => (str) => str.charAt(0) === char ? str.substring(1) : str;
  var stripLeadingSlash = stripLeading("/");
  var stripLeadingHash = stripLeading("#");
  module.exports = (path, hash) => `${BASE_URL}/${stripLeadingSlash(path)}${hash ? "#" + stripLeadingHash(hash) : ""}`;
});

// node_modules/kafkajs/src/protocol/error.js
var require_error = __commonJS((exports, module) => {
  var { KafkaJSProtocolError } = require_errors();
  var websiteUrl = require_websiteUrl();
  var errorCodes = [
    {
      type: "UNKNOWN",
      code: -1,
      retriable: false,
      message: "The server experienced an unexpected error when processing the request"
    },
    {
      type: "OFFSET_OUT_OF_RANGE",
      code: 1,
      retriable: false,
      message: "The requested offset is not within the range of offsets maintained by the server"
    },
    {
      type: "CORRUPT_MESSAGE",
      code: 2,
      retriable: true,
      message: "This message has failed its CRC checksum, exceeds the valid size, or is otherwise corrupt"
    },
    {
      type: "UNKNOWN_TOPIC_OR_PARTITION",
      code: 3,
      retriable: true,
      message: "This server does not host this topic-partition"
    },
    {
      type: "INVALID_FETCH_SIZE",
      code: 4,
      retriable: false,
      message: "The requested fetch size is invalid"
    },
    {
      type: "LEADER_NOT_AVAILABLE",
      code: 5,
      retriable: true,
      message: "There is no leader for this topic-partition as we are in the middle of a leadership election"
    },
    {
      type: "NOT_LEADER_FOR_PARTITION",
      code: 6,
      retriable: true,
      message: "This server is not the leader for that topic-partition"
    },
    {
      type: "REQUEST_TIMED_OUT",
      code: 7,
      retriable: true,
      message: "The request timed out"
    },
    {
      type: "BROKER_NOT_AVAILABLE",
      code: 8,
      retriable: false,
      message: "The broker is not available"
    },
    {
      type: "REPLICA_NOT_AVAILABLE",
      code: 9,
      retriable: true,
      message: "The replica is not available for the requested topic-partition"
    },
    {
      type: "MESSAGE_TOO_LARGE",
      code: 10,
      retriable: false,
      message: "The request included a message larger than the max message size the server will accept"
    },
    {
      type: "STALE_CONTROLLER_EPOCH",
      code: 11,
      retriable: false,
      message: "The controller moved to another broker"
    },
    {
      type: "OFFSET_METADATA_TOO_LARGE",
      code: 12,
      retriable: false,
      message: "The metadata field of the offset request was too large"
    },
    {
      type: "NETWORK_EXCEPTION",
      code: 13,
      retriable: true,
      message: "The server disconnected before a response was received"
    },
    {
      type: "GROUP_LOAD_IN_PROGRESS",
      code: 14,
      retriable: true,
      message: "The coordinator is loading and hence can't process requests for this group"
    },
    {
      type: "GROUP_COORDINATOR_NOT_AVAILABLE",
      code: 15,
      retriable: true,
      message: "The group coordinator is not available"
    },
    {
      type: "NOT_COORDINATOR_FOR_GROUP",
      code: 16,
      retriable: true,
      message: "This is not the correct coordinator for this group"
    },
    {
      type: "INVALID_TOPIC_EXCEPTION",
      code: 17,
      retriable: false,
      message: "The request attempted to perform an operation on an invalid topic"
    },
    {
      type: "RECORD_LIST_TOO_LARGE",
      code: 18,
      retriable: false,
      message: "The request included message batch larger than the configured segment size on the server"
    },
    {
      type: "NOT_ENOUGH_REPLICAS",
      code: 19,
      retriable: true,
      message: "Messages are rejected since there are fewer in-sync replicas than required"
    },
    {
      type: "NOT_ENOUGH_REPLICAS_AFTER_APPEND",
      code: 20,
      retriable: true,
      message: "Messages are written to the log, but to fewer in-sync replicas than required"
    },
    {
      type: "INVALID_REQUIRED_ACKS",
      code: 21,
      retriable: false,
      message: "Produce request specified an invalid value for required acks"
    },
    {
      type: "ILLEGAL_GENERATION",
      code: 22,
      retriable: false,
      message: "Specified group generation id is not valid"
    },
    {
      type: "INCONSISTENT_GROUP_PROTOCOL",
      code: 23,
      retriable: false,
      message: "The group member's supported protocols are incompatible with those of existing members"
    },
    {
      type: "INVALID_GROUP_ID",
      code: 24,
      retriable: false,
      message: "The configured groupId is invalid"
    },
    {
      type: "UNKNOWN_MEMBER_ID",
      code: 25,
      retriable: false,
      message: "The coordinator is not aware of this member"
    },
    {
      type: "INVALID_SESSION_TIMEOUT",
      code: 26,
      retriable: false,
      message: "The session timeout is not within the range allowed by the broker (as configured by group.min.session.timeout.ms and group.max.session.timeout.ms)"
    },
    {
      type: "REBALANCE_IN_PROGRESS",
      code: 27,
      retriable: false,
      message: "The group is rebalancing, so a rejoin is needed",
      helpUrl: websiteUrl("docs/faq", "what-does-it-mean-to-get-rebalance-in-progress-errors")
    },
    {
      type: "INVALID_COMMIT_OFFSET_SIZE",
      code: 28,
      retriable: false,
      message: "The committing offset data size is not valid"
    },
    {
      type: "TOPIC_AUTHORIZATION_FAILED",
      code: 29,
      retriable: false,
      message: "Not authorized to access topics: [Topic authorization failed]"
    },
    {
      type: "GROUP_AUTHORIZATION_FAILED",
      code: 30,
      retriable: false,
      message: "Not authorized to access group: Group authorization failed"
    },
    {
      type: "CLUSTER_AUTHORIZATION_FAILED",
      code: 31,
      retriable: false,
      message: "Cluster authorization failed"
    },
    {
      type: "INVALID_TIMESTAMP",
      code: 32,
      retriable: false,
      message: "The timestamp of the message is out of acceptable range"
    },
    {
      type: "UNSUPPORTED_SASL_MECHANISM",
      code: 33,
      retriable: false,
      message: "The broker does not support the requested SASL mechanism"
    },
    {
      type: "ILLEGAL_SASL_STATE",
      code: 34,
      retriable: false,
      message: "Request is not valid given the current SASL state"
    },
    {
      type: "UNSUPPORTED_VERSION",
      code: 35,
      retriable: false,
      message: "The version of API is not supported"
    },
    {
      type: "TOPIC_ALREADY_EXISTS",
      code: 36,
      retriable: false,
      message: "Topic with this name already exists"
    },
    {
      type: "INVALID_PARTITIONS",
      code: 37,
      retriable: false,
      message: "Number of partitions is invalid"
    },
    {
      type: "INVALID_REPLICATION_FACTOR",
      code: 38,
      retriable: false,
      message: "Replication-factor is invalid"
    },
    {
      type: "INVALID_REPLICA_ASSIGNMENT",
      code: 39,
      retriable: false,
      message: "Replica assignment is invalid"
    },
    {
      type: "INVALID_CONFIG",
      code: 40,
      retriable: false,
      message: "Configuration is invalid"
    },
    {
      type: "NOT_CONTROLLER",
      code: 41,
      retriable: true,
      message: "This is not the correct controller for this cluster"
    },
    {
      type: "INVALID_REQUEST",
      code: 42,
      retriable: false,
      message: "This most likely occurs because of a request being malformed by the client library or the message was sent to an incompatible broker. See the broker logs for more details"
    },
    {
      type: "UNSUPPORTED_FOR_MESSAGE_FORMAT",
      code: 43,
      retriable: false,
      message: "The message format version on the broker does not support the request"
    },
    {
      type: "POLICY_VIOLATION",
      code: 44,
      retriable: false,
      message: "Request parameters do not satisfy the configured policy"
    },
    {
      type: "OUT_OF_ORDER_SEQUENCE_NUMBER",
      code: 45,
      retriable: false,
      message: "The broker received an out of order sequence number"
    },
    {
      type: "DUPLICATE_SEQUENCE_NUMBER",
      code: 46,
      retriable: false,
      message: "The broker received a duplicate sequence number"
    },
    {
      type: "INVALID_PRODUCER_EPOCH",
      code: 47,
      retriable: false,
      message: "Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer's transaction has been expired by the broker"
    },
    {
      type: "INVALID_TXN_STATE",
      code: 48,
      retriable: false,
      message: "The producer attempted a transactional operation in an invalid state"
    },
    {
      type: "INVALID_PRODUCER_ID_MAPPING",
      code: 49,
      retriable: false,
      message: "The producer attempted to use a producer id which is not currently assigned to its transactional id"
    },
    {
      type: "INVALID_TRANSACTION_TIMEOUT",
      code: 50,
      retriable: false,
      message: "The transaction timeout is larger than the maximum value allowed by the broker (as configured by max.transaction.timeout.ms)"
    },
    {
      type: "CONCURRENT_TRANSACTIONS",
      code: 51,
      retriable: true,
      message: "The producer attempted to update a transaction while another concurrent operation on the same transaction was ongoing"
    },
    {
      type: "TRANSACTION_COORDINATOR_FENCED",
      code: 52,
      retriable: false,
      message: "Indicates that the transaction coordinator sending a WriteTxnMarker is no longer the current coordinator for a given producer"
    },
    {
      type: "TRANSACTIONAL_ID_AUTHORIZATION_FAILED",
      code: 53,
      retriable: false,
      message: "Transactional Id authorization failed"
    },
    {
      type: "SECURITY_DISABLED",
      code: 54,
      retriable: false,
      message: "Security features are disabled"
    },
    {
      type: "OPERATION_NOT_ATTEMPTED",
      code: 55,
      retriable: false,
      message: "The broker did not attempt to execute this operation. This may happen for batched RPCs where some operations in the batch failed, causing the broker to respond without trying the rest"
    },
    {
      type: "KAFKA_STORAGE_ERROR",
      code: 56,
      retriable: true,
      message: "Disk error when trying to access log file on the disk"
    },
    {
      type: "LOG_DIR_NOT_FOUND",
      code: 57,
      retriable: false,
      message: "The user-specified log directory is not found in the broker config"
    },
    {
      type: "SASL_AUTHENTICATION_FAILED",
      code: 58,
      retriable: false,
      message: "SASL Authentication failed",
      helpUrl: websiteUrl("docs/configuration", "sasl")
    },
    {
      type: "UNKNOWN_PRODUCER_ID",
      code: 59,
      retriable: false,
      message: "This exception is raised by the broker if it could not locate the producer metadata associated with the producerId in question.ts. This could happen if, for instance, the producer's records were deleted because their retention time had elapsed. Once the last records of the producerId are removed, the producer's metadata is removed from the broker, and future appends by the producer will return this exception"
    },
    {
      type: "REASSIGNMENT_IN_PROGRESS",
      code: 60,
      retriable: false,
      message: "A partition reassignment is in progress"
    },
    {
      type: "DELEGATION_TOKEN_AUTH_DISABLED",
      code: 61,
      retriable: false,
      message: "Delegation Token feature is not enabled"
    },
    {
      type: "DELEGATION_TOKEN_NOT_FOUND",
      code: 62,
      retriable: false,
      message: "Delegation Token is not found on server"
    },
    {
      type: "DELEGATION_TOKEN_OWNER_MISMATCH",
      code: 63,
      retriable: false,
      message: "Specified Principal is not valid Owner/Renewer"
    },
    {
      type: "DELEGATION_TOKEN_REQUEST_NOT_ALLOWED",
      code: 64,
      retriable: false,
      message: "Delegation Token requests are not allowed on PLAINTEXT/1-way SSL channels and on delegation token authenticated channels"
    },
    {
      type: "DELEGATION_TOKEN_AUTHORIZATION_FAILED",
      code: 65,
      retriable: false,
      message: "Delegation Token authorization failed"
    },
    {
      type: "DELEGATION_TOKEN_EXPIRED",
      code: 66,
      retriable: false,
      message: "Delegation Token is expired"
    },
    {
      type: "INVALID_PRINCIPAL_TYPE",
      code: 67,
      retriable: false,
      message: "Supplied principalType is not supported"
    },
    {
      type: "NON_EMPTY_GROUP",
      code: 68,
      retriable: false,
      message: "The group is not empty"
    },
    {
      type: "GROUP_ID_NOT_FOUND",
      code: 69,
      retriable: false,
      message: "The group id was not found"
    },
    {
      type: "FETCH_SESSION_ID_NOT_FOUND",
      code: 70,
      retriable: true,
      message: "The fetch session ID was not found"
    },
    {
      type: "INVALID_FETCH_SESSION_EPOCH",
      code: 71,
      retriable: true,
      message: "The fetch session epoch is invalid"
    },
    {
      type: "LISTENER_NOT_FOUND",
      code: 72,
      retriable: true,
      message: "There is no listener on the leader broker that matches the listener on which metadata request was processed"
    },
    {
      type: "TOPIC_DELETION_DISABLED",
      code: 73,
      retriable: false,
      message: "Topic deletion is disabled"
    },
    {
      type: "FENCED_LEADER_EPOCH",
      code: 74,
      retriable: true,
      message: "The leader epoch in the request is older than the epoch on the broker"
    },
    {
      type: "UNKNOWN_LEADER_EPOCH",
      code: 75,
      retriable: true,
      message: "The leader epoch in the request is newer than the epoch on the broker"
    },
    {
      type: "UNSUPPORTED_COMPRESSION_TYPE",
      code: 76,
      retriable: false,
      message: "The requesting client does not support the compression type of given partition"
    },
    {
      type: "STALE_BROKER_EPOCH",
      code: 77,
      retriable: false,
      message: "Broker epoch has changed"
    },
    {
      type: "OFFSET_NOT_AVAILABLE",
      code: 78,
      retriable: true,
      message: "The leader high watermark has not caught up from a recent leader election so the offsets cannot be guaranteed to be monotonically increasing"
    },
    {
      type: "MEMBER_ID_REQUIRED",
      code: 79,
      retriable: false,
      message: "The group member needs to have a valid member id before actually entering a consumer group"
    },
    {
      type: "PREFERRED_LEADER_NOT_AVAILABLE",
      code: 80,
      retriable: true,
      message: "The preferred leader was not available"
    },
    {
      type: "GROUP_MAX_SIZE_REACHED",
      code: 81,
      retriable: false,
      message: "The consumer group has reached its max size. It already has the configured maximum number of members"
    },
    {
      type: "FENCED_INSTANCE_ID",
      code: 82,
      retriable: false,
      message: "The broker rejected this static consumer since another consumer with the same group instance id has registered with a different member id"
    },
    {
      type: "ELIGIBLE_LEADERS_NOT_AVAILABLE",
      code: 83,
      retriable: true,
      message: "Eligible topic partition leaders are not available"
    },
    {
      type: "ELECTION_NOT_NEEDED",
      code: 84,
      retriable: true,
      message: "Leader election not needed for topic partition"
    },
    {
      type: "NO_REASSIGNMENT_IN_PROGRESS",
      code: 85,
      retriable: false,
      message: "No partition reassignment is in progress"
    },
    {
      type: "GROUP_SUBSCRIBED_TO_TOPIC",
      code: 86,
      retriable: false,
      message: "Deleting offsets of a topic is forbidden while the consumer group is actively subscribed to it"
    },
    {
      type: "INVALID_RECORD",
      code: 87,
      retriable: false,
      message: "This record has failed the validation on broker and hence be rejected"
    },
    {
      type: "UNSTABLE_OFFSET_COMMIT",
      code: 88,
      retriable: true,
      message: "There are unstable offsets that need to be cleared"
    }
  ];
  var unknownErrorCode = (errorCode) => ({
    type: "KAFKAJS_UNKNOWN_ERROR_CODE",
    code: -99,
    retriable: false,
    message: `Unknown error code ${errorCode}`
  });
  var SUCCESS_CODE = 0;
  var UNSUPPORTED_VERSION_CODE = 35;
  var failure = (code) => code !== SUCCESS_CODE;
  var createErrorFromCode = (code) => {
    return new KafkaJSProtocolError(errorCodes.find((e) => e.code === code) || unknownErrorCode(code));
  };
  var failIfVersionNotSupported = (code) => {
    if (code === UNSUPPORTED_VERSION_CODE) {
      throw createErrorFromCode(UNSUPPORTED_VERSION_CODE);
    }
  };
  var staleMetadata = (e) => ["UNKNOWN_TOPIC_OR_PARTITION", "LEADER_NOT_AVAILABLE", "NOT_LEADER_FOR_PARTITION"].includes(e.type);
  module.exports = {
    failure,
    errorCodes,
    createErrorFromCode,
    failIfVersionNotSupported,
    staleMetadata
  };
});

// node_modules/kafkajs/src/protocol/requests/produce/v0/response.js
var require_response = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode } = require_error();
  var partition = (decoder) => ({
    partition: decoder.readInt32(),
    errorCode: decoder.readInt16(),
    offset: decoder.readInt64().toString()
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const topics = decoder.readArray((decoder2) => ({
      topicName: decoder2.readString(),
      partitions: decoder2.readArray(partition)
    }));
    return {
      topics
    };
  };
  var parse = async (data) => {
    const errors = data.topics.flatMap((topic) => {
      return topic.partitions.filter((partition2) => failure(partition2.errorCode));
    });
    if (errors.length > 0) {
      const { errorCode } = errors[0];
      throw createErrorFromCode(errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/produce/v1/request.js
var require_request2 = __commonJS((exports, module) => {
  var requestV0 = require_request();
  module.exports = ({ acks, timeout, topicData }) => {
    return Object.assign(requestV0({ acks, timeout, topicData }), { apiVersion: 1 });
  };
});

// node_modules/kafkajs/src/protocol/requests/produce/v1/response.js
var require_response2 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { parse: parseV0 } = require_response();
  var partition = (decoder) => ({
    partition: decoder.readInt32(),
    errorCode: decoder.readInt16(),
    offset: decoder.readInt64().toString()
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const topics = decoder.readArray((decoder2) => ({
      topicName: decoder2.readString(),
      partitions: decoder2.readArray(partition)
    }));
    const throttleTime = decoder.readInt32();
    return {
      topics,
      throttleTime
    };
  };
  module.exports = {
    decode,
    parse: parseV0
  };
});

// node_modules/kafkajs/src/protocol/requests/produce/v2/request.js
var require_request3 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { Produce: apiKey } = require_apiKeys();
  var MessageSet = require_messageSet();
  var { Types, lookupCodec } = require_compression();
  module.exports = ({ acks, timeout, compression = Types.None, topicData }) => ({
    apiKey,
    apiVersion: 2,
    apiName: "Produce",
    expectResponse: () => acks !== 0,
    encode: async () => {
      const encodeTopic = topicEncoder(compression);
      const encodedTopicData = [];
      for (const data of topicData) {
        encodedTopicData.push(await encodeTopic(data));
      }
      return new Encoder().writeInt16(acks).writeInt32(timeout).writeArray(encodedTopicData);
    }
  });
  var topicEncoder = (compression) => {
    const encodePartitions = partitionsEncoder(compression);
    return async ({ topic, partitions }) => {
      const encodedPartitions = [];
      for (const data of partitions) {
        encodedPartitions.push(await encodePartitions(data));
      }
      return new Encoder().writeString(topic).writeArray(encodedPartitions);
    };
  };
  var partitionsEncoder = (compression) => async ({ partition, messages }) => {
    const messageSet = MessageSet({ messageVersion: 1, compression, entries: messages });
    if (compression === Types.None) {
      return new Encoder().writeInt32(partition).writeInt32(messageSet.size()).writeEncoder(messageSet);
    }
    const timestamp = messages[0].timestamp || Date.now();
    const codec = lookupCodec(compression);
    const compressedValue = await codec.compress(messageSet);
    const compressedMessageSet = MessageSet({
      messageVersion: 1,
      entries: [{ compression, timestamp, value: compressedValue }]
    });
    return new Encoder().writeInt32(partition).writeInt32(compressedMessageSet.size()).writeEncoder(compressedMessageSet);
  };
});

// node_modules/kafkajs/src/protocol/requests/produce/v2/response.js
var require_response3 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { parse: parseV0 } = require_response();
  var partition = (decoder) => ({
    partition: decoder.readInt32(),
    errorCode: decoder.readInt16(),
    offset: decoder.readInt64().toString(),
    timestamp: decoder.readInt64().toString()
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const topics = decoder.readArray((decoder2) => ({
      topicName: decoder2.readString(),
      partitions: decoder2.readArray(partition)
    }));
    const throttleTime = decoder.readInt32();
    return {
      topics,
      throttleTime
    };
  };
  module.exports = {
    decode,
    parse: parseV0
  };
});

// node_modules/kafkajs/src/protocol/recordBatch/header/v0/index.js
var require_v02 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  module.exports = ({ key, value }) => {
    return new Encoder().writeVarIntString(key).writeVarIntBytes(value);
  };
});

// node_modules/kafkajs/src/protocol/recordBatch/record/v0/index.js
var require_v03 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var Header = require_v02();
  module.exports = ({ offsetDelta = 0, timestampDelta = 0, key, value, headers = {} }) => {
    const headersArray = Object.keys(headers).flatMap((headerKey) => !Array.isArray(headers[headerKey]) ? [{ key: headerKey, value: headers[headerKey] }] : headers[headerKey].map((headerValue) => ({ key: headerKey, value: headerValue })));
    const sizeOfBody = 1 + Encoder.sizeOfVarLong(timestampDelta) + Encoder.sizeOfVarInt(offsetDelta) + Encoder.sizeOfVarIntBytes(key) + Encoder.sizeOfVarIntBytes(value) + sizeOfHeaders(headersArray);
    return new Encoder().writeVarInt(sizeOfBody).writeInt8(0).writeVarLong(timestampDelta).writeVarInt(offsetDelta).writeVarIntBytes(key).writeVarIntBytes(value).writeVarIntArray(headersArray.map(Header));
  };
  var sizeOfHeaders = (headersArray) => {
    let size = Encoder.sizeOfVarInt(headersArray.length);
    for (const header of headersArray) {
      const keySize = Buffer.byteLength(header.key);
      const valueSize = Buffer.byteLength(header.value);
      size += Encoder.sizeOfVarInt(keySize) + keySize;
      if (header.value === null) {
        size += Encoder.sizeOfVarInt(-1);
      } else {
        size += Encoder.sizeOfVarInt(valueSize) + valueSize;
      }
    }
    return size;
  };
});

// node_modules/kafkajs/src/protocol/recordBatch/crc32C/crc32C.js
var require_crc32C = __commonJS((exports, module) => {
  var crc32C = (buffer) => {
    let crc = 0 ^ -1;
    for (let i = 0;i < buffer.length; i++) {
      crc = T[(crc ^ buffer[i]) & 255] ^ crc >>> 8;
    }
    return (crc ^ -1) >>> 0;
  };
  module.exports = crc32C;
  var T = new Int32Array([
    0,
    4067132163,
    3778769143,
    324072436,
    3348797215,
    904991772,
    648144872,
    3570033899,
    2329499855,
    2024987596,
    1809983544,
    2575936315,
    1296289744,
    3207089363,
    2893594407,
    1578318884,
    274646895,
    3795141740,
    4049975192,
    51262619,
    3619967088,
    632279923,
    922689671,
    3298075524,
    2592579488,
    1760304291,
    2075979607,
    2312596564,
    1562183871,
    2943781820,
    3156637768,
    1313733451,
    549293790,
    3537243613,
    3246849577,
    871202090,
    3878099393,
    357341890,
    102525238,
    4101499445,
    2858735121,
    1477399826,
    1264559846,
    3107202533,
    1845379342,
    2677391885,
    2361733625,
    2125378298,
    820201905,
    3263744690,
    3520608582,
    598981189,
    4151959214,
    85089709,
    373468761,
    3827903834,
    3124367742,
    1213305469,
    1526817161,
    2842354314,
    2107672161,
    2412447074,
    2627466902,
    1861252501,
    1098587580,
    3004210879,
    2688576843,
    1378610760,
    2262928035,
    1955203488,
    1742404180,
    2511436119,
    3416409459,
    969524848,
    714683780,
    3639785095,
    205050476,
    4266873199,
    3976438427,
    526918040,
    1361435347,
    2739821008,
    2954799652,
    1114974503,
    2529119692,
    1691668175,
    2005155131,
    2247081528,
    3690758684,
    697762079,
    986182379,
    3366744552,
    476452099,
    3993867776,
    4250756596,
    255256311,
    1640403810,
    2477592673,
    2164122517,
    1922457750,
    2791048317,
    1412925310,
    1197962378,
    3037525897,
    3944729517,
    427051182,
    170179418,
    4165941337,
    746937522,
    3740196785,
    3451792453,
    1070968646,
    1905808397,
    2213795598,
    2426610938,
    1657317369,
    3053634322,
    1147748369,
    1463399397,
    2773627110,
    4215344322,
    153784257,
    444234805,
    3893493558,
    1021025245,
    3467647198,
    3722505002,
    797665321,
    2197175160,
    1889384571,
    1674398607,
    2443626636,
    1164749927,
    3070701412,
    2757221520,
    1446797203,
    137323447,
    4198817972,
    3910406976,
    461344835,
    3484808360,
    1037989803,
    781091935,
    3705997148,
    2460548119,
    1623424788,
    1939049696,
    2180517859,
    1429367560,
    2807687179,
    3020495871,
    1180866812,
    410100952,
    3927582683,
    4182430767,
    186734380,
    3756733383,
    763408580,
    1053836080,
    3434856499,
    2722870694,
    1344288421,
    1131464017,
    2971354706,
    1708204729,
    2545590714,
    2229949006,
    1988219213,
    680717673,
    3673779818,
    3383336350,
    1002577565,
    4010310262,
    493091189,
    238226049,
    4233660802,
    2987750089,
    1082061258,
    1395524158,
    2705686845,
    1972364758,
    2279892693,
    2494862625,
    1725896226,
    952904198,
    3399985413,
    3656866545,
    731699698,
    4283874585,
    222117402,
    510512622,
    3959836397,
    3280807620,
    837199303,
    582374963,
    3504198960,
    68661723,
    4135334616,
    3844915500,
    390545967,
    1230274059,
    3141532936,
    2825850620,
    1510247935,
    2395924756,
    2091215383,
    1878366691,
    2644384480,
    3553878443,
    565732008,
    854102364,
    3229815391,
    340358836,
    3861050807,
    4117890627,
    119113024,
    1493875044,
    2875275879,
    3090270611,
    1247431312,
    2660249211,
    1828433272,
    2141937292,
    2378227087,
    3811616794,
    291187481,
    34330861,
    4032846830,
    615137029,
    3603020806,
    3314634738,
    939183345,
    1776939221,
    2609017814,
    2295496738,
    2058945313,
    2926798794,
    1545135305,
    1330124605,
    3173225534,
    4084100981,
    17165430,
    307568514,
    3762199681,
    888469610,
    3332340585,
    3587147933,
    665062302,
    2042050490,
    2346497209,
    2559330125,
    1793573966,
    3190661285,
    1279665062,
    1595330642,
    2910671697
  ]);
});

// node_modules/kafkajs/src/protocol/recordBatch/crc32C/index.js
var require_crc32C2 = __commonJS((exports, module) => {
  var crc32C = require_crc32C();
  var unsigned = (value) => Uint32Array.from([value])[0];
  module.exports = (buffer) => unsigned(crc32C(buffer));
});

// node_modules/kafkajs/src/protocol/recordBatch/v0/index.js
var require_v04 = __commonJS((exports, module) => {
  var Long = require_long();
  var Encoder = require_encoder();
  var crc32C = require_crc32C2();
  var {
    Types: Compression,
    lookupCodec,
    COMPRESSION_CODEC_MASK
  } = require_compression();
  var MAGIC_BYTE = 2;
  var TIMESTAMP_MASK = 0;
  var TRANSACTIONAL_MASK = 16;
  var RecordBatch = async ({
    compression = Compression.None,
    firstOffset = Long.fromInt(0),
    firstTimestamp = Date.now(),
    maxTimestamp = Date.now(),
    partitionLeaderEpoch = 0,
    lastOffsetDelta = 0,
    transactional = false,
    producerId = Long.fromValue(-1),
    producerEpoch = 0,
    firstSequence = 0,
    records = []
  }) => {
    const COMPRESSION_CODEC = compression & COMPRESSION_CODEC_MASK;
    const IN_TRANSACTION = transactional ? TRANSACTIONAL_MASK : 0;
    const attributes = COMPRESSION_CODEC | TIMESTAMP_MASK | IN_TRANSACTION;
    const batchBody = new Encoder().writeInt16(attributes).writeInt32(lastOffsetDelta).writeInt64(firstTimestamp).writeInt64(maxTimestamp).writeInt64(producerId).writeInt16(producerEpoch).writeInt32(firstSequence);
    if (compression === Compression.None) {
      if (records.every((v) => typeof v === typeof records[0])) {
        batchBody.writeArray(records, typeof records[0]);
      } else {
        batchBody.writeArray(records);
      }
    } else {
      const compressedRecords = await compressRecords(compression, records);
      batchBody.writeInt32(records.length).writeBuffer(compressedRecords);
    }
    const batch = new Encoder().writeInt32(partitionLeaderEpoch).writeInt8(MAGIC_BYTE).writeUInt32(crc32C(batchBody.buffer)).writeEncoder(batchBody);
    return new Encoder().writeInt64(firstOffset).writeBytes(batch.buffer);
  };
  var compressRecords = async (compression, records) => {
    const codec = lookupCodec(compression);
    const recordsEncoder = new Encoder;
    recordsEncoder.writeEncoderArray(records);
    return codec.compress(recordsEncoder);
  };
  module.exports = {
    RecordBatch,
    MAGIC_BYTE
  };
});

// node_modules/kafkajs/src/protocol/requests/produce/v3/request.js
var require_request4 = __commonJS((exports, module) => {
  var Long = require_long();
  var Encoder = require_encoder();
  var { Produce: apiKey } = require_apiKeys();
  var { Types } = require_compression();
  var Record = require_v03();
  var { RecordBatch } = require_v04();
  module.exports = ({
    acks,
    timeout,
    transactionalId = null,
    producerId = Long.fromInt(-1),
    producerEpoch = 0,
    compression = Types.None,
    topicData
  }) => ({
    apiKey,
    apiVersion: 3,
    apiName: "Produce",
    expectResponse: () => acks !== 0,
    encode: async () => {
      const encodeTopic = topicEncoder(compression);
      const encodedTopicData = [];
      for (const data of topicData) {
        encodedTopicData.push(await encodeTopic({ ...data, transactionalId, producerId, producerEpoch }));
      }
      return new Encoder().writeString(transactionalId).writeInt16(acks).writeInt32(timeout).writeArray(encodedTopicData);
    }
  });
  var topicEncoder = (compression) => async ({
    topic,
    partitions,
    transactionalId,
    producerId,
    producerEpoch
  }) => {
    const encodePartitions = partitionsEncoder(compression);
    const encodedPartitions = [];
    for (const data of partitions) {
      encodedPartitions.push(await encodePartitions({ ...data, transactionalId, producerId, producerEpoch }));
    }
    return new Encoder().writeString(topic).writeArray(encodedPartitions);
  };
  var partitionsEncoder = (compression) => async ({
    partition,
    messages,
    transactionalId,
    firstSequence,
    producerId,
    producerEpoch
  }) => {
    const dateNow = Date.now();
    const messageTimestamps = messages.map((m) => m.timestamp).filter((timestamp) => timestamp != null).sort();
    const timestamps = messageTimestamps.length === 0 ? [dateNow] : messageTimestamps;
    const firstTimestamp = timestamps[0];
    const maxTimestamp = timestamps[timestamps.length - 1];
    const records = messages.map((message, i) => Record({
      ...message,
      offsetDelta: i,
      timestampDelta: (message.timestamp || dateNow) - firstTimestamp
    }));
    const recordBatch = await RecordBatch({
      compression,
      records,
      firstTimestamp,
      maxTimestamp,
      producerId,
      producerEpoch,
      firstSequence,
      transactional: !!transactionalId,
      lastOffsetDelta: records.length - 1
    });
    return new Encoder().writeInt32(partition).writeInt32(recordBatch.size()).writeEncoder(recordBatch);
  };
});

// node_modules/kafkajs/src/protocol/requests/produce/v3/response.js
var require_response4 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode } = require_error();
  var partition = (decoder) => ({
    partition: decoder.readInt32(),
    errorCode: decoder.readInt16(),
    baseOffset: decoder.readInt64().toString(),
    logAppendTime: decoder.readInt64().toString()
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const topics = decoder.readArray((decoder2) => ({
      topicName: decoder2.readString(),
      partitions: decoder2.readArray(partition)
    }));
    const throttleTime = decoder.readInt32();
    return {
      topics,
      throttleTime
    };
  };
  var parse = async (data) => {
    const errors = data.topics.flatMap((response) => {
      return response.partitions.filter((partition2) => failure(partition2.errorCode));
    });
    if (errors.length > 0) {
      const { errorCode } = errors[0];
      throw createErrorFromCode(errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/produce/v4/request.js
var require_request5 = __commonJS((exports, module) => {
  var requestV3 = require_request4();
  module.exports = ({
    acks,
    timeout,
    transactionalId,
    producerId,
    producerEpoch,
    compression,
    topicData
  }) => Object.assign(requestV3({
    acks,
    timeout,
    transactionalId,
    producerId,
    producerEpoch,
    compression,
    topicData
  }), { apiVersion: 4 });
});

// node_modules/kafkajs/src/protocol/requests/produce/v4/response.js
var require_response5 = __commonJS((exports, module) => {
  var { decode, parse } = require_response4();
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/produce/v5/request.js
var require_request6 = __commonJS((exports, module) => {
  var requestV3 = require_request4();
  module.exports = ({
    acks,
    timeout,
    transactionalId,
    producerId,
    producerEpoch,
    compression,
    topicData
  }) => Object.assign(requestV3({
    acks,
    timeout,
    transactionalId,
    producerId,
    producerEpoch,
    compression,
    topicData
  }), { apiVersion: 5 });
});

// node_modules/kafkajs/src/protocol/requests/produce/v5/response.js
var require_response6 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { parse: parseV3 } = require_response4();
  var partition = (decoder) => ({
    partition: decoder.readInt32(),
    errorCode: decoder.readInt16(),
    baseOffset: decoder.readInt64().toString(),
    logAppendTime: decoder.readInt64().toString(),
    logStartOffset: decoder.readInt64().toString()
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const topics = decoder.readArray((decoder2) => ({
      topicName: decoder2.readString(),
      partitions: decoder2.readArray(partition)
    }));
    const throttleTime = decoder.readInt32();
    return {
      topics,
      throttleTime
    };
  };
  module.exports = {
    decode,
    parse: parseV3
  };
});

// node_modules/kafkajs/src/protocol/requests/produce/v6/request.js
var require_request7 = __commonJS((exports, module) => {
  var requestV5 = require_request6();
  module.exports = ({
    acks,
    timeout,
    transactionalId,
    producerId,
    producerEpoch,
    compression,
    topicData
  }) => Object.assign(requestV5({
    acks,
    timeout,
    transactionalId,
    producerId,
    producerEpoch,
    compression,
    topicData
  }), { apiVersion: 6 });
});

// node_modules/kafkajs/src/protocol/requests/produce/v6/response.js
var require_response7 = __commonJS((exports, module) => {
  var { parse, decode: decodeV5 } = require_response6();
  var decode = async (rawData) => {
    const decoded = await decodeV5(rawData);
    return {
      ...decoded,
      throttleTime: 0,
      clientSideThrottleTime: decoded.throttleTime
    };
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/produce/v7/request.js
var require_request8 = __commonJS((exports, module) => {
  var requestV6 = require_request7();
  module.exports = ({
    acks,
    timeout,
    transactionalId,
    producerId,
    producerEpoch,
    compression,
    topicData
  }) => Object.assign(requestV6({
    acks,
    timeout,
    transactionalId,
    producerId,
    producerEpoch,
    compression,
    topicData
  }), { apiVersion: 7 });
});

// node_modules/kafkajs/src/protocol/requests/produce/v7/response.js
var require_response8 = __commonJS((exports, module) => {
  var { decode, parse } = require_response7();
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/produce/index.js
var require_produce = __commonJS((exports, module) => {
  var versions = {
    0: ({ acks, timeout, topicData }) => {
      const request3 = require_request();
      const response = require_response();
      return { request: request3({ acks, timeout, topicData }), response };
    },
    1: ({ acks, timeout, topicData }) => {
      const request3 = require_request2();
      const response = require_response2();
      return { request: request3({ acks, timeout, topicData }), response };
    },
    2: ({ acks, timeout, topicData, compression }) => {
      const request3 = require_request3();
      const response = require_response3();
      return { request: request3({ acks, timeout, compression, topicData }), response };
    },
    3: ({ acks, timeout, compression, topicData, transactionalId, producerId, producerEpoch }) => {
      const request3 = require_request4();
      const response = require_response4();
      return {
        request: request3({
          acks,
          timeout,
          compression,
          topicData,
          transactionalId,
          producerId,
          producerEpoch
        }),
        response
      };
    },
    4: ({ acks, timeout, compression, topicData, transactionalId, producerId, producerEpoch }) => {
      const request3 = require_request5();
      const response = require_response5();
      return {
        request: request3({
          acks,
          timeout,
          compression,
          topicData,
          transactionalId,
          producerId,
          producerEpoch
        }),
        response
      };
    },
    5: ({ acks, timeout, compression, topicData, transactionalId, producerId, producerEpoch }) => {
      const request3 = require_request6();
      const response = require_response6();
      return {
        request: request3({
          acks,
          timeout,
          compression,
          topicData,
          transactionalId,
          producerId,
          producerEpoch
        }),
        response
      };
    },
    6: ({ acks, timeout, compression, topicData, transactionalId, producerId, producerEpoch }) => {
      const request3 = require_request7();
      const response = require_response7();
      return {
        request: request3({
          acks,
          timeout,
          compression,
          topicData,
          transactionalId,
          producerId,
          producerEpoch
        }),
        response
      };
    },
    7: ({ acks, timeout, compression, topicData, transactionalId, producerId, producerEpoch }) => {
      const request3 = require_request8();
      const response = require_response8();
      return {
        request: request3({
          acks,
          timeout,
          compression,
          topicData,
          transactionalId,
          producerId,
          producerEpoch
        }),
        response
      };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/isolationLevel.js
var require_isolationLevel = __commonJS((exports, module) => {
  module.exports = {
    READ_UNCOMMITTED: 0,
    READ_COMMITTED: 1
  };
});

// node_modules/kafkajs/src/protocol/requests/fetch/v0/request.js
var require_request9 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { Fetch: apiKey } = require_apiKeys();
  module.exports = ({ replicaId, maxWaitTime, minBytes, topics }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "Fetch",
    encode: async () => {
      return new Encoder().writeInt32(replicaId).writeInt32(maxWaitTime).writeInt32(minBytes).writeArray(topics.map(encodeTopic));
    }
  });
  var encodeTopic = ({ topic, partitions }) => {
    return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition));
  };
  var encodePartition = ({ partition, fetchOffset, maxBytes }) => {
    return new Encoder().writeInt32(partition).writeInt64(fetchOffset).writeInt32(maxBytes);
  };
});

// node_modules/kafkajs/src/protocol/message/v0/decoder.js
var require_decoder2 = __commonJS((exports, module) => {
  module.exports = (decoder) => ({
    attributes: decoder.readInt8(),
    key: decoder.readBytes(),
    value: decoder.readBytes()
  });
});

// node_modules/kafkajs/src/protocol/message/v1/decoder.js
var require_decoder3 = __commonJS((exports, module) => {
  module.exports = (decoder) => ({
    attributes: decoder.readInt8(),
    timestamp: decoder.readInt64().toString(),
    key: decoder.readBytes(),
    value: decoder.readBytes()
  });
});

// node_modules/kafkajs/src/protocol/message/decoder.js
var require_decoder4 = __commonJS((exports, module) => {
  var {
    KafkaJSPartialMessageError,
    KafkaJSUnsupportedMagicByteInMessageSet
  } = require_errors();
  var V0Decoder = require_decoder2();
  var V1Decoder = require_decoder3();
  var decodeMessage = (decoder, magicByte) => {
    switch (magicByte) {
      case 0:
        return V0Decoder(decoder);
      case 1:
        return V1Decoder(decoder);
      default:
        throw new KafkaJSUnsupportedMagicByteInMessageSet(`Unsupported MessageSet message version, magic byte: ${magicByte}`);
    }
  };
  module.exports = (offset, size, decoder) => {
    const remainingBytes = Buffer.byteLength(decoder.slice(size).buffer);
    if (remainingBytes < size) {
      throw new KafkaJSPartialMessageError(`Tried to decode a partial message: remainingBytes(${remainingBytes}) < messageSize(${size})`);
    }
    const crc = decoder.readInt32();
    const magicByte = decoder.readInt8();
    const message = decodeMessage(decoder, magicByte);
    return Object.assign({ offset, size, crc, magicByte }, message);
  };
});

// node_modules/kafkajs/src/protocol/messageSet/decoder.js
var require_decoder5 = __commonJS((exports, module) => {
  var Long = require_long();
  var Decoder = require_decoder();
  var MessageDecoder = require_decoder4();
  var { lookupCodecByAttributes } = require_compression();
  var { KafkaJSPartialMessageError } = require_errors();
  module.exports = async (primaryDecoder, size = null) => {
    const messages = [];
    const messageSetSize = size || primaryDecoder.readInt32();
    const messageSetDecoder = primaryDecoder.slice(messageSetSize);
    while (messageSetDecoder.offset < messageSetSize) {
      try {
        const message = EntryDecoder(messageSetDecoder);
        const codec = lookupCodecByAttributes(message.attributes);
        if (codec) {
          const buffer = await codec.decompress(message.value);
          messages.push(...EntriesDecoder(new Decoder(buffer), message));
        } else {
          messages.push(message);
        }
      } catch (e) {
        if (e.name === "KafkaJSPartialMessageError") {
          break;
        }
        if (e.name === "KafkaJSUnsupportedMagicByteInMessageSet") {
          break;
        }
        throw e;
      }
    }
    primaryDecoder.forward(messageSetSize);
    return messages;
  };
  var EntriesDecoder = (decoder, compressedMessage) => {
    const messages = [];
    while (decoder.offset < decoder.buffer.length) {
      messages.push(EntryDecoder(decoder));
    }
    if (compressedMessage.magicByte > 0 && compressedMessage.offset >= 0) {
      const compressedOffset = Long.fromValue(compressedMessage.offset);
      const lastMessageOffset = Long.fromValue(messages[messages.length - 1].offset);
      const baseOffset = compressedOffset - lastMessageOffset;
      for (const message of messages) {
        message.offset = Long.fromValue(message.offset).add(baseOffset).toString();
      }
    }
    return messages;
  };
  var EntryDecoder = (decoder) => {
    if (!decoder.canReadInt64()) {
      throw new KafkaJSPartialMessageError(`Tried to decode a partial message: There isn't enough bytes to read the offset`);
    }
    const offset = decoder.readInt64().toString();
    if (!decoder.canReadInt32()) {
      throw new KafkaJSPartialMessageError(`Tried to decode a partial message: There isn't enough bytes to read the message size`);
    }
    const size = decoder.readInt32();
    return MessageDecoder(offset, size, decoder);
  };
});

// node_modules/kafkajs/src/protocol/requests/fetch/v0/response.js
var require_response9 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { KafkaJSOffsetOutOfRange } = require_errors();
  var { failure, createErrorFromCode, errorCodes } = require_error();
  var MessageSetDecoder = require_decoder5();
  var decodePartition = async (decoder) => ({
    partition: decoder.readInt32(),
    errorCode: decoder.readInt16(),
    highWatermark: decoder.readInt64().toString(),
    messages: await MessageSetDecoder(decoder)
  });
  var decodeResponse = async (decoder) => ({
    topicName: decoder.readString(),
    partitions: await decoder.readArrayAsync(decodePartition)
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const responses = await decoder.readArrayAsync(decodeResponse);
    return {
      responses
    };
  };
  var { code: OFFSET_OUT_OF_RANGE_ERROR_CODE } = errorCodes.find((e) => e.type === "OFFSET_OUT_OF_RANGE");
  var parse = async (data) => {
    const errors = data.responses.flatMap(({ topicName, partitions }) => {
      return partitions.filter((partition) => failure(partition.errorCode)).map((partition) => Object.assign({}, partition, { topic: topicName }));
    });
    if (errors.length > 0) {
      const { errorCode, topic, partition } = errors[0];
      if (errorCode === OFFSET_OUT_OF_RANGE_ERROR_CODE) {
        throw new KafkaJSOffsetOutOfRange(createErrorFromCode(errorCode), { topic, partition });
      }
      throw createErrorFromCode(errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/fetch/v1/request.js
var require_request10 = __commonJS((exports, module) => {
  var requestV0 = require_request9();
  module.exports = ({ replicaId, maxWaitTime, minBytes, topics }) => {
    return Object.assign(requestV0({ replicaId, maxWaitTime, minBytes, topics }), { apiVersion: 1 });
  };
});

// node_modules/kafkajs/src/protocol/requests/fetch/v1/response.js
var require_response10 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { parse: parseV0 } = require_response9();
  var MessageSetDecoder = require_decoder5();
  var decodePartition = async (decoder) => ({
    partition: decoder.readInt32(),
    errorCode: decoder.readInt16(),
    highWatermark: decoder.readInt64().toString(),
    messages: await MessageSetDecoder(decoder)
  });
  var decodeResponse = async (decoder) => ({
    topicName: decoder.readString(),
    partitions: await decoder.readArrayAsync(decodePartition)
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    const responses = await decoder.readArrayAsync(decodeResponse);
    return {
      throttleTime,
      responses
    };
  };
  module.exports = {
    decode,
    parse: parseV0
  };
});

// node_modules/kafkajs/src/protocol/requests/fetch/v2/request.js
var require_request11 = __commonJS((exports, module) => {
  var requestV0 = require_request9();
  module.exports = ({ replicaId, maxWaitTime, minBytes, topics }) => {
    return Object.assign(requestV0({ replicaId, maxWaitTime, minBytes, topics }), { apiVersion: 2 });
  };
});

// node_modules/kafkajs/src/protocol/requests/fetch/v2/response.js
var require_response11 = __commonJS((exports, module) => {
  var { decode, parse } = require_response10();
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/fetch/v3/request.js
var require_request12 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { Fetch: apiKey } = require_apiKeys();
  module.exports = ({ replicaId, maxWaitTime, minBytes, maxBytes, topics }) => ({
    apiKey,
    apiVersion: 3,
    apiName: "Fetch",
    encode: async () => {
      return new Encoder().writeInt32(replicaId).writeInt32(maxWaitTime).writeInt32(minBytes).writeInt32(maxBytes).writeArray(topics.map(encodeTopic));
    }
  });
  var encodeTopic = ({ topic, partitions }) => {
    return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition));
  };
  var encodePartition = ({ partition, fetchOffset, maxBytes }) => {
    return new Encoder().writeInt32(partition).writeInt64(fetchOffset).writeInt32(maxBytes);
  };
});

// node_modules/kafkajs/src/protocol/requests/fetch/v3/response.js
var require_response12 = __commonJS((exports, module) => {
  var { decode, parse } = require_response10();
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/fetch/v4/request.js
var require_request13 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { Fetch: apiKey } = require_apiKeys();
  var ISOLATION_LEVEL = require_isolationLevel();
  module.exports = ({
    replicaId,
    maxWaitTime,
    minBytes,
    maxBytes,
    topics,
    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED
  }) => ({
    apiKey,
    apiVersion: 4,
    apiName: "Fetch",
    encode: async () => {
      return new Encoder().writeInt32(replicaId).writeInt32(maxWaitTime).writeInt32(minBytes).writeInt32(maxBytes).writeInt8(isolationLevel).writeArray(topics.map(encodeTopic));
    }
  });
  var encodeTopic = ({ topic, partitions }) => {
    return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition));
  };
  var encodePartition = ({ partition, fetchOffset, maxBytes }) => {
    return new Encoder().writeInt32(partition).writeInt64(fetchOffset).writeInt32(maxBytes);
  };
});

// node_modules/kafkajs/src/protocol/recordBatch/header/v0/decoder.js
var require_decoder6 = __commonJS((exports, module) => {
  module.exports = (decoder) => ({
    key: decoder.readVarIntString(),
    value: decoder.readVarIntBytes()
  });
});

// node_modules/kafkajs/src/protocol/timestampTypes.js
var require_timestampTypes = __commonJS((exports, module) => {
  module.exports = {
    NO_TIMESTAMP: -1,
    CREATE_TIME: 0,
    LOG_APPEND_TIME: 1
  };
});

// node_modules/kafkajs/src/protocol/recordBatch/record/v0/decoder.js
var require_decoder7 = __commonJS((exports, module) => {
  var Long = require_long();
  var HeaderDecoder = require_decoder6();
  var TimestampTypes = require_timestampTypes();
  module.exports = (decoder, batchContext = {}) => {
    const {
      firstOffset,
      firstTimestamp,
      magicByte,
      isControlBatch = false,
      timestampType,
      maxTimestamp
    } = batchContext;
    const attributes = decoder.readInt8();
    const timestampDelta = decoder.readVarLong();
    const timestamp = timestampType === TimestampTypes.LOG_APPEND_TIME && maxTimestamp ? maxTimestamp : Long.fromValue(firstTimestamp).add(timestampDelta).toString();
    const offsetDelta = decoder.readVarInt();
    const offset = Long.fromValue(firstOffset).add(offsetDelta).toString();
    const key = decoder.readVarIntBytes();
    const value = decoder.readVarIntBytes();
    const headers = decoder.readVarIntArray(HeaderDecoder).reduce((obj, { key: key2, value: value2 }) => ({
      ...obj,
      [key2]: obj[key2] === undefined ? value2 : Array.isArray(obj[key2]) ? obj[key2].concat([value2]) : [obj[key2], value2]
    }), {});
    return {
      magicByte,
      attributes,
      timestamp,
      offset,
      key,
      value,
      headers,
      isControlRecord: isControlBatch,
      batchContext
    };
  };
});

// node_modules/kafkajs/src/protocol/recordBatch/v0/decoder.js
var require_decoder8 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { KafkaJSPartialMessageError } = require_errors();
  var { lookupCodecByAttributes } = require_compression();
  var RecordDecoder = require_decoder7();
  var TimestampTypes = require_timestampTypes();
  var TIMESTAMP_TYPE_FLAG_MASK = 8;
  var TRANSACTIONAL_FLAG_MASK = 16;
  var CONTROL_FLAG_MASK = 32;
  module.exports = async (fetchDecoder) => {
    const firstOffset = fetchDecoder.readInt64().toString();
    const length = fetchDecoder.readInt32();
    const decoder = fetchDecoder.slice(length);
    fetchDecoder.forward(length);
    const remainingBytes = Buffer.byteLength(decoder.buffer);
    if (remainingBytes < length) {
      throw new KafkaJSPartialMessageError(`Tried to decode a partial record batch: remainingBytes(${remainingBytes}) < recordBatchLength(${length})`);
    }
    const partitionLeaderEpoch = decoder.readInt32();
    const magicByte = decoder.readInt8();
    const crc = decoder.readInt32();
    const attributes = decoder.readInt16();
    const lastOffsetDelta = decoder.readInt32();
    const firstTimestamp = decoder.readInt64().toString();
    const maxTimestamp = decoder.readInt64().toString();
    const producerId = decoder.readInt64().toString();
    const producerEpoch = decoder.readInt16();
    const firstSequence = decoder.readInt32();
    const inTransaction = (attributes & TRANSACTIONAL_FLAG_MASK) > 0;
    const isControlBatch = (attributes & CONTROL_FLAG_MASK) > 0;
    const timestampType = (attributes & TIMESTAMP_TYPE_FLAG_MASK) > 0 ? TimestampTypes.LOG_APPEND_TIME : TimestampTypes.CREATE_TIME;
    const codec = lookupCodecByAttributes(attributes);
    const recordContext = {
      firstOffset,
      firstTimestamp,
      partitionLeaderEpoch,
      inTransaction,
      isControlBatch,
      lastOffsetDelta,
      producerId,
      producerEpoch,
      firstSequence,
      maxTimestamp,
      timestampType
    };
    const records = await decodeRecords(codec, decoder, { ...recordContext, magicByte });
    return {
      ...recordContext,
      records
    };
  };
  var decodeRecords = async (codec, recordsDecoder, recordContext) => {
    if (!codec) {
      return recordsDecoder.readArray((decoder) => decodeRecord(decoder, recordContext));
    }
    const length = recordsDecoder.readInt32();
    if (length <= 0) {
      return [];
    }
    const compressedRecordsBuffer = recordsDecoder.readAll();
    const decompressedRecordBuffer = await codec.decompress(compressedRecordsBuffer);
    const decompressedRecordDecoder = new Decoder(decompressedRecordBuffer);
    const records = new Array(length);
    for (let i = 0;i < length; i++) {
      records[i] = decodeRecord(decompressedRecordDecoder, recordContext);
    }
    return records;
  };
  var decodeRecord = (decoder, recordContext) => {
    const recordBuffer = decoder.readVarIntBytes();
    return RecordDecoder(new Decoder(recordBuffer), recordContext);
  };
});

// node_modules/kafkajs/src/protocol/requests/fetch/v4/decodeMessages.js
var require_decodeMessages = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var MessageSetDecoder = require_decoder5();
  var RecordBatchDecoder = require_decoder8();
  var { MAGIC_BYTE } = require_v04();
  var MAGIC_OFFSET = 16;
  var RECORD_BATCH_OVERHEAD = 49;
  var decodeMessages = async (decoder) => {
    const messagesSize = decoder.readInt32();
    if (messagesSize <= 0 || !decoder.canReadBytes(messagesSize)) {
      return [];
    }
    const messagesBuffer = decoder.readBytes(messagesSize);
    const messagesDecoder = new Decoder(messagesBuffer);
    const magicByte = messagesBuffer.slice(MAGIC_OFFSET).readInt8(0);
    if (magicByte === MAGIC_BYTE) {
      const records = [];
      while (messagesDecoder.canReadBytes(RECORD_BATCH_OVERHEAD)) {
        try {
          const recordBatch = await RecordBatchDecoder(messagesDecoder);
          records.push(...recordBatch.records);
        } catch (e) {
          if (e.name === "KafkaJSPartialMessageError") {
            break;
          }
          throw e;
        }
      }
      return records;
    }
    return MessageSetDecoder(messagesDecoder, messagesSize);
  };
  module.exports = decodeMessages;
});

// node_modules/kafkajs/src/protocol/requests/fetch/v4/response.js
var require_response13 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { parse: parseV1 } = require_response10();
  var decodeMessages = require_decodeMessages();
  var decodeAbortedTransactions = (decoder) => ({
    producerId: decoder.readInt64().toString(),
    firstOffset: decoder.readInt64().toString()
  });
  var decodePartition = async (decoder) => ({
    partition: decoder.readInt32(),
    errorCode: decoder.readInt16(),
    highWatermark: decoder.readInt64().toString(),
    lastStableOffset: decoder.readInt64().toString(),
    abortedTransactions: decoder.readArray(decodeAbortedTransactions),
    messages: await decodeMessages(decoder)
  });
  var decodeResponse = async (decoder) => ({
    topicName: decoder.readString(),
    partitions: await decoder.readArrayAsync(decodePartition)
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    const responses = await decoder.readArrayAsync(decodeResponse);
    return {
      throttleTime,
      responses
    };
  };
  module.exports = {
    decode,
    parse: parseV1
  };
});

// node_modules/kafkajs/src/protocol/requests/fetch/v5/request.js
var require_request14 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { Fetch: apiKey } = require_apiKeys();
  var ISOLATION_LEVEL = require_isolationLevel();
  module.exports = ({
    replicaId,
    maxWaitTime,
    minBytes,
    maxBytes,
    topics,
    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED
  }) => ({
    apiKey,
    apiVersion: 5,
    apiName: "Fetch",
    encode: async () => {
      return new Encoder().writeInt32(replicaId).writeInt32(maxWaitTime).writeInt32(minBytes).writeInt32(maxBytes).writeInt8(isolationLevel).writeArray(topics.map(encodeTopic));
    }
  });
  var encodeTopic = ({ topic, partitions }) => {
    return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition));
  };
  var encodePartition = ({ partition, fetchOffset, logStartOffset = -1, maxBytes }) => {
    return new Encoder().writeInt32(partition).writeInt64(fetchOffset).writeInt64(logStartOffset).writeInt32(maxBytes);
  };
});

// node_modules/kafkajs/src/protocol/requests/fetch/v5/response.js
var require_response14 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { parse: parseV1 } = require_response10();
  var decodeMessages = require_decodeMessages();
  var decodeAbortedTransactions = (decoder) => ({
    producerId: decoder.readInt64().toString(),
    firstOffset: decoder.readInt64().toString()
  });
  var decodePartition = async (decoder) => ({
    partition: decoder.readInt32(),
    errorCode: decoder.readInt16(),
    highWatermark: decoder.readInt64().toString(),
    lastStableOffset: decoder.readInt64().toString(),
    lastStartOffset: decoder.readInt64().toString(),
    abortedTransactions: decoder.readArray(decodeAbortedTransactions),
    messages: await decodeMessages(decoder)
  });
  var decodeResponse = async (decoder) => ({
    topicName: decoder.readString(),
    partitions: await decoder.readArrayAsync(decodePartition)
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    const responses = await decoder.readArrayAsync(decodeResponse);
    return {
      throttleTime,
      responses
    };
  };
  module.exports = {
    decode,
    parse: parseV1
  };
});

// node_modules/kafkajs/src/protocol/requests/fetch/v6/request.js
var require_request15 = __commonJS((exports, module) => {
  var ISOLATION_LEVEL = require_isolationLevel();
  var requestV5 = require_request14();
  module.exports = ({
    replicaId,
    maxWaitTime,
    minBytes,
    maxBytes,
    topics,
    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED
  }) => Object.assign(requestV5({
    replicaId,
    maxWaitTime,
    minBytes,
    maxBytes,
    topics,
    isolationLevel
  }), { apiVersion: 6 });
});

// node_modules/kafkajs/src/protocol/requests/fetch/v6/response.js
var require_response15 = __commonJS((exports, module) => {
  var { decode, parse } = require_response14();
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/fetch/v7/request.js
var require_request16 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { Fetch: apiKey } = require_apiKeys();
  var ISOLATION_LEVEL = require_isolationLevel();
  module.exports = ({
    replicaId,
    maxWaitTime,
    minBytes,
    maxBytes,
    topics,
    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,
    sessionId = 0,
    sessionEpoch = -1,
    forgottenTopics = []
  }) => ({
    apiKey,
    apiVersion: 7,
    apiName: "Fetch",
    encode: async () => {
      return new Encoder().writeInt32(replicaId).writeInt32(maxWaitTime).writeInt32(minBytes).writeInt32(maxBytes).writeInt8(isolationLevel).writeInt32(sessionId).writeInt32(sessionEpoch).writeArray(topics.map(encodeTopic)).writeArray(forgottenTopics.map(encodeForgottenTopics));
    }
  });
  var encodeForgottenTopics = ({ topic, partitions }) => {
    return new Encoder().writeString(topic).writeArray(partitions);
  };
  var encodeTopic = ({ topic, partitions }) => {
    return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition));
  };
  var encodePartition = ({ partition, fetchOffset, logStartOffset = -1, maxBytes }) => {
    return new Encoder().writeInt32(partition).writeInt64(fetchOffset).writeInt64(logStartOffset).writeInt32(maxBytes);
  };
});

// node_modules/kafkajs/src/protocol/requests/fetch/v7/response.js
var require_response16 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { parse: parseV1 } = require_response10();
  var decodeMessages = require_decodeMessages();
  var decodeAbortedTransactions = (decoder) => ({
    producerId: decoder.readInt64().toString(),
    firstOffset: decoder.readInt64().toString()
  });
  var decodePartition = async (decoder) => ({
    partition: decoder.readInt32(),
    errorCode: decoder.readInt16(),
    highWatermark: decoder.readInt64().toString(),
    lastStableOffset: decoder.readInt64().toString(),
    lastStartOffset: decoder.readInt64().toString(),
    abortedTransactions: decoder.readArray(decodeAbortedTransactions),
    messages: await decodeMessages(decoder)
  });
  var decodeResponse = async (decoder) => ({
    topicName: decoder.readString(),
    partitions: await decoder.readArrayAsync(decodePartition)
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    const errorCode = decoder.readInt16();
    const sessionId = decoder.readInt32();
    const responses = await decoder.readArrayAsync(decodeResponse);
    return {
      throttleTime,
      errorCode,
      sessionId,
      responses
    };
  };
  module.exports = {
    decode,
    parse: parseV1
  };
});

// node_modules/kafkajs/src/protocol/requests/fetch/v8/request.js
var require_request17 = __commonJS((exports, module) => {
  var ISOLATION_LEVEL = require_isolationLevel();
  var requestV7 = require_request16();
  module.exports = ({
    replicaId,
    maxWaitTime,
    minBytes,
    maxBytes,
    topics,
    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,
    sessionId = 0,
    sessionEpoch = -1,
    forgottenTopics = []
  }) => Object.assign(requestV7({
    replicaId,
    maxWaitTime,
    minBytes,
    maxBytes,
    topics,
    isolationLevel,
    sessionId,
    sessionEpoch,
    forgottenTopics
  }), { apiVersion: 8 });
});

// node_modules/kafkajs/src/protocol/requests/fetch/v8/response.js
var require_response17 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { parse: parseV1 } = require_response10();
  var decodeMessages = require_decodeMessages();
  var decodeAbortedTransactions = (decoder) => ({
    producerId: decoder.readInt64().toString(),
    firstOffset: decoder.readInt64().toString()
  });
  var decodePartition = async (decoder) => ({
    partition: decoder.readInt32(),
    errorCode: decoder.readInt16(),
    highWatermark: decoder.readInt64().toString(),
    lastStableOffset: decoder.readInt64().toString(),
    lastStartOffset: decoder.readInt64().toString(),
    abortedTransactions: decoder.readArray(decodeAbortedTransactions),
    messages: await decodeMessages(decoder)
  });
  var decodeResponse = async (decoder) => ({
    topicName: decoder.readString(),
    partitions: await decoder.readArrayAsync(decodePartition)
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const clientSideThrottleTime = decoder.readInt32();
    const errorCode = decoder.readInt16();
    const sessionId = decoder.readInt32();
    const responses = await decoder.readArrayAsync(decodeResponse);
    return {
      throttleTime: 0,
      clientSideThrottleTime,
      errorCode,
      sessionId,
      responses
    };
  };
  module.exports = {
    decode,
    parse: parseV1
  };
});

// node_modules/kafkajs/src/protocol/requests/fetch/v9/request.js
var require_request18 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { Fetch: apiKey } = require_apiKeys();
  var ISOLATION_LEVEL = require_isolationLevel();
  module.exports = ({
    replicaId,
    maxWaitTime,
    minBytes,
    maxBytes,
    topics,
    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,
    sessionId = 0,
    sessionEpoch = -1,
    forgottenTopics = []
  }) => ({
    apiKey,
    apiVersion: 9,
    apiName: "Fetch",
    encode: async () => {
      return new Encoder().writeInt32(replicaId).writeInt32(maxWaitTime).writeInt32(minBytes).writeInt32(maxBytes).writeInt8(isolationLevel).writeInt32(sessionId).writeInt32(sessionEpoch).writeArray(topics.map(encodeTopic)).writeArray(forgottenTopics.map(encodeForgottenTopics));
    }
  });
  var encodeForgottenTopics = ({ topic, partitions }) => {
    return new Encoder().writeString(topic).writeArray(partitions);
  };
  var encodeTopic = ({ topic, partitions }) => {
    return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition));
  };
  var encodePartition = ({
    partition,
    currentLeaderEpoch = -1,
    fetchOffset,
    logStartOffset = -1,
    maxBytes
  }) => {
    return new Encoder().writeInt32(partition).writeInt32(currentLeaderEpoch).writeInt64(fetchOffset).writeInt64(logStartOffset).writeInt32(maxBytes);
  };
});

// node_modules/kafkajs/src/protocol/requests/fetch/v9/response.js
var require_response18 = __commonJS((exports, module) => {
  var { decode, parse } = require_response17();
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/fetch/v10/request.js
var require_request19 = __commonJS((exports, module) => {
  var ISOLATION_LEVEL = require_isolationLevel();
  var requestV9 = require_request18();
  module.exports = ({
    replicaId,
    maxWaitTime,
    minBytes,
    maxBytes,
    topics,
    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,
    sessionId = 0,
    sessionEpoch = -1,
    forgottenTopics = []
  }) => Object.assign(requestV9({
    replicaId,
    maxWaitTime,
    minBytes,
    maxBytes,
    topics,
    isolationLevel,
    sessionId,
    sessionEpoch,
    forgottenTopics
  }), { apiVersion: 10 });
});

// node_modules/kafkajs/src/protocol/requests/fetch/v10/response.js
var require_response19 = __commonJS((exports, module) => {
  var { decode, parse } = require_response18();
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/fetch/v11/request.js
var require_request20 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { Fetch: apiKey } = require_apiKeys();
  var ISOLATION_LEVEL = require_isolationLevel();
  module.exports = ({
    replicaId,
    maxWaitTime,
    minBytes,
    maxBytes,
    topics,
    rackId = "",
    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,
    sessionId = 0,
    sessionEpoch = -1,
    forgottenTopics = []
  }) => ({
    apiKey,
    apiVersion: 11,
    apiName: "Fetch",
    encode: async () => {
      return new Encoder().writeInt32(replicaId).writeInt32(maxWaitTime).writeInt32(minBytes).writeInt32(maxBytes).writeInt8(isolationLevel).writeInt32(sessionId).writeInt32(sessionEpoch).writeArray(topics.map(encodeTopic)).writeArray(forgottenTopics.map(encodeForgottenTopics)).writeString(rackId);
    }
  });
  var encodeForgottenTopics = ({ topic, partitions }) => {
    return new Encoder().writeString(topic).writeArray(partitions);
  };
  var encodeTopic = ({ topic, partitions }) => {
    return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition));
  };
  var encodePartition = ({
    partition,
    currentLeaderEpoch = -1,
    fetchOffset,
    logStartOffset = -1,
    maxBytes
  }) => {
    return new Encoder().writeInt32(partition).writeInt32(currentLeaderEpoch).writeInt64(fetchOffset).writeInt64(logStartOffset).writeInt32(maxBytes);
  };
});

// node_modules/kafkajs/src/protocol/requests/fetch/v11/response.js
var require_response20 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { parse: parseV1 } = require_response10();
  var decodeMessages = require_decodeMessages();
  var decodeAbortedTransactions = (decoder) => ({
    producerId: decoder.readInt64().toString(),
    firstOffset: decoder.readInt64().toString()
  });
  var decodePartition = async (decoder) => ({
    partition: decoder.readInt32(),
    errorCode: decoder.readInt16(),
    highWatermark: decoder.readInt64().toString(),
    lastStableOffset: decoder.readInt64().toString(),
    lastStartOffset: decoder.readInt64().toString(),
    abortedTransactions: decoder.readArray(decodeAbortedTransactions),
    preferredReadReplica: decoder.readInt32(),
    messages: await decodeMessages(decoder)
  });
  var decodeResponse = async (decoder) => ({
    topicName: decoder.readString(),
    partitions: await decoder.readArrayAsync(decodePartition)
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const clientSideThrottleTime = decoder.readInt32();
    const errorCode = decoder.readInt16();
    const sessionId = decoder.readInt32();
    const responses = await decoder.readArrayAsync(decodeResponse);
    return {
      throttleTime: 0,
      clientSideThrottleTime,
      errorCode,
      sessionId,
      responses
    };
  };
  module.exports = {
    decode,
    parse: parseV1
  };
});

// node_modules/kafkajs/src/protocol/requests/fetch/index.js
var require_fetch = __commonJS((exports, module) => {
  var ISOLATION_LEVEL = require_isolationLevel();
  var REPLICA_ID = -1;
  var NETWORK_DELAY = 100;
  var requestTimeout = (timeout) => Number.isSafeInteger(timeout + NETWORK_DELAY) ? timeout + NETWORK_DELAY : timeout;
  var versions = {
    0: ({ replicaId = REPLICA_ID, maxWaitTime, minBytes, topics }) => {
      const request3 = require_request9();
      const response = require_response9();
      return {
        request: request3({ replicaId, maxWaitTime, minBytes, topics }),
        response,
        requestTimeout: requestTimeout(maxWaitTime)
      };
    },
    1: ({ replicaId = REPLICA_ID, maxWaitTime, minBytes, topics }) => {
      const request3 = require_request10();
      const response = require_response10();
      return {
        request: request3({ replicaId, maxWaitTime, minBytes, topics }),
        response,
        requestTimeout: requestTimeout(maxWaitTime)
      };
    },
    2: ({ replicaId = REPLICA_ID, maxWaitTime, minBytes, topics }) => {
      const request3 = require_request11();
      const response = require_response11();
      return {
        request: request3({ replicaId, maxWaitTime, minBytes, topics }),
        response,
        requestTimeout: requestTimeout(maxWaitTime)
      };
    },
    3: ({ replicaId = REPLICA_ID, maxWaitTime, minBytes, maxBytes, topics }) => {
      const request3 = require_request12();
      const response = require_response12();
      return {
        request: request3({ replicaId, maxWaitTime, minBytes, maxBytes, topics }),
        response,
        requestTimeout: requestTimeout(maxWaitTime)
      };
    },
    4: ({
      replicaId = REPLICA_ID,
      isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,
      maxWaitTime,
      minBytes,
      maxBytes,
      topics
    }) => {
      const request3 = require_request13();
      const response = require_response13();
      return {
        request: request3({ replicaId, isolationLevel, maxWaitTime, minBytes, maxBytes, topics }),
        response,
        requestTimeout: requestTimeout(maxWaitTime)
      };
    },
    5: ({
      replicaId = REPLICA_ID,
      isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,
      maxWaitTime,
      minBytes,
      maxBytes,
      topics
    }) => {
      const request3 = require_request14();
      const response = require_response14();
      return {
        request: request3({ replicaId, isolationLevel, maxWaitTime, minBytes, maxBytes, topics }),
        response,
        requestTimeout: requestTimeout(maxWaitTime)
      };
    },
    6: ({
      replicaId = REPLICA_ID,
      isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,
      maxWaitTime,
      minBytes,
      maxBytes,
      topics
    }) => {
      const request3 = require_request15();
      const response = require_response15();
      return {
        request: request3({ replicaId, isolationLevel, maxWaitTime, minBytes, maxBytes, topics }),
        response,
        requestTimeout: requestTimeout(maxWaitTime)
      };
    },
    7: ({
      replicaId = REPLICA_ID,
      isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,
      sessionId = 0,
      sessionEpoch = -1,
      forgottenTopics = [],
      maxWaitTime,
      minBytes,
      maxBytes,
      topics
    }) => {
      const request3 = require_request16();
      const response = require_response16();
      return {
        request: request3({
          replicaId,
          isolationLevel,
          sessionId,
          sessionEpoch,
          forgottenTopics,
          maxWaitTime,
          minBytes,
          maxBytes,
          topics
        }),
        response,
        requestTimeout: requestTimeout(maxWaitTime)
      };
    },
    8: ({
      replicaId = REPLICA_ID,
      isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,
      sessionId = 0,
      sessionEpoch = -1,
      forgottenTopics = [],
      maxWaitTime,
      minBytes,
      maxBytes,
      topics
    }) => {
      const request3 = require_request17();
      const response = require_response17();
      return {
        request: request3({
          replicaId,
          isolationLevel,
          sessionId,
          sessionEpoch,
          forgottenTopics,
          maxWaitTime,
          minBytes,
          maxBytes,
          topics
        }),
        response,
        requestTimeout: requestTimeout(maxWaitTime)
      };
    },
    9: ({
      replicaId = REPLICA_ID,
      isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,
      sessionId = 0,
      sessionEpoch = -1,
      forgottenTopics = [],
      maxWaitTime,
      minBytes,
      maxBytes,
      topics
    }) => {
      const request3 = require_request18();
      const response = require_response18();
      return {
        request: request3({
          replicaId,
          isolationLevel,
          sessionId,
          sessionEpoch,
          forgottenTopics,
          maxWaitTime,
          minBytes,
          maxBytes,
          topics
        }),
        response,
        requestTimeout: requestTimeout(maxWaitTime)
      };
    },
    10: ({
      replicaId = REPLICA_ID,
      isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,
      sessionId = 0,
      sessionEpoch = -1,
      forgottenTopics = [],
      maxWaitTime,
      minBytes,
      maxBytes,
      topics
    }) => {
      const request3 = require_request19();
      const response = require_response19();
      return {
        request: request3({
          replicaId,
          isolationLevel,
          sessionId,
          sessionEpoch,
          forgottenTopics,
          maxWaitTime,
          minBytes,
          maxBytes,
          topics
        }),
        response,
        requestTimeout: requestTimeout(maxWaitTime)
      };
    },
    11: ({
      replicaId = REPLICA_ID,
      isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,
      sessionId = 0,
      sessionEpoch = -1,
      forgottenTopics = [],
      maxWaitTime,
      minBytes,
      maxBytes,
      topics,
      rackId
    }) => {
      const request3 = require_request20();
      const response = require_response20();
      return {
        request: request3({
          replicaId,
          isolationLevel,
          sessionId,
          sessionEpoch,
          forgottenTopics,
          maxWaitTime,
          minBytes,
          maxBytes,
          topics,
          rackId
        }),
        response,
        requestTimeout: requestTimeout(maxWaitTime)
      };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/listOffsets/v0/request.js
var require_request21 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { ListOffsets: apiKey } = require_apiKeys();
  module.exports = ({ replicaId, topics }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "ListOffsets",
    encode: async () => {
      return new Encoder().writeInt32(replicaId).writeArray(topics.map(encodeTopic));
    }
  });
  var encodeTopic = ({ topic, partitions }) => {
    return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition));
  };
  var encodePartition = ({ partition, timestamp = -1, maxNumOffsets = 1 }) => {
    return new Encoder().writeInt32(partition).writeInt64(timestamp).writeInt32(maxNumOffsets);
  };
});

// node_modules/kafkajs/src/protocol/requests/listOffsets/v0/response.js
var require_response21 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode } = require_error();
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    return {
      responses: decoder.readArray(decodeResponses)
    };
  };
  var decodeResponses = (decoder) => ({
    topic: decoder.readString(),
    partitions: decoder.readArray(decodePartitions)
  });
  var decodePartitions = (decoder) => ({
    partition: decoder.readInt32(),
    errorCode: decoder.readInt16(),
    offsets: decoder.readArray(decodeOffsets)
  });
  var decodeOffsets = (decoder) => decoder.readInt64().toString();
  var parse = async (data) => {
    const partitionsWithError = data.responses.flatMap((response) => response.partitions.filter((partition) => failure(partition.errorCode)));
    const partitionWithError = partitionsWithError[0];
    if (partitionWithError) {
      throw createErrorFromCode(partitionWithError.errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/listOffsets/v1/request.js
var require_request22 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { ListOffsets: apiKey } = require_apiKeys();
  module.exports = ({ replicaId, topics }) => ({
    apiKey,
    apiVersion: 1,
    apiName: "ListOffsets",
    encode: async () => {
      return new Encoder().writeInt32(replicaId).writeArray(topics.map(encodeTopic));
    }
  });
  var encodeTopic = ({ topic, partitions }) => {
    return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition));
  };
  var encodePartition = ({ partition, timestamp = -1 }) => {
    return new Encoder().writeInt32(partition).writeInt64(timestamp);
  };
});

// node_modules/kafkajs/src/protocol/requests/listOffsets/v1/response.js
var require_response22 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode } = require_error();
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    return {
      responses: decoder.readArray(decodeResponses)
    };
  };
  var decodeResponses = (decoder) => ({
    topic: decoder.readString(),
    partitions: decoder.readArray(decodePartitions)
  });
  var decodePartitions = (decoder) => ({
    partition: decoder.readInt32(),
    errorCode: decoder.readInt16(),
    timestamp: decoder.readInt64().toString(),
    offset: decoder.readInt64().toString()
  });
  var parse = async (data) => {
    const partitionsWithError = data.responses.flatMap((response) => response.partitions.filter((partition) => failure(partition.errorCode)));
    const partitionWithError = partitionsWithError[0];
    if (partitionWithError) {
      throw createErrorFromCode(partitionWithError.errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/listOffsets/v2/request.js
var require_request23 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { ListOffsets: apiKey } = require_apiKeys();
  module.exports = ({ replicaId, isolationLevel, topics }) => ({
    apiKey,
    apiVersion: 2,
    apiName: "ListOffsets",
    encode: async () => {
      return new Encoder().writeInt32(replicaId).writeInt8(isolationLevel).writeArray(topics.map(encodeTopic));
    }
  });
  var encodeTopic = ({ topic, partitions }) => {
    return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition));
  };
  var encodePartition = ({ partition, timestamp = -1 }) => {
    return new Encoder().writeInt32(partition).writeInt64(timestamp);
  };
});

// node_modules/kafkajs/src/protocol/requests/listOffsets/v2/response.js
var require_response23 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode } = require_error();
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    return {
      throttleTime: decoder.readInt32(),
      responses: decoder.readArray(decodeResponses)
    };
  };
  var decodeResponses = (decoder) => ({
    topic: decoder.readString(),
    partitions: decoder.readArray(decodePartitions)
  });
  var decodePartitions = (decoder) => ({
    partition: decoder.readInt32(),
    errorCode: decoder.readInt16(),
    timestamp: decoder.readInt64().toString(),
    offset: decoder.readInt64().toString()
  });
  var parse = async (data) => {
    const partitionsWithError = data.responses.flatMap((response) => response.partitions.filter((partition) => failure(partition.errorCode)));
    const partitionWithError = partitionsWithError[0];
    if (partitionWithError) {
      throw createErrorFromCode(partitionWithError.errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/listOffsets/v3/request.js
var require_request24 = __commonJS((exports, module) => {
  var requestV2 = require_request23();
  module.exports = ({ replicaId, isolationLevel, topics }) => Object.assign(requestV2({ replicaId, isolationLevel, topics }), { apiVersion: 3 });
});

// node_modules/kafkajs/src/protocol/requests/listOffsets/v3/response.js
var require_response24 = __commonJS((exports, module) => {
  var { parse, decode: decodeV2 } = require_response23();
  var decode = async (rawData) => {
    const decoded = await decodeV2(rawData);
    return {
      ...decoded,
      throttleTime: 0,
      clientSideThrottleTime: decoded.throttleTime
    };
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/listOffsets/index.js
var require_listOffsets = __commonJS((exports, module) => {
  var ISOLATION_LEVEL = require_isolationLevel();
  var REPLICA_ID = -1;
  var versions = {
    0: ({ replicaId = REPLICA_ID, topics }) => {
      const request3 = require_request21();
      const response = require_response21();
      return { request: request3({ replicaId, topics }), response };
    },
    1: ({ replicaId = REPLICA_ID, topics }) => {
      const request3 = require_request22();
      const response = require_response22();
      return { request: request3({ replicaId, topics }), response };
    },
    2: ({ replicaId = REPLICA_ID, isolationLevel = ISOLATION_LEVEL.READ_COMMITTED, topics }) => {
      const request3 = require_request23();
      const response = require_response23();
      return { request: request3({ replicaId, isolationLevel, topics }), response };
    },
    3: ({ replicaId = REPLICA_ID, isolationLevel = ISOLATION_LEVEL.READ_COMMITTED, topics }) => {
      const request3 = require_request24();
      const response = require_response24();
      return { request: request3({ replicaId, isolationLevel, topics }), response };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/metadata/v0/request.js
var require_request25 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { Metadata: apiKey } = require_apiKeys();
  module.exports = ({ topics }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "Metadata",
    encode: async () => {
      return new Encoder().writeArray(topics);
    }
  });
});

// node_modules/kafkajs/src/protocol/requests/metadata/v0/response.js
var require_response25 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode } = require_error();
  var broker = (decoder) => ({
    nodeId: decoder.readInt32(),
    host: decoder.readString(),
    port: decoder.readInt32()
  });
  var topicMetadata = (decoder) => ({
    topicErrorCode: decoder.readInt16(),
    topic: decoder.readString(),
    partitionMetadata: decoder.readArray(partitionMetadata)
  });
  var partitionMetadata = (decoder) => ({
    partitionErrorCode: decoder.readInt16(),
    partitionId: decoder.readInt32(),
    leader: decoder.readInt32(),
    replicas: decoder.readArray((d) => d.readInt32()),
    isr: decoder.readArray((d) => d.readInt32())
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    return {
      brokers: decoder.readArray(broker),
      topicMetadata: decoder.readArray(topicMetadata)
    };
  };
  var parse = async (data) => {
    const topicsWithErrors = data.topicMetadata.filter((topic) => failure(topic.topicErrorCode));
    if (topicsWithErrors.length > 0) {
      const { topicErrorCode } = topicsWithErrors[0];
      throw createErrorFromCode(topicErrorCode);
    }
    const errors = data.topicMetadata.flatMap((topic) => {
      return topic.partitionMetadata.filter((partition) => failure(partition.partitionErrorCode));
    });
    if (errors.length > 0) {
      const { partitionErrorCode } = errors[0];
      throw createErrorFromCode(partitionErrorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/metadata/v1/request.js
var require_request26 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { Metadata: apiKey } = require_apiKeys();
  module.exports = ({ topics }) => ({
    apiKey,
    apiVersion: 1,
    apiName: "Metadata",
    encode: async () => {
      return new Encoder().writeNullableArray(topics);
    }
  });
});

// node_modules/kafkajs/src/protocol/requests/metadata/v1/response.js
var require_response26 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { parse: parseV0 } = require_response25();
  var broker = (decoder) => ({
    nodeId: decoder.readInt32(),
    host: decoder.readString(),
    port: decoder.readInt32(),
    rack: decoder.readString()
  });
  var topicMetadata = (decoder) => ({
    topicErrorCode: decoder.readInt16(),
    topic: decoder.readString(),
    isInternal: decoder.readBoolean(),
    partitionMetadata: decoder.readArray(partitionMetadata)
  });
  var partitionMetadata = (decoder) => ({
    partitionErrorCode: decoder.readInt16(),
    partitionId: decoder.readInt32(),
    leader: decoder.readInt32(),
    replicas: decoder.readArray((d) => d.readInt32()),
    isr: decoder.readArray((d) => d.readInt32())
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    return {
      brokers: decoder.readArray(broker),
      controllerId: decoder.readInt32(),
      topicMetadata: decoder.readArray(topicMetadata)
    };
  };
  module.exports = {
    decode,
    parse: parseV0
  };
});

// node_modules/kafkajs/src/protocol/requests/metadata/v2/request.js
var require_request27 = __commonJS((exports, module) => {
  var requestV1 = require_request26();
  module.exports = ({ topics }) => Object.assign(requestV1({ topics }), { apiVersion: 2 });
});

// node_modules/kafkajs/src/protocol/requests/metadata/v2/response.js
var require_response27 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { parse: parseV0 } = require_response25();
  var broker = (decoder) => ({
    nodeId: decoder.readInt32(),
    host: decoder.readString(),
    port: decoder.readInt32(),
    rack: decoder.readString()
  });
  var topicMetadata = (decoder) => ({
    topicErrorCode: decoder.readInt16(),
    topic: decoder.readString(),
    isInternal: decoder.readBoolean(),
    partitionMetadata: decoder.readArray(partitionMetadata)
  });
  var partitionMetadata = (decoder) => ({
    partitionErrorCode: decoder.readInt16(),
    partitionId: decoder.readInt32(),
    leader: decoder.readInt32(),
    replicas: decoder.readArray((d) => d.readInt32()),
    isr: decoder.readArray((d) => d.readInt32())
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    return {
      brokers: decoder.readArray(broker),
      clusterId: decoder.readString(),
      controllerId: decoder.readInt32(),
      topicMetadata: decoder.readArray(topicMetadata)
    };
  };
  module.exports = {
    decode,
    parse: parseV0
  };
});

// node_modules/kafkajs/src/protocol/requests/metadata/v3/request.js
var require_request28 = __commonJS((exports, module) => {
  var requestV1 = require_request26();
  module.exports = ({ topics }) => Object.assign(requestV1({ topics }), { apiVersion: 3 });
});

// node_modules/kafkajs/src/protocol/requests/metadata/v3/response.js
var require_response28 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { parse: parseV0 } = require_response25();
  var broker = (decoder) => ({
    nodeId: decoder.readInt32(),
    host: decoder.readString(),
    port: decoder.readInt32(),
    rack: decoder.readString()
  });
  var topicMetadata = (decoder) => ({
    topicErrorCode: decoder.readInt16(),
    topic: decoder.readString(),
    isInternal: decoder.readBoolean(),
    partitionMetadata: decoder.readArray(partitionMetadata)
  });
  var partitionMetadata = (decoder) => ({
    partitionErrorCode: decoder.readInt16(),
    partitionId: decoder.readInt32(),
    leader: decoder.readInt32(),
    replicas: decoder.readArray((d) => d.readInt32()),
    isr: decoder.readArray((d) => d.readInt32())
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    return {
      throttleTime: decoder.readInt32(),
      brokers: decoder.readArray(broker),
      clusterId: decoder.readString(),
      controllerId: decoder.readInt32(),
      topicMetadata: decoder.readArray(topicMetadata)
    };
  };
  module.exports = {
    decode,
    parse: parseV0
  };
});

// node_modules/kafkajs/src/protocol/requests/metadata/v4/request.js
var require_request29 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { Metadata: apiKey } = require_apiKeys();
  module.exports = ({ topics, allowAutoTopicCreation = true }) => ({
    apiKey,
    apiVersion: 4,
    apiName: "Metadata",
    encode: async () => {
      return new Encoder().writeNullableArray(topics).writeBoolean(allowAutoTopicCreation);
    }
  });
});

// node_modules/kafkajs/src/protocol/requests/metadata/v4/response.js
var require_response29 = __commonJS((exports, module) => {
  var { parse: parseV3, decode: decodeV3 } = require_response28();
  module.exports = {
    parse: parseV3,
    decode: decodeV3
  };
});

// node_modules/kafkajs/src/protocol/requests/metadata/v5/request.js
var require_request30 = __commonJS((exports, module) => {
  var requestV4 = require_request29();
  module.exports = ({ topics, allowAutoTopicCreation = true }) => Object.assign(requestV4({ topics, allowAutoTopicCreation }), { apiVersion: 5 });
});

// node_modules/kafkajs/src/protocol/requests/metadata/v5/response.js
var require_response30 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { parse: parseV0 } = require_response25();
  var broker = (decoder) => ({
    nodeId: decoder.readInt32(),
    host: decoder.readString(),
    port: decoder.readInt32(),
    rack: decoder.readString()
  });
  var topicMetadata = (decoder) => ({
    topicErrorCode: decoder.readInt16(),
    topic: decoder.readString(),
    isInternal: decoder.readBoolean(),
    partitionMetadata: decoder.readArray(partitionMetadata)
  });
  var partitionMetadata = (decoder) => ({
    partitionErrorCode: decoder.readInt16(),
    partitionId: decoder.readInt32(),
    leader: decoder.readInt32(),
    replicas: decoder.readArray((d) => d.readInt32()),
    isr: decoder.readArray((d) => d.readInt32()),
    offlineReplicas: decoder.readArray((d) => d.readInt32())
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    return {
      throttleTime: decoder.readInt32(),
      brokers: decoder.readArray(broker),
      clusterId: decoder.readString(),
      controllerId: decoder.readInt32(),
      topicMetadata: decoder.readArray(topicMetadata)
    };
  };
  module.exports = {
    decode,
    parse: parseV0
  };
});

// node_modules/kafkajs/src/protocol/requests/metadata/v6/request.js
var require_request31 = __commonJS((exports, module) => {
  var requestV5 = require_request30();
  module.exports = ({ topics, allowAutoTopicCreation = true }) => Object.assign(requestV5({ topics, allowAutoTopicCreation }), { apiVersion: 6 });
});

// node_modules/kafkajs/src/protocol/requests/metadata/v6/response.js
var require_response31 = __commonJS((exports, module) => {
  var { parse, decode: decodeV1 } = require_response30();
  var decode = async (rawData) => {
    const decoded = await decodeV1(rawData);
    return {
      ...decoded,
      throttleTime: 0,
      clientSideThrottleTime: decoded.throttleTime
    };
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/metadata/index.js
var require_metadata = __commonJS((exports, module) => {
  var versions = {
    0: ({ topics }) => {
      const request3 = require_request25();
      const response = require_response25();
      return { request: request3({ topics }), response };
    },
    1: ({ topics }) => {
      const request3 = require_request26();
      const response = require_response26();
      return { request: request3({ topics }), response };
    },
    2: ({ topics }) => {
      const request3 = require_request27();
      const response = require_response27();
      return { request: request3({ topics }), response };
    },
    3: ({ topics }) => {
      const request3 = require_request28();
      const response = require_response28();
      return { request: request3({ topics }), response };
    },
    4: ({ topics, allowAutoTopicCreation }) => {
      const request3 = require_request29();
      const response = require_response29();
      return { request: request3({ topics, allowAutoTopicCreation }), response };
    },
    5: ({ topics, allowAutoTopicCreation }) => {
      const request3 = require_request30();
      const response = require_response30();
      return { request: request3({ topics, allowAutoTopicCreation }), response };
    },
    6: ({ topics, allowAutoTopicCreation }) => {
      const request3 = require_request31();
      const response = require_response31();
      return { request: request3({ topics, allowAutoTopicCreation }), response };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/offsetCommit/v0/request.js
var require_request32 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { OffsetCommit: apiKey } = require_apiKeys();
  module.exports = ({ groupId, topics }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "OffsetCommit",
    encode: async () => {
      return new Encoder().writeString(groupId).writeArray(topics.map(encodeTopic));
    }
  });
  var encodeTopic = ({ topic, partitions }) => {
    return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition));
  };
  var encodePartition = ({ partition, offset, metadata = null }) => {
    return new Encoder().writeInt32(partition).writeInt64(offset).writeString(metadata);
  };
});

// node_modules/kafkajs/src/protocol/requests/offsetCommit/v0/response.js
var require_response32 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode } = require_error();
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    return {
      responses: decoder.readArray(decodeResponses)
    };
  };
  var decodeResponses = (decoder) => ({
    topic: decoder.readString(),
    partitions: decoder.readArray(decodePartitions)
  });
  var decodePartitions = (decoder) => ({
    partition: decoder.readInt32(),
    errorCode: decoder.readInt16()
  });
  var parse = async (data) => {
    const partitionsWithError = data.responses.flatMap((response) => response.partitions.filter((partition) => failure(partition.errorCode)));
    const partitionWithError = partitionsWithError[0];
    if (partitionWithError) {
      throw createErrorFromCode(partitionWithError.errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/offsetCommit/v1/request.js
var require_request33 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { OffsetCommit: apiKey } = require_apiKeys();
  module.exports = ({ groupId, groupGenerationId, memberId, topics }) => ({
    apiKey,
    apiVersion: 1,
    apiName: "OffsetCommit",
    encode: async () => {
      return new Encoder().writeString(groupId).writeInt32(groupGenerationId).writeString(memberId).writeArray(topics.map(encodeTopic));
    }
  });
  var encodeTopic = ({ topic, partitions }) => {
    return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition));
  };
  var encodePartition = ({ partition, offset, timestamp = Date.now(), metadata = null }) => {
    return new Encoder().writeInt32(partition).writeInt64(offset).writeInt64(timestamp).writeString(metadata);
  };
});

// node_modules/kafkajs/src/protocol/requests/offsetCommit/v1/response.js
var require_response33 = __commonJS((exports, module) => {
  var { parse, decode } = require_response32();
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/offsetCommit/v2/request.js
var require_request34 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { OffsetCommit: apiKey } = require_apiKeys();
  module.exports = ({ groupId, groupGenerationId, memberId, retentionTime, topics }) => ({
    apiKey,
    apiVersion: 2,
    apiName: "OffsetCommit",
    encode: async () => {
      return new Encoder().writeString(groupId).writeInt32(groupGenerationId).writeString(memberId).writeInt64(retentionTime).writeArray(topics.map(encodeTopic));
    }
  });
  var encodeTopic = ({ topic, partitions }) => {
    return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition));
  };
  var encodePartition = ({ partition, offset, metadata = null }) => {
    return new Encoder().writeInt32(partition).writeInt64(offset).writeString(metadata);
  };
});

// node_modules/kafkajs/src/protocol/requests/offsetCommit/v2/response.js
var require_response34 = __commonJS((exports, module) => {
  var { parse, decode } = require_response32();
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/offsetCommit/v3/request.js
var require_request35 = __commonJS((exports, module) => {
  var requestV2 = require_request34();
  module.exports = ({ groupId, groupGenerationId, memberId, retentionTime, topics }) => Object.assign(requestV2({ groupId, groupGenerationId, memberId, retentionTime, topics }), {
    apiVersion: 3
  });
});

// node_modules/kafkajs/src/protocol/requests/offsetCommit/v3/response.js
var require_response35 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { parse: parseV0 } = require_response32();
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    return {
      throttleTime: decoder.readInt32(),
      responses: decoder.readArray(decodeResponses)
    };
  };
  var decodeResponses = (decoder) => ({
    topic: decoder.readString(),
    partitions: decoder.readArray(decodePartitions)
  });
  var decodePartitions = (decoder) => ({
    partition: decoder.readInt32(),
    errorCode: decoder.readInt16()
  });
  module.exports = {
    decode,
    parse: parseV0
  };
});

// node_modules/kafkajs/src/protocol/requests/offsetCommit/v4/request.js
var require_request36 = __commonJS((exports, module) => {
  var requestV3 = require_request35();
  module.exports = ({ groupId, groupGenerationId, memberId, retentionTime, topics }) => Object.assign(requestV3({ groupId, groupGenerationId, memberId, retentionTime, topics }), {
    apiVersion: 4
  });
});

// node_modules/kafkajs/src/protocol/requests/offsetCommit/v4/response.js
var require_response36 = __commonJS((exports, module) => {
  var { parse, decode: decodeV3 } = require_response35();
  var decode = async (rawData) => {
    const decoded = await decodeV3(rawData);
    return {
      ...decoded,
      throttleTime: 0,
      clientSideThrottleTime: decoded.throttleTime
    };
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/offsetCommit/v5/request.js
var require_request37 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { OffsetCommit: apiKey } = require_apiKeys();
  module.exports = ({ groupId, groupGenerationId, memberId, topics }) => ({
    apiKey,
    apiVersion: 5,
    apiName: "OffsetCommit",
    encode: async () => {
      return new Encoder().writeString(groupId).writeInt32(groupGenerationId).writeString(memberId).writeArray(topics.map(encodeTopic));
    }
  });
  var encodeTopic = ({ topic, partitions }) => {
    return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition));
  };
  var encodePartition = ({ partition, offset, metadata = null }) => {
    return new Encoder().writeInt32(partition).writeInt64(offset).writeString(metadata);
  };
});

// node_modules/kafkajs/src/protocol/requests/offsetCommit/v5/response.js
var require_response37 = __commonJS((exports, module) => {
  var { parse, decode } = require_response36();
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/offsetCommit/index.js
var require_offsetCommit = __commonJS((exports, module) => {
  var RETENTION_TIME = -1;
  var versions = {
    0: ({ groupId, topics }) => {
      const request3 = require_request32();
      const response = require_response32();
      return { request: request3({ groupId, topics }), response };
    },
    1: ({ groupId, groupGenerationId, memberId, topics }) => {
      const request3 = require_request33();
      const response = require_response33();
      return { request: request3({ groupId, groupGenerationId, memberId, topics }), response };
    },
    2: ({ groupId, groupGenerationId, memberId, retentionTime = RETENTION_TIME, topics }) => {
      const request3 = require_request34();
      const response = require_response34();
      return {
        request: request3({
          groupId,
          groupGenerationId,
          memberId,
          retentionTime,
          topics
        }),
        response
      };
    },
    3: ({ groupId, groupGenerationId, memberId, retentionTime = RETENTION_TIME, topics }) => {
      const request3 = require_request35();
      const response = require_response35();
      return {
        request: request3({
          groupId,
          groupGenerationId,
          memberId,
          retentionTime,
          topics
        }),
        response
      };
    },
    4: ({ groupId, groupGenerationId, memberId, retentionTime = RETENTION_TIME, topics }) => {
      const request3 = require_request36();
      const response = require_response36();
      return {
        request: request3({
          groupId,
          groupGenerationId,
          memberId,
          retentionTime,
          topics
        }),
        response
      };
    },
    5: ({ groupId, groupGenerationId, memberId, topics }) => {
      const request3 = require_request37();
      const response = require_response37();
      return {
        request: request3({
          groupId,
          groupGenerationId,
          memberId,
          topics
        }),
        response
      };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/offsetFetch/v1/request.js
var require_request38 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { OffsetFetch: apiKey } = require_apiKeys();
  module.exports = ({ groupId, topics }) => ({
    apiKey,
    apiVersion: 1,
    apiName: "OffsetFetch",
    encode: async () => {
      return new Encoder().writeString(groupId).writeArray(topics.map(encodeTopic));
    }
  });
  var encodeTopic = ({ topic, partitions }) => {
    return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition));
  };
  var encodePartition = ({ partition }) => {
    return new Encoder().writeInt32(partition);
  };
});

// node_modules/kafkajs/src/protocol/requests/offsetFetch/v1/response.js
var require_response38 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode } = require_error();
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    return {
      responses: decoder.readArray(decodeResponses)
    };
  };
  var decodeResponses = (decoder) => ({
    topic: decoder.readString(),
    partitions: decoder.readArray(decodePartitions)
  });
  var decodePartitions = (decoder) => ({
    partition: decoder.readInt32(),
    offset: decoder.readInt64().toString(),
    metadata: decoder.readString(),
    errorCode: decoder.readInt16()
  });
  var parse = async (data) => {
    const partitionsWithError = data.responses.flatMap((response) => response.partitions.filter((partition) => failure(partition.errorCode)));
    const partitionWithError = partitionsWithError[0];
    if (partitionWithError) {
      throw createErrorFromCode(partitionWithError.errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/offsetFetch/v2/request.js
var require_request39 = __commonJS((exports, module) => {
  var requestV1 = require_request38();
  module.exports = ({ groupId, topics }) => Object.assign(requestV1({ groupId, topics }), { apiVersion: 2 });
});

// node_modules/kafkajs/src/protocol/requests/offsetFetch/v2/response.js
var require_response39 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode } = require_error();
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    return {
      responses: decoder.readArray(decodeResponses),
      errorCode: decoder.readInt16()
    };
  };
  var decodeResponses = (decoder) => ({
    topic: decoder.readString(),
    partitions: decoder.readArray(decodePartitions)
  });
  var decodePartitions = (decoder) => ({
    partition: decoder.readInt32(),
    offset: decoder.readInt64().toString(),
    metadata: decoder.readString(),
    errorCode: decoder.readInt16()
  });
  var parse = async (data) => {
    if (failure(data.errorCode)) {
      throw createErrorFromCode(data.errorCode);
    }
    const partitionsWithError = data.responses.flatMap((response) => response.partitions.filter((partition) => failure(partition.errorCode)));
    const partitionWithError = partitionsWithError[0];
    if (partitionWithError) {
      throw createErrorFromCode(partitionWithError.errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/offsetFetch/v3/request.js
var require_request40 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { OffsetFetch: apiKey } = require_apiKeys();
  module.exports = ({ groupId, topics }) => ({
    apiKey,
    apiVersion: 3,
    apiName: "OffsetFetch",
    encode: async () => {
      return new Encoder().writeString(groupId).writeNullableArray(topics.map(encodeTopic));
    }
  });
  var encodeTopic = ({ topic, partitions }) => {
    return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition));
  };
  var encodePartition = ({ partition }) => {
    return new Encoder().writeInt32(partition);
  };
});

// node_modules/kafkajs/src/protocol/requests/offsetFetch/v3/response.js
var require_response40 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { parse: parseV2 } = require_response39();
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    return {
      throttleTime: decoder.readInt32(),
      responses: decoder.readArray(decodeResponses),
      errorCode: decoder.readInt16()
    };
  };
  var decodeResponses = (decoder) => ({
    topic: decoder.readString(),
    partitions: decoder.readArray(decodePartitions)
  });
  var decodePartitions = (decoder) => ({
    partition: decoder.readInt32(),
    offset: decoder.readInt64().toString(),
    metadata: decoder.readString(),
    errorCode: decoder.readInt16()
  });
  module.exports = {
    decode,
    parse: parseV2
  };
});

// node_modules/kafkajs/src/protocol/requests/offsetFetch/v4/request.js
var require_request41 = __commonJS((exports, module) => {
  var requestV3 = require_request40();
  module.exports = ({ groupId, topics }) => Object.assign(requestV3({ groupId, topics }), { apiVersion: 4 });
});

// node_modules/kafkajs/src/protocol/requests/offsetFetch/v4/response.js
var require_response41 = __commonJS((exports, module) => {
  var { parse, decode: decodeV3 } = require_response40();
  var decode = async (rawData) => {
    const decoded = await decodeV3(rawData);
    return {
      ...decoded,
      throttleTime: 0,
      clientSideThrottleTime: decoded.throttleTime
    };
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/offsetFetch/index.js
var require_offsetFetch = __commonJS((exports, module) => {
  var versions = {
    1: ({ groupId, topics }) => {
      const request3 = require_request38();
      const response = require_response38();
      return { request: request3({ groupId, topics }), response };
    },
    2: ({ groupId, topics }) => {
      const request3 = require_request39();
      const response = require_response39();
      return { request: request3({ groupId, topics }), response };
    },
    3: ({ groupId, topics }) => {
      const request3 = require_request40();
      const response = require_response40();
      return { request: request3({ groupId, topics }), response };
    },
    4: ({ groupId, topics }) => {
      const request3 = require_request41();
      const response = require_response41();
      return { request: request3({ groupId, topics }), response };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/coordinatorTypes.js
var require_coordinatorTypes = __commonJS((exports, module) => {
  module.exports = {
    GROUP: 0,
    TRANSACTION: 1
  };
});

// node_modules/kafkajs/src/protocol/requests/findCoordinator/v0/request.js
var require_request42 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { GroupCoordinator: apiKey } = require_apiKeys();
  module.exports = ({ groupId }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "GroupCoordinator",
    encode: async () => {
      return new Encoder().writeString(groupId);
    }
  });
});

// node_modules/kafkajs/src/protocol/requests/findCoordinator/v0/response.js
var require_response42 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode, failIfVersionNotSupported } = require_error();
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const errorCode = decoder.readInt16();
    failIfVersionNotSupported(errorCode);
    const coordinator = {
      nodeId: decoder.readInt32(),
      host: decoder.readString(),
      port: decoder.readInt32()
    };
    return {
      errorCode,
      coordinator
    };
  };
  var parse = async (data) => {
    if (failure(data.errorCode)) {
      throw createErrorFromCode(data.errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/findCoordinator/v1/request.js
var require_request43 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { GroupCoordinator: apiKey } = require_apiKeys();
  module.exports = ({ coordinatorKey, coordinatorType }) => ({
    apiKey,
    apiVersion: 1,
    apiName: "GroupCoordinator",
    encode: async () => {
      return new Encoder().writeString(coordinatorKey).writeInt8(coordinatorType);
    }
  });
});

// node_modules/kafkajs/src/protocol/requests/findCoordinator/v1/response.js
var require_response43 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode, failIfVersionNotSupported } = require_error();
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    const errorCode = decoder.readInt16();
    failIfVersionNotSupported(errorCode);
    const errorMessage = decoder.readString();
    const coordinator = {
      nodeId: decoder.readInt32(),
      host: decoder.readString(),
      port: decoder.readInt32()
    };
    return {
      throttleTime,
      errorCode,
      errorMessage,
      coordinator
    };
  };
  var parse = async (data) => {
    if (failure(data.errorCode)) {
      throw createErrorFromCode(data.errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/findCoordinator/v2/request.js
var require_request44 = __commonJS((exports, module) => {
  var requestV1 = require_request43();
  module.exports = ({ coordinatorKey, coordinatorType }) => Object.assign(requestV1({ coordinatorKey, coordinatorType }), { apiVersion: 2 });
});

// node_modules/kafkajs/src/protocol/requests/findCoordinator/v2/response.js
var require_response44 = __commonJS((exports, module) => {
  var { parse, decode: decodeV1 } = require_response43();
  var decode = async (rawData) => {
    const decoded = await decodeV1(rawData);
    return {
      ...decoded,
      throttleTime: 0,
      clientSideThrottleTime: decoded.throttleTime
    };
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/findCoordinator/index.js
var require_findCoordinator = __commonJS((exports, module) => {
  var COORDINATOR_TYPES = require_coordinatorTypes();
  var versions = {
    0: ({ groupId }) => {
      const request3 = require_request42();
      const response = require_response42();
      return { request: request3({ groupId }), response };
    },
    1: ({ groupId, coordinatorType = COORDINATOR_TYPES.GROUP }) => {
      const request3 = require_request43();
      const response = require_response43();
      return { request: request3({ coordinatorKey: groupId, coordinatorType }), response };
    },
    2: ({ groupId, coordinatorType = COORDINATOR_TYPES.GROUP }) => {
      const request3 = require_request44();
      const response = require_response44();
      return { request: request3({ coordinatorKey: groupId, coordinatorType }), response };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/joinGroup/v0/request.js
var require_request45 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { JoinGroup: apiKey } = require_apiKeys();
  module.exports = ({ groupId, sessionTimeout, memberId, protocolType, groupProtocols }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "JoinGroup",
    encode: async () => {
      return new Encoder().writeString(groupId).writeInt32(sessionTimeout).writeString(memberId).writeString(protocolType).writeArray(groupProtocols.map(encodeGroupProtocols));
    }
  });
  var encodeGroupProtocols = ({ name, metadata = Buffer.alloc(0) }) => {
    return new Encoder().writeString(name).writeBytes(metadata);
  };
});

// node_modules/kafkajs/src/protocol/requests/joinGroup/v0/response.js
var require_response45 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode, failIfVersionNotSupported } = require_error();
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const errorCode = decoder.readInt16();
    failIfVersionNotSupported(errorCode);
    return {
      errorCode,
      generationId: decoder.readInt32(),
      groupProtocol: decoder.readString(),
      leaderId: decoder.readString(),
      memberId: decoder.readString(),
      members: decoder.readArray((decoder2) => ({
        memberId: decoder2.readString(),
        memberMetadata: decoder2.readBytes()
      }))
    };
  };
  var parse = async (data) => {
    if (failure(data.errorCode)) {
      throw createErrorFromCode(data.errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/joinGroup/v1/request.js
var require_request46 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { JoinGroup: apiKey } = require_apiKeys();
  module.exports = ({
    groupId,
    sessionTimeout,
    rebalanceTimeout,
    memberId,
    protocolType,
    groupProtocols
  }) => ({
    apiKey,
    apiVersion: 1,
    apiName: "JoinGroup",
    encode: async () => {
      return new Encoder().writeString(groupId).writeInt32(sessionTimeout).writeInt32(rebalanceTimeout).writeString(memberId).writeString(protocolType).writeArray(groupProtocols.map(encodeGroupProtocols));
    }
  });
  var encodeGroupProtocols = ({ name, metadata = Buffer.alloc(0) }) => {
    return new Encoder().writeString(name).writeBytes(metadata);
  };
});

// node_modules/kafkajs/src/protocol/requests/joinGroup/v1/response.js
var require_response46 = __commonJS((exports, module) => {
  var { parse, decode } = require_response45();
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/joinGroup/v2/request.js
var require_request47 = __commonJS((exports, module) => {
  var requestV1 = require_request46();
  module.exports = ({
    groupId,
    sessionTimeout,
    rebalanceTimeout,
    memberId,
    protocolType,
    groupProtocols
  }) => Object.assign(requestV1({
    groupId,
    sessionTimeout,
    rebalanceTimeout,
    memberId,
    protocolType,
    groupProtocols
  }), { apiVersion: 2 });
});

// node_modules/kafkajs/src/protocol/requests/joinGroup/v2/response.js
var require_response47 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failIfVersionNotSupported } = require_error();
  var { parse: parseV0 } = require_response45();
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    const errorCode = decoder.readInt16();
    failIfVersionNotSupported(errorCode);
    return {
      throttleTime,
      errorCode,
      generationId: decoder.readInt32(),
      groupProtocol: decoder.readString(),
      leaderId: decoder.readString(),
      memberId: decoder.readString(),
      members: decoder.readArray((decoder2) => ({
        memberId: decoder2.readString(),
        memberMetadata: decoder2.readBytes()
      }))
    };
  };
  module.exports = {
    decode,
    parse: parseV0
  };
});

// node_modules/kafkajs/src/protocol/requests/joinGroup/v3/request.js
var require_request48 = __commonJS((exports, module) => {
  var requestV2 = require_request47();
  module.exports = ({
    groupId,
    sessionTimeout,
    rebalanceTimeout,
    memberId,
    protocolType,
    groupProtocols
  }) => Object.assign(requestV2({
    groupId,
    sessionTimeout,
    rebalanceTimeout,
    memberId,
    protocolType,
    groupProtocols
  }), { apiVersion: 3 });
});

// node_modules/kafkajs/src/protocol/requests/joinGroup/v3/response.js
var require_response48 = __commonJS((exports, module) => {
  var { parse, decode: decodeV2 } = require_response47();
  var decode = async (rawData) => {
    const decoded = await decodeV2(rawData);
    return {
      ...decoded,
      throttleTime: 0,
      clientSideThrottleTime: decoded.throttleTime
    };
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/joinGroup/v4/request.js
var require_request49 = __commonJS((exports, module) => {
  var requestV3 = require_request48();
  module.exports = ({
    groupId,
    sessionTimeout,
    rebalanceTimeout,
    memberId,
    protocolType,
    groupProtocols
  }) => Object.assign(requestV3({
    groupId,
    sessionTimeout,
    rebalanceTimeout,
    memberId,
    protocolType,
    groupProtocols
  }), { apiVersion: 4 });
});

// node_modules/kafkajs/src/protocol/requests/joinGroup/v4/response.js
var require_response49 = __commonJS((exports, module) => {
  var { decode } = require_response48();
  var { KafkaJSMemberIdRequired } = require_errors();
  var { failure, createErrorFromCode, errorCodes } = require_error();
  var { code: MEMBER_ID_REQUIRED_ERROR_CODE } = errorCodes.find((e) => e.type === "MEMBER_ID_REQUIRED");
  var parse = async (data) => {
    if (failure(data.errorCode)) {
      if (data.errorCode === MEMBER_ID_REQUIRED_ERROR_CODE) {
        throw new KafkaJSMemberIdRequired(createErrorFromCode(data.errorCode), {
          memberId: data.memberId
        });
      }
      throw createErrorFromCode(data.errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/joinGroup/v5/request.js
var require_request50 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { JoinGroup: apiKey } = require_apiKeys();
  module.exports = ({
    groupId,
    sessionTimeout,
    rebalanceTimeout,
    memberId,
    groupInstanceId = null,
    protocolType,
    groupProtocols
  }) => ({
    apiKey,
    apiVersion: 5,
    apiName: "JoinGroup",
    encode: async () => {
      return new Encoder().writeString(groupId).writeInt32(sessionTimeout).writeInt32(rebalanceTimeout).writeString(memberId).writeString(groupInstanceId).writeString(protocolType).writeArray(groupProtocols.map(encodeGroupProtocols));
    }
  });
  var encodeGroupProtocols = ({ name, metadata = Buffer.alloc(0) }) => {
    return new Encoder().writeString(name).writeBytes(metadata);
  };
});

// node_modules/kafkajs/src/protocol/requests/joinGroup/v5/response.js
var require_response50 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { KafkaJSMemberIdRequired } = require_errors();
  var {
    failure,
    createErrorFromCode,
    errorCodes,
    failIfVersionNotSupported
  } = require_error();
  var { code: MEMBER_ID_REQUIRED_ERROR_CODE } = errorCodes.find((e) => e.type === "MEMBER_ID_REQUIRED");
  var parse = async (data) => {
    if (failure(data.errorCode)) {
      if (data.errorCode === MEMBER_ID_REQUIRED_ERROR_CODE) {
        throw new KafkaJSMemberIdRequired(createErrorFromCode(data.errorCode), {
          memberId: data.memberId
        });
      }
      throw createErrorFromCode(data.errorCode);
    }
    return data;
  };
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    const errorCode = decoder.readInt16();
    failIfVersionNotSupported(errorCode);
    return {
      throttleTime: 0,
      clientSideThrottleTime: throttleTime,
      errorCode,
      generationId: decoder.readInt32(),
      groupProtocol: decoder.readString(),
      leaderId: decoder.readString(),
      memberId: decoder.readString(),
      members: decoder.readArray((decoder2) => ({
        memberId: decoder2.readString(),
        groupInstanceId: decoder2.readString(),
        memberMetadata: decoder2.readBytes()
      }))
    };
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/joinGroup/index.js
var require_joinGroup = __commonJS((exports, module) => {
  var NETWORK_DELAY = 5000;
  var requestTimeout = ({ rebalanceTimeout, sessionTimeout }) => {
    const timeout = rebalanceTimeout || sessionTimeout;
    return Number.isSafeInteger(timeout + NETWORK_DELAY) ? timeout + NETWORK_DELAY : timeout;
  };
  var logResponseError = (memberId) => memberId != null && memberId !== "";
  var versions = {
    0: ({ groupId, sessionTimeout, memberId, protocolType, groupProtocols }) => {
      const request3 = require_request45();
      const response = require_response45();
      return {
        request: request3({
          groupId,
          sessionTimeout,
          memberId,
          protocolType,
          groupProtocols
        }),
        response,
        requestTimeout: requestTimeout({ rebalanceTimeout: null, sessionTimeout })
      };
    },
    1: ({ groupId, sessionTimeout, rebalanceTimeout, memberId, protocolType, groupProtocols }) => {
      const request3 = require_request46();
      const response = require_response46();
      return {
        request: request3({
          groupId,
          sessionTimeout,
          rebalanceTimeout,
          memberId,
          protocolType,
          groupProtocols
        }),
        response,
        requestTimeout: requestTimeout({ rebalanceTimeout, sessionTimeout })
      };
    },
    2: ({ groupId, sessionTimeout, rebalanceTimeout, memberId, protocolType, groupProtocols }) => {
      const request3 = require_request47();
      const response = require_response47();
      return {
        request: request3({
          groupId,
          sessionTimeout,
          rebalanceTimeout,
          memberId,
          protocolType,
          groupProtocols
        }),
        response,
        requestTimeout: requestTimeout({ rebalanceTimeout, sessionTimeout })
      };
    },
    3: ({ groupId, sessionTimeout, rebalanceTimeout, memberId, protocolType, groupProtocols }) => {
      const request3 = require_request48();
      const response = require_response48();
      return {
        request: request3({
          groupId,
          sessionTimeout,
          rebalanceTimeout,
          memberId,
          protocolType,
          groupProtocols
        }),
        response,
        requestTimeout: requestTimeout({ rebalanceTimeout, sessionTimeout })
      };
    },
    4: ({ groupId, sessionTimeout, rebalanceTimeout, memberId, protocolType, groupProtocols }) => {
      const request3 = require_request49();
      const response = require_response49();
      return {
        request: request3({
          groupId,
          sessionTimeout,
          rebalanceTimeout,
          memberId,
          protocolType,
          groupProtocols
        }),
        response,
        requestTimeout: requestTimeout({ rebalanceTimeout, sessionTimeout }),
        logResponseError: logResponseError(memberId)
      };
    },
    5: ({
      groupId,
      sessionTimeout,
      rebalanceTimeout,
      memberId,
      groupInstanceId,
      protocolType,
      groupProtocols
    }) => {
      const request3 = require_request50();
      const response = require_response50();
      return {
        request: request3({
          groupId,
          sessionTimeout,
          rebalanceTimeout,
          memberId,
          groupInstanceId,
          protocolType,
          groupProtocols
        }),
        response,
        requestTimeout: requestTimeout({ rebalanceTimeout, sessionTimeout }),
        logResponseError: logResponseError(memberId)
      };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/heartbeat/v0/request.js
var require_request51 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { Heartbeat: apiKey } = require_apiKeys();
  module.exports = ({ groupId, groupGenerationId, memberId }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "Heartbeat",
    encode: async () => {
      return new Encoder().writeString(groupId).writeInt32(groupGenerationId).writeString(memberId);
    }
  });
});

// node_modules/kafkajs/src/protocol/requests/heartbeat/v0/response.js
var require_response51 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode, failIfVersionNotSupported } = require_error();
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const errorCode = decoder.readInt16();
    failIfVersionNotSupported(errorCode);
    return { errorCode };
  };
  var parse = async (data) => {
    if (failure(data.errorCode)) {
      throw createErrorFromCode(data.errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/heartbeat/v1/request.js
var require_request52 = __commonJS((exports, module) => {
  var requestV0 = require_request51();
  module.exports = ({ groupId, groupGenerationId, memberId }) => Object.assign(requestV0({ groupId, groupGenerationId, memberId }), { apiVersion: 1 });
});

// node_modules/kafkajs/src/protocol/requests/heartbeat/v1/response.js
var require_response52 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failIfVersionNotSupported } = require_error();
  var { parse: parseV0 } = require_response51();
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    const errorCode = decoder.readInt16();
    failIfVersionNotSupported(errorCode);
    return { throttleTime, errorCode };
  };
  module.exports = {
    decode,
    parse: parseV0
  };
});

// node_modules/kafkajs/src/protocol/requests/heartbeat/v2/request.js
var require_request53 = __commonJS((exports, module) => {
  var requestV1 = require_request52();
  module.exports = ({ groupId, groupGenerationId, memberId }) => Object.assign(requestV1({ groupId, groupGenerationId, memberId }), { apiVersion: 2 });
});

// node_modules/kafkajs/src/protocol/requests/heartbeat/v2/response.js
var require_response53 = __commonJS((exports, module) => {
  var { parse, decode: decodeV1 } = require_response52();
  var decode = async (rawData) => {
    const decoded = await decodeV1(rawData);
    return {
      ...decoded,
      throttleTime: 0,
      clientSideThrottleTime: decoded.throttleTime
    };
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/heartbeat/v3/request.js
var require_request54 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { Heartbeat: apiKey } = require_apiKeys();
  module.exports = ({ groupId, groupGenerationId, memberId, groupInstanceId }) => ({
    apiKey,
    apiVersion: 3,
    apiName: "Heartbeat",
    encode: async () => {
      return new Encoder().writeString(groupId).writeInt32(groupGenerationId).writeString(memberId).writeString(groupInstanceId);
    }
  });
});

// node_modules/kafkajs/src/protocol/requests/heartbeat/v3/response.js
var require_response54 = __commonJS((exports, module) => {
  var { parse, decode } = require_response53();
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/heartbeat/index.js
var require_heartbeat = __commonJS((exports, module) => {
  var versions = {
    0: ({ groupId, groupGenerationId, memberId }) => {
      const request3 = require_request51();
      const response = require_response51();
      return {
        request: request3({ groupId, groupGenerationId, memberId }),
        response
      };
    },
    1: ({ groupId, groupGenerationId, memberId }) => {
      const request3 = require_request52();
      const response = require_response52();
      return {
        request: request3({ groupId, groupGenerationId, memberId }),
        response
      };
    },
    2: ({ groupId, groupGenerationId, memberId }) => {
      const request3 = require_request53();
      const response = require_response53();
      return {
        request: request3({ groupId, groupGenerationId, memberId }),
        response
      };
    },
    3: ({ groupId, groupGenerationId, memberId, groupInstanceId }) => {
      const request3 = require_request54();
      const response = require_response54();
      return {
        request: request3({ groupId, groupGenerationId, memberId, groupInstanceId }),
        response
      };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/leaveGroup/v0/request.js
var require_request55 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { LeaveGroup: apiKey } = require_apiKeys();
  module.exports = ({ groupId, memberId }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "LeaveGroup",
    encode: async () => {
      return new Encoder().writeString(groupId).writeString(memberId);
    }
  });
});

// node_modules/kafkajs/src/protocol/requests/leaveGroup/v0/response.js
var require_response55 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode, failIfVersionNotSupported } = require_error();
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const errorCode = decoder.readInt16();
    failIfVersionNotSupported(errorCode);
    return { errorCode };
  };
  var parse = async (data) => {
    if (failure(data.errorCode)) {
      throw createErrorFromCode(data.errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/leaveGroup/v1/request.js
var require_request56 = __commonJS((exports, module) => {
  var requestV0 = require_request55();
  module.exports = ({ groupId, memberId }) => Object.assign(requestV0({ groupId, memberId }), { apiVersion: 1 });
});

// node_modules/kafkajs/src/protocol/requests/leaveGroup/v1/response.js
var require_response56 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failIfVersionNotSupported } = require_error();
  var { parse: parseV0 } = require_response55();
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    const errorCode = decoder.readInt16();
    failIfVersionNotSupported(errorCode);
    return { throttleTime, errorCode };
  };
  module.exports = {
    decode,
    parse: parseV0
  };
});

// node_modules/kafkajs/src/protocol/requests/leaveGroup/v2/request.js
var require_request57 = __commonJS((exports, module) => {
  var requestV1 = require_request56();
  module.exports = ({ groupId, memberId }) => Object.assign(requestV1({ groupId, memberId }), { apiVersion: 2 });
});

// node_modules/kafkajs/src/protocol/requests/leaveGroup/v2/response.js
var require_response57 = __commonJS((exports, module) => {
  var { parse, decode: decodeV1 } = require_response56();
  var decode = async (rawData) => {
    const decoded = await decodeV1(rawData);
    return {
      ...decoded,
      throttleTime: 0,
      clientSideThrottleTime: decoded.throttleTime
    };
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/leaveGroup/v3/request.js
var require_request58 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { LeaveGroup: apiKey } = require_apiKeys();
  module.exports = ({ groupId, members }) => ({
    apiKey,
    apiVersion: 3,
    apiName: "LeaveGroup",
    encode: async () => {
      return new Encoder().writeString(groupId).writeArray(members.map((member) => encodeMember(member)));
    }
  });
  var encodeMember = ({ memberId, groupInstanceId = null }) => {
    return new Encoder().writeString(memberId).writeString(groupInstanceId);
  };
});

// node_modules/kafkajs/src/protocol/requests/leaveGroup/v3/response.js
var require_response58 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failIfVersionNotSupported, failure, createErrorFromCode } = require_error();
  var { parse: parseV2 } = require_response57();
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    const errorCode = decoder.readInt16();
    const members = decoder.readArray(decodeMembers);
    failIfVersionNotSupported(errorCode);
    return { throttleTime: 0, clientSideThrottleTime: throttleTime, errorCode, members };
  };
  var decodeMembers = (decoder) => ({
    memberId: decoder.readString(),
    groupInstanceId: decoder.readString(),
    errorCode: decoder.readInt16()
  });
  var parse = async (data) => {
    const parsed = parseV2(data);
    const memberWithError = data.members.find((member) => failure(member.errorCode));
    if (memberWithError) {
      throw createErrorFromCode(memberWithError.errorCode);
    }
    return parsed;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/leaveGroup/index.js
var require_leaveGroup = __commonJS((exports, module) => {
  var versions = {
    0: ({ groupId, memberId }) => {
      const request3 = require_request55();
      const response = require_response55();
      return {
        request: request3({ groupId, memberId }),
        response
      };
    },
    1: ({ groupId, memberId }) => {
      const request3 = require_request56();
      const response = require_response56();
      return {
        request: request3({ groupId, memberId }),
        response
      };
    },
    2: ({ groupId, memberId }) => {
      const request3 = require_request57();
      const response = require_response57();
      return {
        request: request3({ groupId, memberId }),
        response
      };
    },
    3: ({ groupId, memberId, groupInstanceId }) => {
      const request3 = require_request58();
      const response = require_response58();
      return {
        request: request3({ groupId, members: [{ memberId, groupInstanceId }] }),
        response
      };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/syncGroup/v0/request.js
var require_request59 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { SyncGroup: apiKey } = require_apiKeys();
  module.exports = ({ groupId, generationId, memberId, groupAssignment }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "SyncGroup",
    encode: async () => {
      return new Encoder().writeString(groupId).writeInt32(generationId).writeString(memberId).writeArray(groupAssignment.map(encodeGroupAssignment));
    }
  });
  var encodeGroupAssignment = ({ memberId, memberAssignment }) => {
    return new Encoder().writeString(memberId).writeBytes(memberAssignment);
  };
});

// node_modules/kafkajs/src/protocol/requests/syncGroup/v0/response.js
var require_response59 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode, failIfVersionNotSupported } = require_error();
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const errorCode = decoder.readInt16();
    failIfVersionNotSupported(errorCode);
    return {
      errorCode,
      memberAssignment: decoder.readBytes()
    };
  };
  var parse = async (data) => {
    if (failure(data.errorCode)) {
      throw createErrorFromCode(data.errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/syncGroup/v1/request.js
var require_request60 = __commonJS((exports, module) => {
  var requestV0 = require_request59();
  module.exports = ({ groupId, generationId, memberId, groupAssignment }) => Object.assign(requestV0({ groupId, generationId, memberId, groupAssignment }), { apiVersion: 1 });
});

// node_modules/kafkajs/src/protocol/requests/syncGroup/v1/response.js
var require_response60 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failIfVersionNotSupported } = require_error();
  var { parse: parseV0 } = require_response59();
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    const errorCode = decoder.readInt16();
    failIfVersionNotSupported(errorCode);
    return {
      throttleTime,
      errorCode,
      memberAssignment: decoder.readBytes()
    };
  };
  module.exports = {
    decode,
    parse: parseV0
  };
});

// node_modules/kafkajs/src/protocol/requests/syncGroup/v2/request.js
var require_request61 = __commonJS((exports, module) => {
  var requestV1 = require_request60();
  module.exports = ({ groupId, generationId, memberId, groupAssignment }) => Object.assign(requestV1({ groupId, generationId, memberId, groupAssignment }), { apiVersion: 2 });
});

// node_modules/kafkajs/src/protocol/requests/syncGroup/v2/response.js
var require_response61 = __commonJS((exports, module) => {
  var { parse, decode: decodeV1 } = require_response60();
  var decode = async (rawData) => {
    const decoded = await decodeV1(rawData);
    return {
      ...decoded,
      throttleTime: 0,
      clientSideThrottleTime: decoded.throttleTime
    };
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/syncGroup/v3/request.js
var require_request62 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { SyncGroup: apiKey } = require_apiKeys();
  module.exports = ({
    groupId,
    generationId,
    memberId,
    groupInstanceId = null,
    groupAssignment
  }) => ({
    apiKey,
    apiVersion: 3,
    apiName: "SyncGroup",
    encode: async () => {
      return new Encoder().writeString(groupId).writeInt32(generationId).writeString(memberId).writeString(groupInstanceId).writeArray(groupAssignment.map(encodeGroupAssignment));
    }
  });
  var encodeGroupAssignment = ({ memberId, memberAssignment }) => {
    return new Encoder().writeString(memberId).writeBytes(memberAssignment);
  };
});

// node_modules/kafkajs/src/protocol/requests/syncGroup/v3/response.js
var require_response62 = __commonJS((exports, module) => {
  var { decode, parse } = require_response61();
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/syncGroup/index.js
var require_syncGroup = __commonJS((exports, module) => {
  var versions = {
    0: ({ groupId, generationId, memberId, groupAssignment }) => {
      const request3 = require_request59();
      const response = require_response59();
      return {
        request: request3({ groupId, generationId, memberId, groupAssignment }),
        response
      };
    },
    1: ({ groupId, generationId, memberId, groupAssignment }) => {
      const request3 = require_request60();
      const response = require_response60();
      return {
        request: request3({ groupId, generationId, memberId, groupAssignment }),
        response
      };
    },
    2: ({ groupId, generationId, memberId, groupAssignment }) => {
      const request3 = require_request61();
      const response = require_response61();
      return {
        request: request3({ groupId, generationId, memberId, groupAssignment }),
        response
      };
    },
    3: ({ groupId, generationId, memberId, groupInstanceId, groupAssignment }) => {
      const request3 = require_request62();
      const response = require_response62();
      return {
        request: request3({ groupId, generationId, memberId, groupInstanceId, groupAssignment }),
        response
      };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/describeGroups/v0/request.js
var require_request63 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { DescribeGroups: apiKey } = require_apiKeys();
  module.exports = ({ groupIds }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "DescribeGroups",
    encode: async () => {
      return new Encoder().writeArray(groupIds);
    }
  });
});

// node_modules/kafkajs/src/protocol/requests/describeGroups/v0/response.js
var require_response63 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode } = require_error();
  var decoderMember = (decoder) => ({
    memberId: decoder.readString(),
    clientId: decoder.readString(),
    clientHost: decoder.readString(),
    memberMetadata: decoder.readBytes(),
    memberAssignment: decoder.readBytes()
  });
  var decodeGroup = (decoder) => ({
    errorCode: decoder.readInt16(),
    groupId: decoder.readString(),
    state: decoder.readString(),
    protocolType: decoder.readString(),
    protocol: decoder.readString(),
    members: decoder.readArray(decoderMember)
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const groups = decoder.readArray(decodeGroup);
    return {
      groups
    };
  };
  var parse = async (data) => {
    const groupsWithError = data.groups.filter(({ errorCode }) => failure(errorCode));
    if (groupsWithError.length > 0) {
      throw createErrorFromCode(groupsWithError[0].errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/describeGroups/v1/request.js
var require_request64 = __commonJS((exports, module) => {
  var requestV0 = require_request63();
  module.exports = ({ groupIds }) => Object.assign(requestV0({ groupIds }), { apiVersion: 1 });
});

// node_modules/kafkajs/src/protocol/requests/describeGroups/v1/response.js
var require_response64 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { parse: parseV0 } = require_response63();
  var decoderMember = (decoder) => ({
    memberId: decoder.readString(),
    clientId: decoder.readString(),
    clientHost: decoder.readString(),
    memberMetadata: decoder.readBytes(),
    memberAssignment: decoder.readBytes()
  });
  var decodeGroup = (decoder) => ({
    errorCode: decoder.readInt16(),
    groupId: decoder.readString(),
    state: decoder.readString(),
    protocolType: decoder.readString(),
    protocol: decoder.readString(),
    members: decoder.readArray(decoderMember)
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    const groups = decoder.readArray(decodeGroup);
    return {
      throttleTime,
      groups
    };
  };
  module.exports = {
    decode,
    parse: parseV0
  };
});

// node_modules/kafkajs/src/protocol/requests/describeGroups/v2/request.js
var require_request65 = __commonJS((exports, module) => {
  var requestV1 = require_request64();
  module.exports = ({ groupIds }) => Object.assign(requestV1({ groupIds }), { apiVersion: 2 });
});

// node_modules/kafkajs/src/protocol/requests/describeGroups/v2/response.js
var require_response65 = __commonJS((exports, module) => {
  var { parse, decode: decodeV1 } = require_response64();
  var decode = async (rawData) => {
    const decoded = await decodeV1(rawData);
    return {
      ...decoded,
      throttleTime: 0,
      clientSideThrottleTime: decoded.throttleTime
    };
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/describeGroups/index.js
var require_describeGroups = __commonJS((exports, module) => {
  var versions = {
    0: ({ groupIds }) => {
      const request3 = require_request63();
      const response = require_response63();
      return { request: request3({ groupIds }), response };
    },
    1: ({ groupIds }) => {
      const request3 = require_request64();
      const response = require_response64();
      return { request: request3({ groupIds }), response };
    },
    2: ({ groupIds }) => {
      const request3 = require_request65();
      const response = require_response65();
      return { request: request3({ groupIds }), response };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/listGroups/v0/request.js
var require_request66 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { ListGroups: apiKey } = require_apiKeys();
  module.exports = () => ({
    apiKey,
    apiVersion: 0,
    apiName: "ListGroups",
    encode: async () => {
      return new Encoder;
    }
  });
});

// node_modules/kafkajs/src/protocol/requests/listGroups/v0/response.js
var require_response66 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode } = require_error();
  var decodeGroup = (decoder) => ({
    groupId: decoder.readString(),
    protocolType: decoder.readString()
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const errorCode = decoder.readInt16();
    const groups = decoder.readArray(decodeGroup);
    return {
      errorCode,
      groups
    };
  };
  var parse = async (data) => {
    if (failure(data.errorCode)) {
      throw createErrorFromCode(data.errorCode);
    }
    return data;
  };
  module.exports = {
    decodeGroup,
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/listGroups/v1/request.js
var require_request67 = __commonJS((exports, module) => {
  var requestV0 = require_request66();
  module.exports = () => Object.assign(requestV0(), { apiVersion: 1 });
});

// node_modules/kafkajs/src/protocol/requests/listGroups/v1/response.js
var require_response67 = __commonJS((exports, module) => {
  var responseV0 = require_response66();
  var Decoder = require_decoder();
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    const errorCode = decoder.readInt16();
    const groups = decoder.readArray(responseV0.decodeGroup);
    return {
      throttleTime,
      errorCode,
      groups
    };
  };
  module.exports = {
    decode,
    parse: responseV0.parse
  };
});

// node_modules/kafkajs/src/protocol/requests/listGroups/v2/request.js
var require_request68 = __commonJS((exports, module) => {
  var requestV1 = require_request67();
  module.exports = () => Object.assign(requestV1(), { apiVersion: 2 });
});

// node_modules/kafkajs/src/protocol/requests/listGroups/v2/response.js
var require_response68 = __commonJS((exports, module) => {
  var { parse, decode: decodeV1 } = require_response67();
  var decode = async (rawData) => {
    const decoded = await decodeV1(rawData);
    return {
      ...decoded,
      throttleTime: 0,
      clientSideThrottleTime: decoded.throttleTime
    };
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/listGroups/index.js
var require_listGroups = __commonJS((exports, module) => {
  var versions = {
    0: () => {
      const request3 = require_request66();
      const response = require_response66();
      return { request: request3(), response };
    },
    1: () => {
      const request3 = require_request67();
      const response = require_response67();
      return { request: request3(), response };
    },
    2: () => {
      const request3 = require_request68();
      const response = require_response68();
      return { request: request3(), response };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/saslHandshake/v0/request.js
var require_request69 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { SaslHandshake: apiKey } = require_apiKeys();
  module.exports = ({ mechanism }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "SaslHandshake",
    encode: async () => new Encoder().writeString(mechanism)
  });
});

// node_modules/kafkajs/src/protocol/requests/saslHandshake/v0/response.js
var require_response69 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode, failIfVersionNotSupported } = require_error();
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const errorCode = decoder.readInt16();
    failIfVersionNotSupported(errorCode);
    return {
      errorCode,
      enabledMechanisms: decoder.readArray((decoder2) => decoder2.readString())
    };
  };
  var parse = async (data) => {
    if (failure(data.errorCode)) {
      throw createErrorFromCode(data.errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/saslHandshake/v1/request.js
var require_request70 = __commonJS((exports, module) => {
  var requestV0 = require_request69();
  module.exports = ({ mechanism }) => ({ ...requestV0({ mechanism }), apiVersion: 1 });
});

// node_modules/kafkajs/src/protocol/requests/saslHandshake/v1/response.js
var require_response70 = __commonJS((exports, module) => {
  var { decode: decodeV0, parse: parseV0 } = require_response69();
  module.exports = {
    decode: decodeV0,
    parse: parseV0
  };
});

// node_modules/kafkajs/src/protocol/requests/saslHandshake/index.js
var require_saslHandshake = __commonJS((exports, module) => {
  var versions = {
    0: ({ mechanism }) => {
      const request3 = require_request69();
      const response = require_response69();
      return { request: request3({ mechanism }), response };
    },
    1: ({ mechanism }) => {
      const request3 = require_request70();
      const response = require_response70();
      return { request: request3({ mechanism }), response };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/apiVersions/v0/request.js
var require_request71 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { ApiVersions: apiKey } = require_apiKeys();
  module.exports = () => ({
    apiKey,
    apiVersion: 0,
    apiName: "ApiVersions",
    encode: async () => new Encoder
  });
});

// node_modules/kafkajs/src/protocol/requests/apiVersions/v0/response.js
var require_response71 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode, failIfVersionNotSupported } = require_error();
  var apiVersion = (decoder) => ({
    apiKey: decoder.readInt16(),
    minVersion: decoder.readInt16(),
    maxVersion: decoder.readInt16()
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const errorCode = decoder.readInt16();
    failIfVersionNotSupported(errorCode);
    return {
      errorCode,
      apiVersions: decoder.readArray(apiVersion)
    };
  };
  var parse = async (data) => {
    if (failure(data.errorCode)) {
      throw createErrorFromCode(data.errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/apiVersions/v1/request.js
var require_request72 = __commonJS((exports, module) => {
  var requestV0 = require_request71();
  module.exports = () => ({ ...requestV0(), apiVersion: 1 });
});

// node_modules/kafkajs/src/protocol/requests/apiVersions/v1/response.js
var require_response72 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failIfVersionNotSupported } = require_error();
  var { parse: parseV0 } = require_response71();
  var apiVersion = (decoder) => ({
    apiKey: decoder.readInt16(),
    minVersion: decoder.readInt16(),
    maxVersion: decoder.readInt16()
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const errorCode = decoder.readInt16();
    failIfVersionNotSupported(errorCode);
    const apiVersions = decoder.readArray(apiVersion);
    const throttleTime = decoder.canReadInt32() ? decoder.readInt32() : 0;
    return {
      errorCode,
      apiVersions,
      throttleTime
    };
  };
  module.exports = {
    decode,
    parse: parseV0
  };
});

// node_modules/kafkajs/src/protocol/requests/apiVersions/v2/request.js
var require_request73 = __commonJS((exports, module) => {
  var requestV0 = require_request71();
  module.exports = () => ({ ...requestV0(), apiVersion: 2 });
});

// node_modules/kafkajs/src/protocol/requests/apiVersions/v2/response.js
var require_response73 = __commonJS((exports, module) => {
  var { parse, decode: decodeV1 } = require_response72();
  var decode = async (rawData) => {
    const decoded = await decodeV1(rawData);
    return {
      ...decoded,
      throttleTime: 0,
      clientSideThrottleTime: decoded.throttleTime
    };
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/apiVersions/index.js
var require_apiVersions = __commonJS((exports, module) => {
  var logResponseError = false;
  var versions = {
    0: () => {
      const request3 = require_request71();
      const response = require_response71();
      return { request: request3(), response, logResponseError: true };
    },
    1: () => {
      const request3 = require_request72();
      const response = require_response72();
      return { request: request3(), response, logResponseError };
    },
    2: () => {
      const request3 = require_request73();
      const response = require_response73();
      return { request: request3(), response, logResponseError };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/createTopics/v0/request.js
var require_request74 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { CreateTopics: apiKey } = require_apiKeys();
  module.exports = ({ topics, timeout = 5000 }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "CreateTopics",
    encode: async () => {
      return new Encoder().writeArray(topics.map(encodeTopics)).writeInt32(timeout);
    }
  });
  var encodeTopics = ({
    topic,
    numPartitions = -1,
    replicationFactor = -1,
    replicaAssignment = [],
    configEntries = []
  }) => {
    return new Encoder().writeString(topic).writeInt32(numPartitions).writeInt16(replicationFactor).writeArray(replicaAssignment.map(encodeReplicaAssignment)).writeArray(configEntries.map(encodeConfigEntries));
  };
  var encodeReplicaAssignment = ({ partition, replicas }) => {
    return new Encoder().writeInt32(partition).writeArray(replicas);
  };
  var encodeConfigEntries = ({ name, value }) => {
    return new Encoder().writeString(name).writeString(value);
  };
});

// node_modules/kafkajs/src/protocol/requests/createTopics/v0/response.js
var require_response74 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode } = require_error();
  var { KafkaJSAggregateError, KafkaJSCreateTopicError } = require_errors();
  var topicNameComparator = (a, b) => a.topic.localeCompare(b.topic);
  var topicErrors = (decoder) => ({
    topic: decoder.readString(),
    errorCode: decoder.readInt16()
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    return {
      topicErrors: decoder.readArray(topicErrors).sort(topicNameComparator)
    };
  };
  var parse = async (data) => {
    const topicsWithError = data.topicErrors.filter(({ errorCode }) => failure(errorCode));
    if (topicsWithError.length > 0) {
      throw new KafkaJSAggregateError("Topic creation errors", topicsWithError.map((error) => new KafkaJSCreateTopicError(createErrorFromCode(error.errorCode), error.topic)));
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/createTopics/v1/request.js
var require_request75 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { CreateTopics: apiKey } = require_apiKeys();
  module.exports = ({ topics, validateOnly = false, timeout = 5000 }) => ({
    apiKey,
    apiVersion: 1,
    apiName: "CreateTopics",
    encode: async () => {
      return new Encoder().writeArray(topics.map(encodeTopics)).writeInt32(timeout).writeBoolean(validateOnly);
    }
  });
  var encodeTopics = ({
    topic,
    numPartitions = -1,
    replicationFactor = -1,
    replicaAssignment = [],
    configEntries = []
  }) => {
    return new Encoder().writeString(topic).writeInt32(numPartitions).writeInt16(replicationFactor).writeArray(replicaAssignment.map(encodeReplicaAssignment)).writeArray(configEntries.map(encodeConfigEntries));
  };
  var encodeReplicaAssignment = ({ partition, replicas }) => {
    return new Encoder().writeInt32(partition).writeArray(replicas);
  };
  var encodeConfigEntries = ({ name, value }) => {
    return new Encoder().writeString(name).writeString(value);
  };
});

// node_modules/kafkajs/src/protocol/requests/createTopics/v1/response.js
var require_response75 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { parse: parseV0 } = require_response74();
  var topicNameComparator = (a, b) => a.topic.localeCompare(b.topic);
  var topicErrors = (decoder) => ({
    topic: decoder.readString(),
    errorCode: decoder.readInt16(),
    errorMessage: decoder.readString()
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    return {
      topicErrors: decoder.readArray(topicErrors).sort(topicNameComparator)
    };
  };
  module.exports = {
    decode,
    parse: parseV0
  };
});

// node_modules/kafkajs/src/protocol/requests/createTopics/v2/request.js
var require_request76 = __commonJS((exports, module) => {
  var requestV1 = require_request75();
  module.exports = ({ topics, validateOnly, timeout }) => Object.assign(requestV1({ topics, validateOnly, timeout }), { apiVersion: 2 });
});

// node_modules/kafkajs/src/protocol/requests/createTopics/v2/response.js
var require_response76 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { parse: parseV1 } = require_response75();
  var topicNameComparator = (a, b) => a.topic.localeCompare(b.topic);
  var topicErrors = (decoder) => ({
    topic: decoder.readString(),
    errorCode: decoder.readInt16(),
    errorMessage: decoder.readString()
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    return {
      throttleTime: decoder.readInt32(),
      topicErrors: decoder.readArray(topicErrors).sort(topicNameComparator)
    };
  };
  module.exports = {
    decode,
    parse: parseV1
  };
});

// node_modules/kafkajs/src/protocol/requests/createTopics/v3/request.js
var require_request77 = __commonJS((exports, module) => {
  var requestV2 = require_request76();
  module.exports = ({ topics, validateOnly, timeout }) => Object.assign(requestV2({ topics, validateOnly, timeout }), { apiVersion: 3 });
});

// node_modules/kafkajs/src/protocol/requests/createTopics/v3/response.js
var require_response77 = __commonJS((exports, module) => {
  var { parse, decode: decodeV2 } = require_response76();
  var decode = async (rawData) => {
    const decoded = await decodeV2(rawData);
    return {
      ...decoded,
      throttleTime: 0,
      clientSideThrottleTime: decoded.throttleTime
    };
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/createTopics/index.js
var require_createTopics = __commonJS((exports, module) => {
  var versions = {
    0: ({ topics, timeout }) => {
      const request3 = require_request74();
      const response = require_response74();
      return { request: request3({ topics, timeout }), response };
    },
    1: ({ topics, validateOnly, timeout }) => {
      const request3 = require_request75();
      const response = require_response75();
      return { request: request3({ topics, validateOnly, timeout }), response };
    },
    2: ({ topics, validateOnly, timeout }) => {
      const request3 = require_request76();
      const response = require_response76();
      return { request: request3({ topics, validateOnly, timeout }), response };
    },
    3: ({ topics, validateOnly, timeout }) => {
      const request3 = require_request77();
      const response = require_response77();
      return { request: request3({ topics, validateOnly, timeout }), response };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/deleteTopics/v0/request.js
var require_request78 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { DeleteTopics: apiKey } = require_apiKeys();
  module.exports = ({ topics, timeout = 5000 }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "DeleteTopics",
    encode: async () => {
      return new Encoder().writeArray(topics).writeInt32(timeout);
    }
  });
});

// node_modules/kafkajs/src/protocol/requests/deleteTopics/v0/response.js
var require_response78 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode } = require_error();
  var topicNameComparator = (a, b) => a.topic.localeCompare(b.topic);
  var topicErrors = (decoder) => ({
    topic: decoder.readString(),
    errorCode: decoder.readInt16()
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    return {
      topicErrors: decoder.readArray(topicErrors).sort(topicNameComparator)
    };
  };
  var parse = async (data) => {
    const topicsWithError = data.topicErrors.filter(({ errorCode }) => failure(errorCode));
    if (topicsWithError.length > 0) {
      throw createErrorFromCode(topicsWithError[0].errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/deleteTopics/v1/request.js
var require_request79 = __commonJS((exports, module) => {
  var requestV0 = require_request78();
  module.exports = ({ topics, timeout }) => Object.assign(requestV0({ topics, timeout }), { apiVersion: 1 });
});

// node_modules/kafkajs/src/protocol/requests/deleteTopics/v1/response.js
var require_response79 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { parse: parseV0 } = require_response78();
  var topicNameComparator = (a, b) => a.topic.localeCompare(b.topic);
  var topicErrors = (decoder) => ({
    topic: decoder.readString(),
    errorCode: decoder.readInt16()
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    return {
      throttleTime: 0,
      clientSideThrottleTime: throttleTime,
      topicErrors: decoder.readArray(topicErrors).sort(topicNameComparator)
    };
  };
  module.exports = {
    decode,
    parse: parseV0
  };
});

// node_modules/kafkajs/src/protocol/requests/deleteTopics/index.js
var require_deleteTopics = __commonJS((exports, module) => {
  var versions = {
    0: ({ topics, timeout }) => {
      const request3 = require_request78();
      const response = require_response78();
      return { request: request3({ topics, timeout }), response };
    },
    1: ({ topics, timeout }) => {
      const request3 = require_request79();
      const response = require_response79();
      return { request: request3({ topics, timeout }), response };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/deleteRecords/v0/request.js
var require_request80 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { DeleteRecords: apiKey } = require_apiKeys();
  module.exports = ({ topics, timeout = 5000 }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "DeleteRecords",
    encode: async () => {
      return new Encoder().writeArray(topics.map(({ topic, partitions }) => {
        return new Encoder().writeString(topic).writeArray(partitions.map(({ partition, offset }) => {
          return new Encoder().writeInt32(partition).writeInt64(offset);
        }));
      })).writeInt32(timeout);
    }
  });
});

// node_modules/kafkajs/src/protocol/requests/deleteRecords/v0/response.js
var require_response80 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { KafkaJSDeleteTopicRecordsError } = require_errors();
  var { failure, createErrorFromCode } = require_error();
  var topicNameComparator = (a, b) => a.topic.localeCompare(b.topic);
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    return {
      throttleTime: decoder.readInt32(),
      topics: decoder.readArray((decoder2) => ({
        topic: decoder2.readString(),
        partitions: decoder2.readArray((decoder3) => ({
          partition: decoder3.readInt32(),
          lowWatermark: decoder3.readInt64(),
          errorCode: decoder3.readInt16()
        }))
      })).sort(topicNameComparator)
    };
  };
  var parse = (requestTopics) => async (data) => {
    const topicsWithErrors = data.topics.map(({ partitions }) => ({
      partitionsWithErrors: partitions.filter(({ errorCode }) => failure(errorCode))
    })).filter(({ partitionsWithErrors }) => partitionsWithErrors.length);
    if (topicsWithErrors.length > 0) {
      const [{ topic }] = data.topics;
      const [{ partitions: requestPartitions }] = requestTopics;
      const [{ partitionsWithErrors }] = topicsWithErrors;
      throw new KafkaJSDeleteTopicRecordsError({
        topic,
        partitions: partitionsWithErrors.map(({ partition, errorCode }) => ({
          partition,
          error: createErrorFromCode(errorCode),
          offset: requestPartitions.find((p) => p.partition === partition).offset
        }))
      });
    }
    return data;
  };
  module.exports = ({ topics }) => ({
    decode,
    parse: parse(topics)
  });
});

// node_modules/kafkajs/src/protocol/requests/deleteRecords/v1/request.js
var require_request81 = __commonJS((exports, module) => {
  var requestV0 = require_request80();
  module.exports = ({ topics, timeout }) => Object.assign(requestV0({ topics, timeout }), { apiVersion: 1 });
});

// node_modules/kafkajs/src/protocol/requests/deleteRecords/v1/response.js
var require_response81 = __commonJS((exports, module) => {
  var responseV0 = require_response80();
  module.exports = ({ topics }) => {
    const { parse, decode: decodeV0 } = responseV0({ topics });
    const decode = async (rawData) => {
      const decoded = await decodeV0(rawData);
      return {
        ...decoded,
        throttleTime: 0,
        clientSideThrottleTime: decoded.throttleTime
      };
    };
    return {
      decode,
      parse
    };
  };
});

// node_modules/kafkajs/src/protocol/requests/deleteRecords/index.js
var require_deleteRecords = __commonJS((exports, module) => {
  var versions = {
    0: ({ topics, timeout }) => {
      const request3 = require_request80();
      const response = require_response80();
      return { request: request3({ topics, timeout }), response: response({ topics }) };
    },
    1: ({ topics, timeout }) => {
      const request3 = require_request81();
      const response = require_response81();
      return { request: request3({ topics, timeout }), response: response({ topics }) };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/initProducerId/v0/request.js
var require_request82 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { InitProducerId: apiKey } = require_apiKeys();
  module.exports = ({ transactionalId, transactionTimeout }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "InitProducerId",
    encode: async () => {
      return new Encoder().writeString(transactionalId).writeInt32(transactionTimeout);
    }
  });
});

// node_modules/kafkajs/src/protocol/requests/initProducerId/v0/response.js
var require_response82 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode, failIfVersionNotSupported } = require_error();
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    const errorCode = decoder.readInt16();
    failIfVersionNotSupported(errorCode);
    return {
      throttleTime,
      errorCode,
      producerId: decoder.readInt64().toString(),
      producerEpoch: decoder.readInt16()
    };
  };
  var parse = async (data) => {
    if (failure(data.errorCode)) {
      throw createErrorFromCode(data.errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/initProducerId/v1/request.js
var require_request83 = __commonJS((exports, module) => {
  var requestV0 = require_request82();
  module.exports = ({ transactionalId, transactionTimeout }) => Object.assign(requestV0({ transactionalId, transactionTimeout }), { apiVersion: 1 });
});

// node_modules/kafkajs/src/protocol/requests/initProducerId/v1/response.js
var require_response83 = __commonJS((exports, module) => {
  var { parse, decode: decodeV0 } = require_response82();
  var decode = async (rawData) => {
    const decoded = await decodeV0(rawData);
    return {
      ...decoded,
      throttleTime: 0,
      clientSideThrottleTime: decoded.throttleTime
    };
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/initProducerId/index.js
var require_initProducerId = __commonJS((exports, module) => {
  var versions = {
    0: ({ transactionalId, transactionTimeout = 5000 }) => {
      const request3 = require_request82();
      const response = require_response82();
      return { request: request3({ transactionalId, transactionTimeout }), response };
    },
    1: ({ transactionalId, transactionTimeout = 5000 }) => {
      const request3 = require_request83();
      const response = require_response83();
      return { request: request3({ transactionalId, transactionTimeout }), response };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/addPartitionsToTxn/v0/request.js
var require_request84 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { AddPartitionsToTxn: apiKey } = require_apiKeys();
  module.exports = ({ transactionalId, producerId, producerEpoch, topics }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "AddPartitionsToTxn",
    encode: async () => {
      return new Encoder().writeString(transactionalId).writeInt64(producerId).writeInt16(producerEpoch).writeArray(topics.map(encodeTopic));
    }
  });
  var encodeTopic = ({ topic, partitions }) => {
    return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition));
  };
  var encodePartition = (partition) => {
    return new Encoder().writeInt32(partition);
  };
});

// node_modules/kafkajs/src/protocol/requests/addPartitionsToTxn/v0/response.js
var require_response84 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode } = require_error();
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    const errors = await decoder.readArrayAsync(decodeError);
    return {
      throttleTime,
      errors
    };
  };
  var decodeError = async (decoder) => ({
    topic: decoder.readString(),
    partitionErrors: await decoder.readArrayAsync(decodePartitionError)
  });
  var decodePartitionError = (decoder) => ({
    partition: decoder.readInt32(),
    errorCode: decoder.readInt16()
  });
  var parse = async (data) => {
    const topicsWithErrors = data.errors.map(({ partitionErrors }) => ({
      partitionsWithErrors: partitionErrors.filter(({ errorCode }) => failure(errorCode))
    })).filter(({ partitionsWithErrors }) => partitionsWithErrors.length);
    if (topicsWithErrors.length > 0) {
      throw createErrorFromCode(topicsWithErrors[0].partitionsWithErrors[0].errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/addPartitionsToTxn/v1/request.js
var require_request85 = __commonJS((exports, module) => {
  var requestV0 = require_request84();
  module.exports = ({ transactionalId, producerId, producerEpoch, topics }) => Object.assign(requestV0({
    transactionalId,
    producerId,
    producerEpoch,
    topics
  }), { apiVersion: 1 });
});

// node_modules/kafkajs/src/protocol/requests/addPartitionsToTxn/v1/response.js
var require_response85 = __commonJS((exports, module) => {
  var { parse, decode: decodeV0 } = require_response84();
  var decode = async (rawData) => {
    const decoded = await decodeV0(rawData);
    return {
      ...decoded,
      throttleTime: 0,
      clientSideThrottleTime: decoded.throttleTime
    };
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/addPartitionsToTxn/index.js
var require_addPartitionsToTxn = __commonJS((exports, module) => {
  var versions = {
    0: ({ transactionalId, producerId, producerEpoch, topics }) => {
      const request3 = require_request84();
      const response = require_response84();
      return { request: request3({ transactionalId, producerId, producerEpoch, topics }), response };
    },
    1: ({ transactionalId, producerId, producerEpoch, topics }) => {
      const request3 = require_request85();
      const response = require_response85();
      return { request: request3({ transactionalId, producerId, producerEpoch, topics }), response };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/addOffsetsToTxn/v0/request.js
var require_request86 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { AddOffsetsToTxn: apiKey } = require_apiKeys();
  module.exports = ({ transactionalId, producerId, producerEpoch, groupId }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "AddOffsetsToTxn",
    encode: async () => {
      return new Encoder().writeString(transactionalId).writeInt64(producerId).writeInt16(producerEpoch).writeString(groupId);
    }
  });
});

// node_modules/kafkajs/src/protocol/requests/addOffsetsToTxn/v0/response.js
var require_response86 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode, failIfVersionNotSupported } = require_error();
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    const errorCode = decoder.readInt16();
    failIfVersionNotSupported(errorCode);
    return {
      throttleTime,
      errorCode
    };
  };
  var parse = async (data) => {
    if (failure(data.errorCode)) {
      throw createErrorFromCode(data.errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/addOffsetsToTxn/v1/request.js
var require_request87 = __commonJS((exports, module) => {
  var requestV0 = require_request86();
  module.exports = ({ transactionalId, producerId, producerEpoch, groupId }) => Object.assign(requestV0({
    transactionalId,
    producerId,
    producerEpoch,
    groupId
  }), { apiVersion: 1 });
});

// node_modules/kafkajs/src/protocol/requests/addOffsetsToTxn/v1/response.js
var require_response87 = __commonJS((exports, module) => {
  var { parse, decode: decodeV0 } = require_response86();
  var decode = async (rawData) => {
    const decoded = await decodeV0(rawData);
    return {
      ...decoded,
      throttleTime: 0,
      clientSideThrottleTime: decoded.throttleTime
    };
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/addOffsetsToTxn/index.js
var require_addOffsetsToTxn = __commonJS((exports, module) => {
  var versions = {
    0: ({ transactionalId, producerId, producerEpoch, groupId }) => {
      const request3 = require_request86();
      const response = require_response86();
      return { request: request3({ transactionalId, producerId, producerEpoch, groupId }), response };
    },
    1: ({ transactionalId, producerId, producerEpoch, groupId }) => {
      const request3 = require_request87();
      const response = require_response87();
      return { request: request3({ transactionalId, producerId, producerEpoch, groupId }), response };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/endTxn/v0/request.js
var require_request88 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { EndTxn: apiKey } = require_apiKeys();
  module.exports = ({ transactionalId, producerId, producerEpoch, transactionResult }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "EndTxn",
    encode: async () => {
      return new Encoder().writeString(transactionalId).writeInt64(producerId).writeInt16(producerEpoch).writeBoolean(transactionResult);
    }
  });
});

// node_modules/kafkajs/src/protocol/requests/endTxn/v0/response.js
var require_response88 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode, failIfVersionNotSupported } = require_error();
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    const errorCode = decoder.readInt16();
    failIfVersionNotSupported(errorCode);
    return {
      throttleTime,
      errorCode
    };
  };
  var parse = async (data) => {
    if (failure(data.errorCode)) {
      throw createErrorFromCode(data.errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/endTxn/v1/request.js
var require_request89 = __commonJS((exports, module) => {
  var requestV0 = require_request88();
  module.exports = ({ transactionalId, producerId, producerEpoch, transactionResult }) => Object.assign(requestV0({ transactionalId, producerId, producerEpoch, transactionResult }), {
    apiVersion: 1
  });
});

// node_modules/kafkajs/src/protocol/requests/endTxn/v1/response.js
var require_response89 = __commonJS((exports, module) => {
  var { parse, decode: decodeV0 } = require_response88();
  var decode = async (rawData) => {
    const decoded = await decodeV0(rawData);
    return {
      ...decoded,
      throttleTime: 0,
      clientSideThrottleTime: decoded.throttleTime
    };
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/endTxn/index.js
var require_endTxn = __commonJS((exports, module) => {
  var versions = {
    0: ({ transactionalId, producerId, producerEpoch, transactionResult }) => {
      const request3 = require_request88();
      const response = require_response88();
      return {
        request: request3({ transactionalId, producerId, producerEpoch, transactionResult }),
        response
      };
    },
    1: ({ transactionalId, producerId, producerEpoch, transactionResult }) => {
      const request3 = require_request89();
      const response = require_response89();
      return {
        request: request3({ transactionalId, producerId, producerEpoch, transactionResult }),
        response
      };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/txnOffsetCommit/v0/request.js
var require_request90 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { TxnOffsetCommit: apiKey } = require_apiKeys();
  module.exports = ({ transactionalId, groupId, producerId, producerEpoch, topics }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "TxnOffsetCommit",
    encode: async () => {
      return new Encoder().writeString(transactionalId).writeString(groupId).writeInt64(producerId).writeInt16(producerEpoch).writeArray(topics.map(encodeTopic));
    }
  });
  var encodeTopic = ({ topic, partitions }) => {
    return new Encoder().writeString(topic).writeArray(partitions.map(encodePartition));
  };
  var encodePartition = ({ partition, offset, metadata }) => {
    return new Encoder().writeInt32(partition).writeInt64(offset).writeString(metadata);
  };
});

// node_modules/kafkajs/src/protocol/requests/txnOffsetCommit/v0/response.js
var require_response90 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode } = require_error();
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    const topics = await decoder.readArrayAsync(decodeTopic);
    return {
      throttleTime,
      topics
    };
  };
  var decodeTopic = async (decoder) => ({
    topic: decoder.readString(),
    partitions: await decoder.readArrayAsync(decodePartition)
  });
  var decodePartition = (decoder) => ({
    partition: decoder.readInt32(),
    errorCode: decoder.readInt16()
  });
  var parse = async (data) => {
    const topicsWithErrors = data.topics.map(({ partitions }) => ({
      partitionsWithErrors: partitions.filter(({ errorCode }) => failure(errorCode))
    })).filter(({ partitionsWithErrors }) => partitionsWithErrors.length);
    if (topicsWithErrors.length > 0) {
      throw createErrorFromCode(topicsWithErrors[0].partitionsWithErrors[0].errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/txnOffsetCommit/v1/request.js
var require_request91 = __commonJS((exports, module) => {
  var requestV0 = require_request90();
  module.exports = ({ transactionalId, groupId, producerId, producerEpoch, topics }) => Object.assign(requestV0({ transactionalId, groupId, producerId, producerEpoch, topics }), {
    apiVersion: 1
  });
});

// node_modules/kafkajs/src/protocol/requests/txnOffsetCommit/v1/response.js
var require_response91 = __commonJS((exports, module) => {
  var { parse, decode: decodeV1 } = require_response90();
  var decode = async (rawData) => {
    const decoded = await decodeV1(rawData);
    return {
      ...decoded,
      throttleTime: 0,
      clientSideThrottleTime: decoded.throttleTime
    };
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/txnOffsetCommit/index.js
var require_txnOffsetCommit = __commonJS((exports, module) => {
  var versions = {
    0: ({ transactionalId, groupId, producerId, producerEpoch, topics }) => {
      const request3 = require_request90();
      const response = require_response90();
      return {
        request: request3({ transactionalId, groupId, producerId, producerEpoch, topics }),
        response
      };
    },
    1: ({ transactionalId, groupId, producerId, producerEpoch, topics }) => {
      const request3 = require_request91();
      const response = require_response91();
      return {
        request: request3({ transactionalId, groupId, producerId, producerEpoch, topics }),
        response
      };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/describeAcls/v0/request.js
var require_request92 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { DescribeAcls: apiKey } = require_apiKeys();
  module.exports = ({ resourceType, resourceName, principal, host, operation, permissionType }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "DescribeAcls",
    encode: async () => {
      return new Encoder().writeInt8(resourceType).writeString(resourceName).writeString(principal).writeString(host).writeInt8(operation).writeInt8(permissionType);
    }
  });
});

// node_modules/kafkajs/src/protocol/requests/describeAcls/v0/response.js
var require_response92 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode } = require_error();
  var decodeAcls = (decoder) => ({
    principal: decoder.readString(),
    host: decoder.readString(),
    operation: decoder.readInt8(),
    permissionType: decoder.readInt8()
  });
  var decodeResources = (decoder) => ({
    resourceType: decoder.readInt8(),
    resourceName: decoder.readString(),
    acls: decoder.readArray(decodeAcls)
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    const errorCode = decoder.readInt16();
    const errorMessage = decoder.readString();
    const resources = decoder.readArray(decodeResources);
    return {
      throttleTime,
      errorCode,
      errorMessage,
      resources
    };
  };
  var parse = async (data) => {
    if (failure(data.errorCode)) {
      throw createErrorFromCode(data.errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/describeAcls/v1/request.js
var require_request93 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { DescribeAcls: apiKey } = require_apiKeys();
  module.exports = ({
    resourceType,
    resourceName,
    resourcePatternType,
    principal,
    host,
    operation,
    permissionType
  }) => ({
    apiKey,
    apiVersion: 1,
    apiName: "DescribeAcls",
    encode: async () => {
      return new Encoder().writeInt8(resourceType).writeString(resourceName).writeInt8(resourcePatternType).writeString(principal).writeString(host).writeInt8(operation).writeInt8(permissionType);
    }
  });
});

// node_modules/kafkajs/src/protocol/requests/describeAcls/v1/response.js
var require_response93 = __commonJS((exports, module) => {
  var { parse } = require_response92();
  var Decoder = require_decoder();
  var decodeAcls = (decoder) => ({
    principal: decoder.readString(),
    host: decoder.readString(),
    operation: decoder.readInt8(),
    permissionType: decoder.readInt8()
  });
  var decodeResources = (decoder) => ({
    resourceType: decoder.readInt8(),
    resourceName: decoder.readString(),
    resourcePatternType: decoder.readInt8(),
    acls: decoder.readArray(decodeAcls)
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    const errorCode = decoder.readInt16();
    const errorMessage = decoder.readString();
    const resources = decoder.readArray(decodeResources);
    return {
      throttleTime: 0,
      clientSideThrottleTime: throttleTime,
      errorCode,
      errorMessage,
      resources
    };
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/describeAcls/index.js
var require_describeAcls = __commonJS((exports, module) => {
  var versions = {
    0: ({ resourceType, resourceName, principal, host, operation, permissionType }) => {
      const request3 = require_request92();
      const response = require_response92();
      return {
        request: request3({ resourceType, resourceName, principal, host, operation, permissionType }),
        response
      };
    },
    1: ({
      resourceType,
      resourceName,
      resourcePatternType,
      principal,
      host,
      operation,
      permissionType
    }) => {
      const request3 = require_request93();
      const response = require_response93();
      return {
        request: request3({
          resourceType,
          resourceName,
          resourcePatternType,
          principal,
          host,
          operation,
          permissionType
        }),
        response
      };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/createAcls/v0/request.js
var require_request94 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { CreateAcls: apiKey } = require_apiKeys();
  var encodeCreations = ({
    resourceType,
    resourceName,
    principal,
    host,
    operation,
    permissionType
  }) => {
    return new Encoder().writeInt8(resourceType).writeString(resourceName).writeString(principal).writeString(host).writeInt8(operation).writeInt8(permissionType);
  };
  module.exports = ({ creations }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "CreateAcls",
    encode: async () => {
      return new Encoder().writeArray(creations.map(encodeCreations));
    }
  });
});

// node_modules/kafkajs/src/protocol/requests/createAcls/v0/response.js
var require_response94 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode } = require_error();
  var decodeCreationResponse = (decoder) => ({
    errorCode: decoder.readInt16(),
    errorMessage: decoder.readString()
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    const creationResponses = decoder.readArray(decodeCreationResponse);
    return {
      throttleTime,
      creationResponses
    };
  };
  var parse = async (data) => {
    const creationResponsesWithError = data.creationResponses.filter(({ errorCode }) => failure(errorCode));
    if (creationResponsesWithError.length > 0) {
      throw createErrorFromCode(creationResponsesWithError[0].errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/createAcls/v1/request.js
var require_request95 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { CreateAcls: apiKey } = require_apiKeys();
  var encodeCreations = ({
    resourceType,
    resourceName,
    resourcePatternType,
    principal,
    host,
    operation,
    permissionType
  }) => {
    return new Encoder().writeInt8(resourceType).writeString(resourceName).writeInt8(resourcePatternType).writeString(principal).writeString(host).writeInt8(operation).writeInt8(permissionType);
  };
  module.exports = ({ creations }) => ({
    apiKey,
    apiVersion: 1,
    apiName: "CreateAcls",
    encode: async () => {
      return new Encoder().writeArray(creations.map(encodeCreations));
    }
  });
});

// node_modules/kafkajs/src/protocol/requests/createAcls/v1/response.js
var require_response95 = __commonJS((exports, module) => {
  var { parse, decode: decodeV0 } = require_response94();
  var decode = async (rawData) => {
    const decoded = await decodeV0(rawData);
    return {
      ...decoded,
      throttleTime: 0,
      clientSideThrottleTime: decoded.throttleTime
    };
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/createAcls/index.js
var require_createAcls = __commonJS((exports, module) => {
  var versions = {
    0: ({ creations }) => {
      const request3 = require_request94();
      const response = require_response94();
      return { request: request3({ creations }), response };
    },
    1: ({ creations }) => {
      const request3 = require_request95();
      const response = require_response95();
      return { request: request3({ creations }), response };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/deleteAcls/v0/request.js
var require_request96 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { DeleteAcls: apiKey } = require_apiKeys();
  var encodeFilters = ({
    resourceType,
    resourceName,
    principal,
    host,
    operation,
    permissionType
  }) => {
    return new Encoder().writeInt8(resourceType).writeString(resourceName).writeString(principal).writeString(host).writeInt8(operation).writeInt8(permissionType);
  };
  module.exports = ({ filters }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "DeleteAcls",
    encode: async () => {
      return new Encoder().writeArray(filters.map(encodeFilters));
    }
  });
});

// node_modules/kafkajs/src/protocol/requests/deleteAcls/v0/response.js
var require_response96 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode } = require_error();
  var decodeMatchingAcls = (decoder) => ({
    errorCode: decoder.readInt16(),
    errorMessage: decoder.readString(),
    resourceType: decoder.readInt8(),
    resourceName: decoder.readString(),
    principal: decoder.readString(),
    host: decoder.readString(),
    operation: decoder.readInt8(),
    permissionType: decoder.readInt8()
  });
  var decodeFilterResponse = (decoder) => ({
    errorCode: decoder.readInt16(),
    errorMessage: decoder.readString(),
    matchingAcls: decoder.readArray(decodeMatchingAcls)
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    const filterResponses = decoder.readArray(decodeFilterResponse);
    return {
      throttleTime,
      filterResponses
    };
  };
  var parse = async (data) => {
    const filterResponsesWithError = data.filterResponses.filter(({ errorCode }) => failure(errorCode));
    if (filterResponsesWithError.length > 0) {
      throw createErrorFromCode(filterResponsesWithError[0].errorCode);
    }
    for (const filterResponse of data.filterResponses) {
      const matchingAcls = filterResponse.matchingAcls;
      const matchingAclsWithError = matchingAcls.filter(({ errorCode }) => failure(errorCode));
      if (matchingAclsWithError.length > 0) {
        throw createErrorFromCode(matchingAclsWithError[0].errorCode);
      }
    }
    return data;
  };
  module.exports = {
    decodeMatchingAcls,
    decodeFilterResponse,
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/deleteAcls/v1/request.js
var require_request97 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { DeleteAcls: apiKey } = require_apiKeys();
  var encodeFilters = ({
    resourceType,
    resourceName,
    resourcePatternType,
    principal,
    host,
    operation,
    permissionType
  }) => {
    return new Encoder().writeInt8(resourceType).writeString(resourceName).writeInt8(resourcePatternType).writeString(principal).writeString(host).writeInt8(operation).writeInt8(permissionType);
  };
  module.exports = ({ filters }) => ({
    apiKey,
    apiVersion: 1,
    apiName: "DeleteAcls",
    encode: async () => {
      return new Encoder().writeArray(filters.map(encodeFilters));
    }
  });
});

// node_modules/kafkajs/src/protocol/requests/deleteAcls/v1/response.js
var require_response97 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { parse: parseV0 } = require_response96();
  var decodeMatchingAcls = (decoder) => ({
    errorCode: decoder.readInt16(),
    errorMessage: decoder.readString(),
    resourceType: decoder.readInt8(),
    resourceName: decoder.readString(),
    resourcePatternType: decoder.readInt8(),
    principal: decoder.readString(),
    host: decoder.readString(),
    operation: decoder.readInt8(),
    permissionType: decoder.readInt8()
  });
  var decodeFilterResponse = (decoder) => ({
    errorCode: decoder.readInt16(),
    errorMessage: decoder.readString(),
    matchingAcls: decoder.readArray(decodeMatchingAcls)
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    const filterResponses = decoder.readArray(decodeFilterResponse);
    return {
      throttleTime: 0,
      clientSideThrottleTime: throttleTime,
      filterResponses
    };
  };
  module.exports = {
    decode,
    parse: parseV0
  };
});

// node_modules/kafkajs/src/protocol/requests/deleteAcls/index.js
var require_deleteAcls = __commonJS((exports, module) => {
  var versions = {
    0: ({ filters }) => {
      const request3 = require_request96();
      const response = require_response96();
      return { request: request3({ filters }), response };
    },
    1: ({ filters }) => {
      const request3 = require_request97();
      const response = require_response97();
      return { request: request3({ filters }), response };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/describeConfigs/v0/request.js
var require_request98 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { DescribeConfigs: apiKey } = require_apiKeys();
  module.exports = ({ resources }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "DescribeConfigs",
    encode: async () => {
      return new Encoder().writeArray(resources.map(encodeResource));
    }
  });
  var encodeResource = ({ type, name, configNames = [] }) => {
    return new Encoder().writeInt8(type).writeString(name).writeNullableArray(configNames);
  };
});

// node_modules/kafkajs/src/protocol/configSource.js
var require_configSource = __commonJS((exports, module) => {
  module.exports = {
    UNKNOWN: 0,
    TOPIC_CONFIG: 1,
    DYNAMIC_BROKER_CONFIG: 2,
    DYNAMIC_DEFAULT_BROKER_CONFIG: 3,
    STATIC_BROKER_CONFIG: 4,
    DEFAULT_CONFIG: 5,
    DYNAMIC_BROKER_LOGGER_CONFIG: 6
  };
});

// node_modules/kafkajs/src/protocol/configResourceTypes.js
var require_configResourceTypes = __commonJS((exports, module) => {
  module.exports = {
    UNKNOWN: 0,
    TOPIC: 2,
    BROKER: 4,
    BROKER_LOGGER: 8
  };
});

// node_modules/kafkajs/src/protocol/requests/describeConfigs/v0/response.js
var require_response98 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode } = require_error();
  var ConfigSource = require_configSource();
  var ConfigResourceTypes = require_configResourceTypes();
  var decodeConfigEntries = (decoder, resourceType) => {
    const configName = decoder.readString();
    const configValue = decoder.readString();
    const readOnly = decoder.readBoolean();
    const isDefault = decoder.readBoolean();
    const isSensitive = decoder.readBoolean();
    let configSource;
    if (isDefault) {
      configSource = ConfigSource.DEFAULT_CONFIG;
    } else {
      switch (resourceType) {
        case ConfigResourceTypes.BROKER:
          configSource = ConfigSource.STATIC_BROKER_CONFIG;
          break;
        case ConfigResourceTypes.TOPIC:
          configSource = ConfigSource.TOPIC_CONFIG;
          break;
        default:
          configSource = ConfigSource.UNKNOWN;
      }
    }
    return {
      configName,
      configValue,
      readOnly,
      isDefault,
      configSource,
      isSensitive
    };
  };
  var decodeResources = (decoder) => {
    const errorCode = decoder.readInt16();
    const errorMessage = decoder.readString();
    const resourceType = decoder.readInt8();
    const resourceName = decoder.readString();
    const configEntries = decoder.readArray((decoder2) => decodeConfigEntries(decoder2, resourceType));
    return {
      errorCode,
      errorMessage,
      resourceType,
      resourceName,
      configEntries
    };
  };
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    const resources = decoder.readArray(decodeResources);
    return {
      throttleTime,
      resources
    };
  };
  var parse = async (data) => {
    const resourcesWithError = data.resources.filter(({ errorCode }) => failure(errorCode));
    if (resourcesWithError.length > 0) {
      throw createErrorFromCode(resourcesWithError[0].errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/describeConfigs/v1/request.js
var require_request99 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { DescribeConfigs: apiKey } = require_apiKeys();
  module.exports = ({ resources, includeSynonyms = false }) => ({
    apiKey,
    apiVersion: 1,
    apiName: "DescribeConfigs",
    encode: async () => {
      return new Encoder().writeArray(resources.map(encodeResource)).writeBoolean(includeSynonyms);
    }
  });
  var encodeResource = ({ type, name, configNames = [] }) => {
    return new Encoder().writeInt8(type).writeString(name).writeNullableArray(configNames);
  };
});

// node_modules/kafkajs/src/protocol/requests/describeConfigs/v1/response.js
var require_response99 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { parse: parseV0 } = require_response98();
  var { DEFAULT_CONFIG } = require_configSource();
  var decodeSynonyms = (decoder) => ({
    configName: decoder.readString(),
    configValue: decoder.readString(),
    configSource: decoder.readInt8()
  });
  var decodeConfigEntries = (decoder) => {
    const configName = decoder.readString();
    const configValue = decoder.readString();
    const readOnly = decoder.readBoolean();
    const configSource = decoder.readInt8();
    const isSensitive = decoder.readBoolean();
    const configSynonyms = decoder.readArray(decodeSynonyms);
    return {
      configName,
      configValue,
      readOnly,
      isDefault: configSource === DEFAULT_CONFIG,
      configSource,
      isSensitive,
      configSynonyms
    };
  };
  var decodeResources = (decoder) => ({
    errorCode: decoder.readInt16(),
    errorMessage: decoder.readString(),
    resourceType: decoder.readInt8(),
    resourceName: decoder.readString(),
    configEntries: decoder.readArray(decodeConfigEntries)
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    const resources = decoder.readArray(decodeResources);
    return {
      throttleTime,
      resources
    };
  };
  module.exports = {
    decode,
    parse: parseV0
  };
});

// node_modules/kafkajs/src/protocol/requests/describeConfigs/v2/request.js
var require_request100 = __commonJS((exports, module) => {
  var requestV1 = require_request99();
  module.exports = ({ resources, includeSynonyms }) => Object.assign(requestV1({ resources, includeSynonyms }), { apiVersion: 2 });
});

// node_modules/kafkajs/src/protocol/requests/describeConfigs/v2/response.js
var require_response100 = __commonJS((exports, module) => {
  var { parse, decode: decodeV1 } = require_response99();
  var decode = async (rawData) => {
    const decoded = await decodeV1(rawData);
    return {
      ...decoded,
      throttleTime: 0,
      clientSideThrottleTime: decoded.throttleTime
    };
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/describeConfigs/index.js
var require_describeConfigs = __commonJS((exports, module) => {
  var versions = {
    0: ({ resources }) => {
      const request3 = require_request98();
      const response = require_response98();
      return { request: request3({ resources }), response };
    },
    1: ({ resources, includeSynonyms }) => {
      const request3 = require_request99();
      const response = require_response99();
      return { request: request3({ resources, includeSynonyms }), response };
    },
    2: ({ resources, includeSynonyms }) => {
      const request3 = require_request100();
      const response = require_response100();
      return { request: request3({ resources, includeSynonyms }), response };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/alterConfigs/v0/request.js
var require_request101 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { AlterConfigs: apiKey } = require_apiKeys();
  module.exports = ({ resources, validateOnly = false }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "AlterConfigs",
    encode: async () => {
      return new Encoder().writeArray(resources.map(encodeResource)).writeBoolean(validateOnly);
    }
  });
  var encodeResource = ({ type, name, configEntries }) => {
    return new Encoder().writeInt8(type).writeString(name).writeArray(configEntries.map(encodeConfigEntries));
  };
  var encodeConfigEntries = ({ name, value }) => {
    return new Encoder().writeString(name).writeString(value);
  };
});

// node_modules/kafkajs/src/protocol/requests/alterConfigs/v0/response.js
var require_response101 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode } = require_error();
  var decodeResources = (decoder) => ({
    errorCode: decoder.readInt16(),
    errorMessage: decoder.readString(),
    resourceType: decoder.readInt8(),
    resourceName: decoder.readString()
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    const resources = decoder.readArray(decodeResources);
    return {
      throttleTime,
      resources
    };
  };
  var parse = async (data) => {
    const resourcesWithError = data.resources.filter(({ errorCode }) => failure(errorCode));
    if (resourcesWithError.length > 0) {
      throw createErrorFromCode(resourcesWithError[0].errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/alterConfigs/v1/request.js
var require_request102 = __commonJS((exports, module) => {
  var requestV0 = require_request101();
  module.exports = ({ resources, validateOnly }) => Object.assign(requestV0({
    resources,
    validateOnly
  }), { apiVersion: 1 });
});

// node_modules/kafkajs/src/protocol/requests/alterConfigs/v1/response.js
var require_response102 = __commonJS((exports, module) => {
  var { parse, decode: decodeV0 } = require_response101();
  var decode = async (rawData) => {
    const decoded = await decodeV0(rawData);
    return {
      ...decoded,
      throttleTime: 0,
      clientSideThrottleTime: decoded.throttleTime
    };
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/alterConfigs/index.js
var require_alterConfigs = __commonJS((exports, module) => {
  var versions = {
    0: ({ resources, validateOnly }) => {
      const request3 = require_request101();
      const response = require_response101();
      return { request: request3({ resources, validateOnly }), response };
    },
    1: ({ resources, validateOnly }) => {
      const request3 = require_request102();
      const response = require_response102();
      return { request: request3({ resources, validateOnly }), response };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/saslAuthenticate/v0/request.js
var require_request103 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { SaslAuthenticate: apiKey } = require_apiKeys();
  module.exports = ({ authBytes }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "SaslAuthenticate",
    encode: async () => {
      return new Encoder().writeBuffer(authBytes);
    }
  });
});

// node_modules/kafkajs/src/protocol/requests/saslAuthenticate/v0/response.js
var require_response103 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var Encoder = require_encoder();
  var {
    failure,
    createErrorFromCode,
    failIfVersionNotSupported,
    errorCodes
  } = require_error();
  var { KafkaJSProtocolError } = require_errors();
  var SASL_AUTHENTICATION_FAILED = 58;
  var protocolAuthError = errorCodes.find((e) => e.code === SASL_AUTHENTICATION_FAILED);
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const errorCode = decoder.readInt16();
    failIfVersionNotSupported(errorCode);
    const errorMessage = decoder.readString();
    const authBytesEncoder = new Encoder().writeBytes(decoder.readBytes());
    const authBytes = authBytesEncoder.buffer;
    return {
      errorCode,
      errorMessage,
      authBytes
    };
  };
  var parse = async (data) => {
    if (data.errorCode === SASL_AUTHENTICATION_FAILED && data.errorMessage) {
      throw new KafkaJSProtocolError({
        ...protocolAuthError,
        message: data.errorMessage
      });
    }
    if (failure(data.errorCode)) {
      throw createErrorFromCode(data.errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/saslAuthenticate/v1/request.js
var require_request104 = __commonJS((exports, module) => {
  var requestV0 = require_request103();
  module.exports = ({ authBytes }) => Object.assign(requestV0({ authBytes }), { apiVersion: 1 });
});

// node_modules/kafkajs/src/protocol/requests/saslAuthenticate/v1/response.js
var require_response104 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var Encoder = require_encoder();
  var { parse: parseV0 } = require_response103();
  var { failIfVersionNotSupported } = require_error();
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const errorCode = decoder.readInt16();
    failIfVersionNotSupported(errorCode);
    const errorMessage = decoder.readString();
    const authBytesEncoder = new Encoder().writeBytes(decoder.readBytes());
    const authBytes = authBytesEncoder.buffer;
    const sessionLifetimeMs = decoder.readInt64().toString();
    return {
      errorCode,
      errorMessage,
      authBytes,
      sessionLifetimeMs
    };
  };
  module.exports = {
    decode,
    parse: parseV0
  };
});

// node_modules/kafkajs/src/protocol/requests/saslAuthenticate/index.js
var require_saslAuthenticate = __commonJS((exports, module) => {
  var versions = {
    0: ({ authBytes }) => {
      const request3 = require_request103();
      const response = require_response103();
      return { request: request3({ authBytes }), response };
    },
    1: ({ authBytes }) => {
      const request3 = require_request104();
      const response = require_response104();
      return { request: request3({ authBytes }), response };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/createPartitions/v0/request.js
var require_request105 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { CreatePartitions: apiKey } = require_apiKeys();
  module.exports = ({ topicPartitions, validateOnly = false, timeout = 5000 }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "CreatePartitions",
    encode: async () => {
      return new Encoder().writeArray(topicPartitions.map(encodeTopicPartitions)).writeInt32(timeout).writeBoolean(validateOnly);
    }
  });
  var encodeTopicPartitions = ({ topic, count, assignments = [] }) => {
    return new Encoder().writeString(topic).writeInt32(count).writeNullableArray(assignments.map(encodeAssignments));
  };
  var encodeAssignments = (brokerIds) => {
    return new Encoder().writeNullableArray(brokerIds);
  };
});

// node_modules/kafkajs/src/protocol/requests/createPartitions/v0/response.js
var require_response105 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode } = require_error();
  var topicNameComparator = (a, b) => a.topic.localeCompare(b.topic);
  var topicErrors = (decoder) => ({
    topic: decoder.readString(),
    errorCode: decoder.readInt16(),
    errorMessage: decoder.readString()
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTime = decoder.readInt32();
    return {
      throttleTime,
      topicErrors: decoder.readArray(topicErrors).sort(topicNameComparator)
    };
  };
  var parse = async (data) => {
    const topicsWithError = data.topicErrors.filter(({ errorCode }) => failure(errorCode));
    if (topicsWithError.length > 0) {
      throw createErrorFromCode(topicsWithError[0].errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/createPartitions/v1/request.js
var require_request106 = __commonJS((exports, module) => {
  var requestV0 = require_request105();
  module.exports = ({ topicPartitions, validateOnly, timeout }) => Object.assign(requestV0({ topicPartitions, validateOnly, timeout }), { apiVersion: 1 });
});

// node_modules/kafkajs/src/protocol/requests/createPartitions/v1/response.js
var require_response106 = __commonJS((exports, module) => {
  var { parse, decode: decodeV0 } = require_response105();
  var decode = async (rawData) => {
    const decoded = await decodeV0(rawData);
    return {
      ...decoded,
      throttleTime: 0,
      clientSideThrottleTime: decoded.throttleTime
    };
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/createPartitions/index.js
var require_createPartitions = __commonJS((exports, module) => {
  var versions = {
    0: ({ topicPartitions, timeout, validateOnly }) => {
      const request3 = require_request105();
      const response = require_response105();
      return { request: request3({ topicPartitions, timeout, validateOnly }), response };
    },
    1: ({ topicPartitions, validateOnly, timeout }) => {
      const request3 = require_request106();
      const response = require_response106();
      return { request: request3({ topicPartitions, validateOnly, timeout }), response };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/deleteGroups/v0/request.js
var require_request107 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { DeleteGroups: apiKey } = require_apiKeys();
  module.exports = (groupIds) => ({
    apiKey,
    apiVersion: 0,
    apiName: "DeleteGroups",
    encode: async () => {
      return new Encoder().writeArray(groupIds.map(encodeGroups));
    }
  });
  var encodeGroups = (group) => {
    return new Encoder().writeString(group);
  };
});

// node_modules/kafkajs/src/protocol/requests/deleteGroups/v0/response.js
var require_response107 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode } = require_error();
  var decodeGroup = (decoder) => ({
    groupId: decoder.readString(),
    errorCode: decoder.readInt16()
  });
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    const throttleTimeMs = decoder.readInt32();
    const results = decoder.readArray(decodeGroup);
    for (const result of results) {
      if (failure(result.errorCode)) {
        result.error = createErrorFromCode(result.errorCode);
      }
    }
    return {
      throttleTimeMs,
      results
    };
  };
  var parse = async (data) => {
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/deleteGroups/v1/request.js
var require_request108 = __commonJS((exports, module) => {
  var requestV0 = require_request107();
  module.exports = (groupIds) => Object.assign(requestV0(groupIds), { apiVersion: 1 });
});

// node_modules/kafkajs/src/protocol/requests/deleteGroups/v1/response.js
var require_response108 = __commonJS((exports, module) => {
  var { parse, decode: decodeV0 } = require_response107();
  var decode = async (rawData) => {
    const decoded = await decodeV0(rawData);
    return {
      ...decoded,
      throttleTime: 0,
      clientSideThrottleTime: decoded.throttleTime
    };
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/deleteGroups/index.js
var require_deleteGroups = __commonJS((exports, module) => {
  var versions = {
    0: (groupIds) => {
      const request3 = require_request107();
      const response = require_response107();
      return { request: request3(groupIds), response };
    },
    1: (groupIds) => {
      const request3 = require_request108();
      const response = require_response108();
      return { request: request3(groupIds), response };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/alterPartitionReassignments/v0/request.js
var require_request109 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { AlterPartitionReassignments: apiKey } = require_apiKeys();
  module.exports = ({ topics, timeout = 5000 }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "AlterPartitionReassignments",
    encode: async () => {
      return new Encoder().writeUVarIntBytes().writeInt32(timeout).writeUVarIntArray(topics.map(encodeTopics)).writeUVarIntBytes();
    }
  });
  var encodeTopics = ({ topic, partitionAssignment }) => {
    return new Encoder().writeUVarIntString(topic).writeUVarIntArray(partitionAssignment.map(encodePartitionAssignment)).writeUVarIntBytes();
  };
  var encodePartitionAssignment = ({ partition, replicas }) => {
    return new Encoder().writeInt32(partition).writeUVarIntArray(replicas.map(encodeReplicas)).writeUVarIntBytes();
  };
  var encodeReplicas = (replica) => {
    return new Encoder().writeInt32(replica);
  };
});

// node_modules/kafkajs/src/protocol/requests/alterPartitionReassignments/v0/response.js
var require_response109 = __commonJS((exports, module) => {
  var {
    KafkaJSAggregateError,
    KafkaJSAlterPartitionReassignmentsError
  } = require_errors();
  var Decoder = require_decoder();
  var { failure, createErrorFromCode } = require_error();
  var decodeResponses = (decoder) => {
    const response = {
      topic: decoder.readUVarIntString(),
      partitions: decoder.readUVarIntArray(decodePartitions)
    };
    decoder.readTaggedFields();
    return response;
  };
  var decodePartitions = (decoder) => {
    const partition = {
      partition: decoder.readInt32(),
      errorCode: decoder.readInt16()
    };
    decoder.readUVarIntString();
    decoder.readTaggedFields();
    return partition;
  };
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    decoder.readTaggedFields();
    const throttleTime = decoder.readInt32();
    const errorCode = decoder.readInt16();
    decoder.readUVarIntString();
    return {
      throttleTime,
      errorCode,
      responses: decoder.readUVarIntArray(decodeResponses)
    };
  };
  var parse = async (data) => {
    if (failure(data.errorCode)) {
      throw new KafkaJSAlterPartitionReassignmentsError(createErrorFromCode(data.errorCode));
    }
    const topicPartitionsWithError = data.responses.flatMap(({ partitions, topic }) => partitions.filter((partition) => failure(partition.errorCode)).map((partition) => ({
      ...partition,
      topic
    })));
    if (topicPartitionsWithError.length > 0) {
      throw new KafkaJSAggregateError("Errors altering partition reassignments", topicPartitionsWithError.map(({ topic, partition, errorCode }) => new KafkaJSAlterPartitionReassignmentsError(createErrorFromCode(errorCode), topic, partition)));
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/alterPartitionReassignments/index.js
var require_alterPartitionReassignments = __commonJS((exports, module) => {
  var versions = {
    0: ({ topics, timeout }) => {
      const request3 = require_request109();
      const response = require_response109();
      return { request: request3({ topics, timeout }), response };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/listPartitionReassignments/v0/request.js
var require_request110 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var { ListPartitionReassignments: apiKey } = require_apiKeys();
  module.exports = ({ topics = null, timeout = 5000 }) => ({
    apiKey,
    apiVersion: 0,
    apiName: "ListPartitionReassignments",
    encode: async () => {
      return new Encoder().writeUVarIntBytes().writeInt32(timeout).writeUVarIntArray(topics === null ? topics : topics.map(encodeTopics)).writeUVarIntBytes();
    }
  });
  var encodeTopics = ({ topic, partitions }) => {
    return new Encoder().writeUVarIntString(topic).writeUVarIntArray(partitions.map(encodePartitions)).writeUVarIntBytes();
  };
  var encodePartitions = (partition) => {
    return new Encoder().writeInt32(partition);
  };
});

// node_modules/kafkajs/src/protocol/requests/listPartitionReassignments/v0/response.js
var require_response110 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var { failure, createErrorFromCode } = require_error();
  var decodeReplicas = (decoder) => {
    return decoder.readInt32();
  };
  var decodePartitions = (decoder) => {
    const partition = {
      partition: decoder.readInt32(),
      replicas: decoder.readUVarIntArray(decodeReplicas),
      addingReplicas: decoder.readUVarIntArray(decodeReplicas),
      removingReplicas: decoder.readUVarIntArray(decodeReplicas)
    };
    decoder.readTaggedFields();
    return partition;
  };
  var decodeTopics = (decoder) => {
    const topic = {
      name: decoder.readUVarIntString(),
      partitions: decoder.readUVarIntArray(decodePartitions)
    };
    decoder.readTaggedFields();
    return topic;
  };
  var decode = async (rawData) => {
    const decoder = new Decoder(rawData);
    decoder.readTaggedFields();
    const throttleTime = decoder.readInt32();
    const errorCode = decoder.readInt16();
    decoder.readUVarIntString();
    return {
      throttleTime,
      errorCode,
      topics: decoder.readUVarIntArray(decodeTopics)
    };
  };
  var parse = async (data) => {
    if (failure(data.errorCode)) {
      throw createErrorFromCode(data.errorCode);
    }
    return data;
  };
  module.exports = {
    decode,
    parse
  };
});

// node_modules/kafkajs/src/protocol/requests/listPartitionReassignments/index.js
var require_listPartitionReassignments = __commonJS((exports, module) => {
  var versions = {
    0: ({ topics, timeout }) => {
      const request3 = require_request110();
      const response = require_response110();
      return { request: request3({ topics, timeout }), response };
    }
  };
  module.exports = {
    versions: Object.keys(versions),
    protocol: ({ version }) => versions[version]
  };
});

// node_modules/kafkajs/src/protocol/requests/index.js
var require_requests = __commonJS((exports, module) => {
  var apiKeys = require_apiKeys();
  var { KafkaJSServerDoesNotSupportApiKey, KafkaJSNotImplemented } = require_errors();
  var noImplementedRequestDefinitions = {
    versions: [],
    protocol: () => {
      throw new KafkaJSNotImplemented;
    }
  };
  var requests = {
    Produce: require_produce(),
    Fetch: require_fetch(),
    ListOffsets: require_listOffsets(),
    Metadata: require_metadata(),
    LeaderAndIsr: noImplementedRequestDefinitions,
    StopReplica: noImplementedRequestDefinitions,
    UpdateMetadata: noImplementedRequestDefinitions,
    ControlledShutdown: noImplementedRequestDefinitions,
    OffsetCommit: require_offsetCommit(),
    OffsetFetch: require_offsetFetch(),
    GroupCoordinator: require_findCoordinator(),
    JoinGroup: require_joinGroup(),
    Heartbeat: require_heartbeat(),
    LeaveGroup: require_leaveGroup(),
    SyncGroup: require_syncGroup(),
    DescribeGroups: require_describeGroups(),
    ListGroups: require_listGroups(),
    SaslHandshake: require_saslHandshake(),
    ApiVersions: require_apiVersions(),
    CreateTopics: require_createTopics(),
    DeleteTopics: require_deleteTopics(),
    DeleteRecords: require_deleteRecords(),
    InitProducerId: require_initProducerId(),
    OffsetForLeaderEpoch: noImplementedRequestDefinitions,
    AddPartitionsToTxn: require_addPartitionsToTxn(),
    AddOffsetsToTxn: require_addOffsetsToTxn(),
    EndTxn: require_endTxn(),
    WriteTxnMarkers: noImplementedRequestDefinitions,
    TxnOffsetCommit: require_txnOffsetCommit(),
    DescribeAcls: require_describeAcls(),
    CreateAcls: require_createAcls(),
    DeleteAcls: require_deleteAcls(),
    DescribeConfigs: require_describeConfigs(),
    AlterConfigs: require_alterConfigs(),
    AlterReplicaLogDirs: noImplementedRequestDefinitions,
    DescribeLogDirs: noImplementedRequestDefinitions,
    SaslAuthenticate: require_saslAuthenticate(),
    CreatePartitions: require_createPartitions(),
    CreateDelegationToken: noImplementedRequestDefinitions,
    RenewDelegationToken: noImplementedRequestDefinitions,
    ExpireDelegationToken: noImplementedRequestDefinitions,
    DescribeDelegationToken: noImplementedRequestDefinitions,
    DeleteGroups: require_deleteGroups(),
    ElectLeaders: noImplementedRequestDefinitions,
    IncrementalAlterConfigs: noImplementedRequestDefinitions,
    AlterPartitionReassignments: require_alterPartitionReassignments(),
    ListPartitionReassignments: require_listPartitionReassignments()
  };
  var names = Object.keys(apiKeys);
  var keys = Object.values(apiKeys);
  var findApiName = (apiKey) => names[keys.indexOf(apiKey)];
  var lookup = (versions) => (apiKey, definition) => {
    const version = versions[apiKey];
    const availableVersions = definition.versions.map(Number);
    const bestImplementedVersion = Math.max(...availableVersions);
    if (!version || version.maxVersion == null) {
      throw new KafkaJSServerDoesNotSupportApiKey(`The Kafka server does not support the requested API version`, { apiKey, apiName: findApiName(apiKey) });
    }
    const bestSupportedVersion = Math.min(bestImplementedVersion, version.maxVersion);
    return definition.protocol({ version: bestSupportedVersion });
  };
  module.exports = {
    requests,
    lookup
  };
});

// node_modules/kafkajs/src/utils/shuffle.js
var require_shuffle = __commonJS((exports, module) => {
  module.exports = (array) => {
    if (!Array.isArray(array)) {
      throw new TypeError("'array' is not an array");
    }
    if (array.length < 2) {
      return array;
    }
    const copy = array.slice();
    for (let i = copy.length - 1;i > 0; i--) {
      const j = Math.floor(Math.random() * (i + 1));
      const temp = copy[i];
      copy[i] = copy[j];
      copy[j] = temp;
    }
    return copy;
  };
});

// node_modules/kafkajs/src/broker/index.js
var require_broker = __commonJS((exports, module) => {
  var Lock = require_lock();
  var { Types: Compression } = require_compression();
  var { requests, lookup } = require_requests();
  var { KafkaJSNonRetriableError } = require_errors();
  var apiKeys = require_apiKeys();
  var shuffle = require_shuffle();
  var PRIVATE = {
    SEND_REQUEST: Symbol("private:Broker:sendRequest")
  };
  var notInitializedLookup = () => {
    throw new Error("Broker not connected");
  };
  module.exports = class Broker {
    constructor({
      connectionPool,
      logger,
      nodeId = null,
      versions = null,
      authenticationTimeout = 1e4,
      allowAutoTopicCreation = true
    }) {
      this.connectionPool = connectionPool;
      this.nodeId = nodeId;
      this.rootLogger = logger;
      this.logger = logger.namespace("Broker");
      this.versions = versions;
      this.authenticationTimeout = authenticationTimeout;
      this.allowAutoTopicCreation = allowAutoTopicCreation;
      const lockTimeout = 2 * this.connectionPool.connectionTimeout + this.authenticationTimeout;
      this.brokerAddress = `${this.connectionPool.host}:${this.connectionPool.port}`;
      this.lock = new Lock({
        timeout: lockTimeout,
        description: `connect to broker ${this.brokerAddress}`
      });
      this.lookupRequest = notInitializedLookup;
    }
    isConnected() {
      return this.connectionPool.sasl ? this.connectionPool.isConnected() && this.connectionPool.isAuthenticated() : this.connectionPool.isConnected();
    }
    async connect() {
      await this.lock.acquire();
      try {
        if (this.isConnected()) {
          return;
        }
        const connection = await this.connectionPool.getConnection();
        if (!this.versions) {
          this.versions = await this.apiVersions();
        }
        this.connectionPool.setVersions(this.versions);
        this.lookupRequest = lookup(this.versions);
        if (connection.getSupportAuthenticationProtocol() === null) {
          let supportAuthenticationProtocol = false;
          try {
            this.lookupRequest(apiKeys.SaslAuthenticate, requests.SaslAuthenticate);
            supportAuthenticationProtocol = true;
          } catch (_) {
            supportAuthenticationProtocol = false;
          }
          this.connectionPool.setSupportAuthenticationProtocol(supportAuthenticationProtocol);
          this.logger.debug(`Verified support for SaslAuthenticate`, {
            broker: this.brokerAddress,
            supportAuthenticationProtocol
          });
        }
        await connection.authenticate();
      } finally {
        await this.lock.release();
      }
    }
    async disconnect() {
      await this.connectionPool.destroy();
    }
    async apiVersions() {
      let response;
      const availableVersions = requests.ApiVersions.versions.map(Number).sort().reverse();
      for (const candidateVersion of availableVersions) {
        try {
          const apiVersions = requests.ApiVersions.protocol({ version: candidateVersion });
          response = await this[PRIVATE.SEND_REQUEST]({
            ...apiVersions(),
            requestTimeout: this.connectionPool.connectionTimeout
          });
          break;
        } catch (e) {
          if (e.type !== "UNSUPPORTED_VERSION") {
            throw e;
          }
        }
      }
      if (!response) {
        throw new KafkaJSNonRetriableError("API Versions not supported");
      }
      return response.apiVersions.reduce((obj, version) => Object.assign(obj, {
        [version.apiKey]: {
          minVersion: version.minVersion,
          maxVersion: version.maxVersion
        }
      }), {});
    }
    async metadata(topics = []) {
      const metadata = this.lookupRequest(apiKeys.Metadata, requests.Metadata);
      const shuffledTopics = shuffle(topics);
      return await this[PRIVATE.SEND_REQUEST](metadata({ topics: shuffledTopics, allowAutoTopicCreation: this.allowAutoTopicCreation }));
    }
    async produce({
      topicData,
      transactionalId,
      producerId,
      producerEpoch,
      acks = -1,
      timeout = 30000,
      compression = Compression.None
    }) {
      const produce = this.lookupRequest(apiKeys.Produce, requests.Produce);
      return await this[PRIVATE.SEND_REQUEST](produce({
        acks,
        timeout,
        compression,
        topicData,
        transactionalId,
        producerId,
        producerEpoch
      }));
    }
    async fetch({
      replicaId,
      isolationLevel,
      maxWaitTime = 5000,
      minBytes = 1,
      maxBytes = 10485760,
      topics,
      rackId = ""
    }) {
      const fetch2 = this.lookupRequest(apiKeys.Fetch, requests.Fetch);
      const flattenedTopicPartitions = topics.reduce((topicPartitions, { topic, partitions }) => {
        partitions.forEach((partition) => {
          topicPartitions.push({ topic, partition });
        });
        return topicPartitions;
      }, []);
      const shuffledTopicPartitions = shuffle(flattenedTopicPartitions);
      const consolidatedTopicPartitions = shuffledTopicPartitions.reduce((topicPartitions, { topic, partition }) => {
        const last = topicPartitions[topicPartitions.length - 1];
        if (last != null && last.topic === topic) {
          topicPartitions[topicPartitions.length - 1].partitions.push(partition);
        } else {
          topicPartitions.push({ topic, partitions: [partition] });
        }
        return topicPartitions;
      }, []);
      return await this[PRIVATE.SEND_REQUEST](fetch2({
        replicaId,
        isolationLevel,
        maxWaitTime,
        minBytes,
        maxBytes,
        topics: consolidatedTopicPartitions,
        rackId
      }));
    }
    async heartbeat({ groupId, groupGenerationId, memberId }) {
      const heartbeat = this.lookupRequest(apiKeys.Heartbeat, requests.Heartbeat);
      return await this[PRIVATE.SEND_REQUEST](heartbeat({ groupId, groupGenerationId, memberId }));
    }
    async findGroupCoordinator({ groupId, coordinatorType }) {
      const findCoordinator = this.lookupRequest(apiKeys.GroupCoordinator, requests.GroupCoordinator);
      return await this[PRIVATE.SEND_REQUEST](findCoordinator({ groupId, coordinatorType }));
    }
    async joinGroup({
      groupId,
      sessionTimeout,
      rebalanceTimeout,
      memberId = "",
      protocolType = "consumer",
      groupProtocols
    }) {
      const joinGroup = this.lookupRequest(apiKeys.JoinGroup, requests.JoinGroup);
      const makeRequest = (assignedMemberId = memberId) => this[PRIVATE.SEND_REQUEST](joinGroup({
        groupId,
        sessionTimeout,
        rebalanceTimeout,
        memberId: assignedMemberId,
        protocolType,
        groupProtocols
      }));
      try {
        return await makeRequest();
      } catch (error) {
        if (error.name === "KafkaJSMemberIdRequired") {
          return makeRequest(error.memberId);
        }
        throw error;
      }
    }
    async leaveGroup({ groupId, memberId }) {
      const leaveGroup = this.lookupRequest(apiKeys.LeaveGroup, requests.LeaveGroup);
      return await this[PRIVATE.SEND_REQUEST](leaveGroup({ groupId, memberId }));
    }
    async syncGroup({ groupId, generationId, memberId, groupAssignment }) {
      const syncGroup = this.lookupRequest(apiKeys.SyncGroup, requests.SyncGroup);
      return await this[PRIVATE.SEND_REQUEST](syncGroup({
        groupId,
        generationId,
        memberId,
        groupAssignment
      }));
    }
    async listOffsets({ replicaId, isolationLevel, topics }) {
      const listOffsets = this.lookupRequest(apiKeys.ListOffsets, requests.ListOffsets);
      const result = await this[PRIVATE.SEND_REQUEST](listOffsets({ replicaId, isolationLevel, topics }));
      for (const response of result.responses) {
        response.partitions = response.partitions.map(({ offsets, ...partitionData }) => {
          return offsets ? { ...partitionData, offset: offsets.pop() } : partitionData;
        });
      }
      return result;
    }
    async offsetCommit({ groupId, groupGenerationId, memberId, retentionTime, topics }) {
      const offsetCommit = this.lookupRequest(apiKeys.OffsetCommit, requests.OffsetCommit);
      return await this[PRIVATE.SEND_REQUEST](offsetCommit({
        groupId,
        groupGenerationId,
        memberId,
        retentionTime,
        topics
      }));
    }
    async offsetFetch({ groupId, topics }) {
      const offsetFetch = this.lookupRequest(apiKeys.OffsetFetch, requests.OffsetFetch);
      return await this[PRIVATE.SEND_REQUEST](offsetFetch({ groupId, topics }));
    }
    async describeGroups({ groupIds }) {
      const describeGroups = this.lookupRequest(apiKeys.DescribeGroups, requests.DescribeGroups);
      return await this[PRIVATE.SEND_REQUEST](describeGroups({ groupIds }));
    }
    async createTopics({ topics, validateOnly = false, timeout = 5000 }) {
      const createTopics = this.lookupRequest(apiKeys.CreateTopics, requests.CreateTopics);
      return await this[PRIVATE.SEND_REQUEST](createTopics({ topics, validateOnly, timeout }));
    }
    async createPartitions({ topicPartitions, validateOnly = false, timeout = 5000 }) {
      const createPartitions = this.lookupRequest(apiKeys.CreatePartitions, requests.CreatePartitions);
      return await this[PRIVATE.SEND_REQUEST](createPartitions({ topicPartitions, validateOnly, timeout }));
    }
    async deleteTopics({ topics, timeout = 5000 }) {
      const deleteTopics = this.lookupRequest(apiKeys.DeleteTopics, requests.DeleteTopics);
      return await this[PRIVATE.SEND_REQUEST](deleteTopics({ topics, timeout }));
    }
    async describeConfigs({ resources, includeSynonyms = false }) {
      const describeConfigs = this.lookupRequest(apiKeys.DescribeConfigs, requests.DescribeConfigs);
      return await this[PRIVATE.SEND_REQUEST](describeConfigs({ resources, includeSynonyms }));
    }
    async alterConfigs({ resources, validateOnly = false }) {
      const alterConfigs = this.lookupRequest(apiKeys.AlterConfigs, requests.AlterConfigs);
      return await this[PRIVATE.SEND_REQUEST](alterConfigs({ resources, validateOnly }));
    }
    async initProducerId({ transactionalId, transactionTimeout }) {
      const initProducerId = this.lookupRequest(apiKeys.InitProducerId, requests.InitProducerId);
      return await this[PRIVATE.SEND_REQUEST](initProducerId({ transactionalId, transactionTimeout }));
    }
    async addPartitionsToTxn({ transactionalId, producerId, producerEpoch, topics }) {
      const addPartitionsToTxn = this.lookupRequest(apiKeys.AddPartitionsToTxn, requests.AddPartitionsToTxn);
      return await this[PRIVATE.SEND_REQUEST](addPartitionsToTxn({ transactionalId, producerId, producerEpoch, topics }));
    }
    async addOffsetsToTxn({ transactionalId, producerId, producerEpoch, groupId }) {
      const addOffsetsToTxn = this.lookupRequest(apiKeys.AddOffsetsToTxn, requests.AddOffsetsToTxn);
      return await this[PRIVATE.SEND_REQUEST](addOffsetsToTxn({ transactionalId, producerId, producerEpoch, groupId }));
    }
    async txnOffsetCommit({ transactionalId, groupId, producerId, producerEpoch, topics }) {
      const txnOffsetCommit = this.lookupRequest(apiKeys.TxnOffsetCommit, requests.TxnOffsetCommit);
      return await this[PRIVATE.SEND_REQUEST](txnOffsetCommit({ transactionalId, groupId, producerId, producerEpoch, topics }));
    }
    async endTxn({ transactionalId, producerId, producerEpoch, transactionResult }) {
      const endTxn = this.lookupRequest(apiKeys.EndTxn, requests.EndTxn);
      return await this[PRIVATE.SEND_REQUEST](endTxn({ transactionalId, producerId, producerEpoch, transactionResult }));
    }
    async listGroups() {
      const listGroups = this.lookupRequest(apiKeys.ListGroups, requests.ListGroups);
      return await this[PRIVATE.SEND_REQUEST](listGroups());
    }
    async deleteGroups(groupIds) {
      const deleteGroups = this.lookupRequest(apiKeys.DeleteGroups, requests.DeleteGroups);
      return await this[PRIVATE.SEND_REQUEST](deleteGroups(groupIds));
    }
    async deleteRecords({ topics }) {
      const deleteRecords = this.lookupRequest(apiKeys.DeleteRecords, requests.DeleteRecords);
      return await this[PRIVATE.SEND_REQUEST](deleteRecords({ topics }));
    }
    async createAcls({ acl }) {
      const createAcls = this.lookupRequest(apiKeys.CreateAcls, requests.CreateAcls);
      return await this[PRIVATE.SEND_REQUEST](createAcls({ creations: acl }));
    }
    async describeAcls({
      resourceType,
      resourceName,
      resourcePatternType,
      principal,
      host,
      operation,
      permissionType
    }) {
      const describeAcls = this.lookupRequest(apiKeys.DescribeAcls, requests.DescribeAcls);
      return await this[PRIVATE.SEND_REQUEST](describeAcls({
        resourceType,
        resourceName,
        resourcePatternType,
        principal,
        host,
        operation,
        permissionType
      }));
    }
    async deleteAcls({ filters }) {
      const deleteAcls = this.lookupRequest(apiKeys.DeleteAcls, requests.DeleteAcls);
      return await this[PRIVATE.SEND_REQUEST](deleteAcls({ filters }));
    }
    async alterPartitionReassignments({ topics, timeout }) {
      const alterPartitionReassignments = this.lookupRequest(apiKeys.AlterPartitionReassignments, requests.AlterPartitionReassignments);
      return await this[PRIVATE.SEND_REQUEST](alterPartitionReassignments({ topics, timeout }));
    }
    async listPartitionReassignments({ topics = null, timeout }) {
      const listPartitionReassignments = this.lookupRequest(apiKeys.ListPartitionReassignments, requests.ListPartitionReassignments);
      return await this[PRIVATE.SEND_REQUEST](listPartitionReassignments({ topics, timeout }));
    }
    async[PRIVATE.SEND_REQUEST](protocolRequest) {
      try {
        return await this.connectionPool.send(protocolRequest);
      } catch (e) {
        if (e.name === "KafkaJSConnectionClosedError") {
          await this.disconnect();
        }
        throw e;
      }
    }
  };
});

// node_modules/kafkajs/src/retry/defaults.test.js
var require_defaults_test = __commonJS((exports, module) => {
  module.exports = {
    maxRetryTime: 1000,
    initialRetryTime: 50,
    factor: 0.02,
    multiplier: 1.5,
    retries: 15
  };
});

// node_modules/kafkajs/src/retry/defaults.js
var require_defaults = __commonJS((exports, module) => {
  module.exports = {
    maxRetryTime: 30 * 1000,
    initialRetryTime: 300,
    factor: 0.2,
    multiplier: 2,
    retries: 5
  };
});

// node_modules/kafkajs/src/retry/index.js
var require_retry = __commonJS((exports, module) => {
  var { KafkaJSNumberOfRetriesExceeded, KafkaJSNonRetriableError } = require_errors();
  var isTestMode = false;
  var RETRY_DEFAULT = isTestMode ? require_defaults_test() : require_defaults();
  var random = (min, max) => {
    return Math.random() * (max - min) + min;
  };
  var randomFromRetryTime = (factor, retryTime) => {
    const delta = factor * retryTime;
    return Math.ceil(random(retryTime - delta, retryTime + delta));
  };
  var UNRECOVERABLE_ERRORS = ["RangeError", "ReferenceError", "SyntaxError", "TypeError"];
  var isErrorUnrecoverable = (e) => UNRECOVERABLE_ERRORS.includes(e.name);
  var isErrorRetriable = (error) => (error.retriable || error.retriable !== false) && !isErrorUnrecoverable(error);
  var createRetriable = (configs, resolve, reject, fn) => {
    let aborted = false;
    const { factor, multiplier, maxRetryTime, retries } = configs;
    const bail = (error) => {
      aborted = true;
      reject(error || new Error("Aborted"));
    };
    const calculateExponentialRetryTime = (retryTime) => {
      return Math.min(randomFromRetryTime(factor, retryTime) * multiplier, maxRetryTime);
    };
    const retry = (retryTime, retryCount = 0) => {
      if (aborted)
        return;
      const nextRetryTime = calculateExponentialRetryTime(retryTime);
      const shouldRetry = retryCount < retries;
      const scheduleRetry = () => {
        setTimeout(() => retry(nextRetryTime, retryCount + 1), retryTime);
      };
      fn(bail, retryCount, retryTime).then(resolve).catch((e) => {
        if (isErrorRetriable(e)) {
          if (shouldRetry) {
            scheduleRetry();
          } else {
            reject(new KafkaJSNumberOfRetriesExceeded(e, { retryCount, retryTime, cause: e.cause || e }));
          }
        } else {
          reject(new KafkaJSNonRetriableError(e, { cause: e.cause || e }));
        }
      });
    };
    return retry;
  };
  module.exports = (opts = {}) => (fn) => {
    return new Promise((resolve, reject) => {
      const configs = Object.assign({}, RETRY_DEFAULT, opts);
      const start = createRetriable(configs, resolve, reject, fn);
      start(randomFromRetryTime(configs.factor, configs.initialRetryTime));
    });
  };
});

// node_modules/kafkajs/src/utils/arrayDiff.js
var require_arrayDiff = __commonJS((exports, module) => {
  module.exports = (a, b) => {
    const result = [];
    const length = a.length;
    let i = 0;
    while (i < length) {
      if (b.indexOf(a[i]) === -1) {
        result.push(a[i]);
      }
      i += 1;
    }
    return result;
  };
});

// node_modules/kafkajs/src/cluster/brokerPool.js
var require_brokerPool = __commonJS((exports, module) => {
  var Broker = require_broker();
  var createRetry = require_retry();
  var shuffle = require_shuffle();
  var arrayDiff = require_arrayDiff();
  var { KafkaJSBrokerNotFound, KafkaJSProtocolError } = require_errors();
  var { keys, assign, values } = Object;
  var hasBrokerBeenReplaced = (broker, { host, port, rack }) => broker.connectionPool.host !== host || broker.connectionPool.port !== port || broker.connectionPool.rack !== rack;
  module.exports = class BrokerPool {
    constructor({
      connectionPoolBuilder,
      logger,
      retry,
      allowAutoTopicCreation,
      authenticationTimeout,
      metadataMaxAge
    }) {
      this.rootLogger = logger;
      this.connectionPoolBuilder = connectionPoolBuilder;
      this.metadataMaxAge = metadataMaxAge || 0;
      this.logger = logger.namespace("BrokerPool");
      this.retrier = createRetry(assign({}, retry));
      this.createBroker = (options) => new Broker({
        allowAutoTopicCreation,
        authenticationTimeout,
        ...options
      });
      this.brokers = {};
      this.seedBroker = undefined;
      this.metadata = null;
      this.metadataExpireAt = null;
      this.versions = null;
    }
    hasConnectedBrokers() {
      const brokers = values(this.brokers);
      return !!brokers.find((broker) => broker.isConnected()) || (this.seedBroker ? this.seedBroker.isConnected() : false);
    }
    async createSeedBroker() {
      if (this.seedBroker) {
        await this.seedBroker.disconnect();
      }
      const connectionPool = await this.connectionPoolBuilder.build();
      this.seedBroker = this.createBroker({
        connectionPool,
        logger: this.rootLogger
      });
    }
    async connect() {
      if (this.hasConnectedBrokers()) {
        return;
      }
      if (!this.seedBroker) {
        await this.createSeedBroker();
      }
      return this.retrier(async (bail, retryCount, retryTime) => {
        try {
          await this.seedBroker.connect();
          this.versions = this.seedBroker.versions;
        } catch (e) {
          if (e.name === "KafkaJSConnectionError" || e.type === "ILLEGAL_SASL_STATE") {
            await this.createSeedBroker();
            this.logger.error(`Failed to connect to seed broker, trying another broker from the list: ${e.message}`, { retryCount, retryTime });
          } else {
            this.logger.error(e.message, { retryCount, retryTime });
          }
          if (e.retriable)
            throw e;
          bail(e);
        }
      });
    }
    async disconnect() {
      this.seedBroker && await this.seedBroker.disconnect();
      await Promise.all(values(this.brokers).map((broker) => broker.disconnect()));
      this.brokers = {};
      this.metadata = null;
      this.versions = null;
    }
    removeBroker({ host, port }) {
      const removedBroker = values(this.brokers).find((broker) => broker.connectionPool.host === host && broker.connectionPool.port === port);
      if (removedBroker) {
        delete this.brokers[removedBroker.nodeId];
        this.metadataExpireAt = null;
        if (this.seedBroker.nodeId === removedBroker.nodeId) {
          this.seedBroker = shuffle(values(this.brokers))[0];
        }
      }
    }
    async refreshMetadata(topics) {
      const broker = await this.findConnectedBroker();
      const { host: seedHost, port: seedPort } = this.seedBroker.connectionPool;
      return this.retrier(async (bail, retryCount, retryTime) => {
        try {
          this.metadata = await broker.metadata(topics);
          this.metadataExpireAt = Date.now() + this.metadataMaxAge;
          const replacedBrokers = [];
          this.brokers = await this.metadata.brokers.reduce(async (resultPromise, { nodeId, host, port, rack }) => {
            const result = await resultPromise;
            if (result[nodeId]) {
              if (!hasBrokerBeenReplaced(result[nodeId], { host, port, rack })) {
                return result;
              }
              replacedBrokers.push(result[nodeId]);
            }
            if (host === seedHost && port === seedPort) {
              this.seedBroker.nodeId = nodeId;
              this.seedBroker.connectionPool.rack = rack;
              return assign(result, {
                [nodeId]: this.seedBroker
              });
            }
            return assign(result, {
              [nodeId]: this.createBroker({
                logger: this.rootLogger,
                versions: this.versions,
                connectionPool: await this.connectionPoolBuilder.build({ host, port, rack }),
                nodeId
              })
            });
          }, this.brokers);
          const freshBrokerIds = this.metadata.brokers.map(({ nodeId }) => `${nodeId}`).sort();
          const currentBrokerIds = keys(this.brokers).sort();
          const unusedBrokerIds = arrayDiff(currentBrokerIds, freshBrokerIds);
          const brokerDisconnects = unusedBrokerIds.map((nodeId) => {
            const broker2 = this.brokers[nodeId];
            return broker2.disconnect().then(() => {
              delete this.brokers[nodeId];
            });
          });
          const replacedBrokersDisconnects = replacedBrokers.map((broker2) => broker2.disconnect());
          await Promise.all([...brokerDisconnects, ...replacedBrokersDisconnects]);
        } catch (e) {
          if (e.type === "LEADER_NOT_AVAILABLE") {
            throw e;
          }
          bail(e);
        }
      });
    }
    async refreshMetadataIfNecessary(topics) {
      const shouldRefresh = this.metadata == null || this.metadataExpireAt == null || Date.now() > this.metadataExpireAt || !topics.every((topic) => this.metadata.topicMetadata.some((topicMetadata) => topicMetadata.topic === topic));
      if (shouldRefresh) {
        return this.refreshMetadata(topics);
      }
    }
    getNodeIds() {
      return keys(this.brokers);
    }
    async findBroker({ nodeId }) {
      const broker = this.brokers[nodeId];
      if (!broker) {
        throw new KafkaJSBrokerNotFound(`Broker ${nodeId} not found in the cached metadata`);
      }
      await this.connectBroker(broker);
      return broker;
    }
    async withBroker(callback) {
      const brokers = shuffle(keys(this.brokers));
      if (brokers.length === 0) {
        throw new KafkaJSBrokerNotFound("No brokers in the broker pool");
      }
      for (const nodeId of brokers) {
        const broker = await this.findBroker({ nodeId });
        try {
          return await callback({ nodeId, broker });
        } catch (e) {
        }
      }
      return null;
    }
    async findConnectedBroker() {
      const nodeIds = shuffle(keys(this.brokers));
      const connectedBrokerId = nodeIds.find((nodeId) => this.brokers[nodeId].isConnected());
      if (connectedBrokerId) {
        return await this.findBroker({ nodeId: connectedBrokerId });
      }
      for (const nodeId of nodeIds) {
        try {
          return await this.findBroker({ nodeId });
        } catch (e) {
        }
      }
      await this.connect();
      return this.seedBroker;
    }
    async connectBroker(broker) {
      if (broker.isConnected()) {
        return;
      }
      return this.retrier(async (bail, retryCount, retryTime) => {
        try {
          await broker.connect();
        } catch (e) {
          if (e.name === "KafkaJSConnectionError" || e.type === "ILLEGAL_SASL_STATE") {
            await broker.disconnect();
          }
          if (e.name === "KafkaJSConnectionError") {
            return bail(e);
          }
          if (e.type === "ILLEGAL_SASL_STATE") {
            broker.connectionPool = await this.connectionPoolBuilder.build({
              host: broker.connectionPool.host,
              port: broker.connectionPool.port,
              rack: broker.connectionPool.rack
            });
            this.logger.error(`Failed to connect to broker, reconnecting`, { retryCount, retryTime });
            throw new KafkaJSProtocolError(e, { retriable: true });
          }
          if (e.retriable)
            throw e;
          this.logger.error(e, { retryCount, retryTime, stack: e.stack });
          bail(e);
        }
      });
    }
  };
});

// node_modules/kafkajs/src/utils/sharedPromiseTo.js
var require_sharedPromiseTo = __commonJS((exports, module) => {
  module.exports = (asyncFunction) => {
    let promise = null;
    return (...args) => {
      if (promise == null) {
        promise = asyncFunction(...args).finally(() => promise = null);
      }
      return promise;
    };
  };
});

// node_modules/kafkajs/src/network/socket.js
var require_socket = __commonJS((exports, module) => {
  module.exports = ({
    socketFactory,
    host,
    port,
    ssl,
    onConnect,
    onData,
    onEnd,
    onError,
    onTimeout
  }) => {
    const socket = socketFactory({ host, port, ssl, onConnect });
    socket.on("data", onData);
    socket.on("end", onEnd);
    socket.on("error", onError);
    socket.on("timeout", onTimeout);
    return socket;
  };
});

// node_modules/kafkajs/src/protocol/request.js
var require_request111 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  module.exports = async ({ correlationId, clientId, request: { apiKey, apiVersion, encode } }) => {
    const payload = await encode();
    const requestPayload = new Encoder().writeInt16(apiKey).writeInt16(apiVersion).writeInt32(correlationId).writeString(clientId).writeEncoder(payload);
    return new Encoder().writeInt32(requestPayload.size()).writeEncoder(requestPayload);
  };
});

// node_modules/kafkajs/src/constants.js
var require_constants2 = __commonJS((exports, module) => {
  var EARLIEST_OFFSET = -2;
  var LATEST_OFFSET = -1;
  var INT_32_MAX_VALUE = Math.pow(2, 31) - 1;
  module.exports = {
    EARLIEST_OFFSET,
    LATEST_OFFSET,
    INT_32_MAX_VALUE
  };
});

// node_modules/kafkajs/src/env.js
var require_env = __commonJS((exports, module) => {
  module.exports = () => ({
    KAFKAJS_DEBUG_PROTOCOL_BUFFERS: process.env.KAFKAJS_DEBUG_PROTOCOL_BUFFERS,
    KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS: process.env.KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS
  });
});

// node_modules/kafkajs/src/instrumentation/eventType.js
var require_eventType = __commonJS((exports, module) => {
  module.exports = (namespace) => (type) => `${namespace}.${type}`;
});

// node_modules/kafkajs/src/network/instrumentationEvents.js
var require_instrumentationEvents = __commonJS((exports, module) => {
  var InstrumentationEventType = require_eventType();
  var eventType = InstrumentationEventType("network");
  module.exports = {
    NETWORK_REQUEST: eventType("request"),
    NETWORK_REQUEST_TIMEOUT: eventType("request_timeout"),
    NETWORK_REQUEST_QUEUE_SIZE: eventType("request_queue_size")
  };
});

// node_modules/kafkajs/src/network/requestQueue/socketRequest.js
var require_socketRequest = __commonJS((exports, module) => {
  var { KafkaJSRequestTimeoutError, KafkaJSNonRetriableError } = require_errors();
  var events = require_instrumentationEvents();
  var PRIVATE = {
    STATE: Symbol("private:SocketRequest:state"),
    EMIT_EVENT: Symbol("private:SocketRequest:emitEvent")
  };
  var REQUEST_STATE = {
    PENDING: Symbol("PENDING"),
    SENT: Symbol("SENT"),
    COMPLETED: Symbol("COMPLETED"),
    REJECTED: Symbol("REJECTED")
  };
  module.exports = class SocketRequest {
    constructor({
      requestTimeout,
      broker,
      clientId,
      entry,
      expectResponse,
      send,
      timeout,
      instrumentationEmitter = null
    }) {
      this.createdAt = Date.now();
      this.requestTimeout = requestTimeout;
      this.broker = broker;
      this.clientId = clientId;
      this.entry = entry;
      this.correlationId = entry.correlationId;
      this.expectResponse = expectResponse;
      this.sendRequest = send;
      this.timeoutHandler = timeout;
      this.sentAt = null;
      this.duration = null;
      this.pendingDuration = null;
      this[PRIVATE.STATE] = REQUEST_STATE.PENDING;
      this[PRIVATE.EMIT_EVENT] = (eventName, payload) => instrumentationEmitter && instrumentationEmitter.emit(eventName, payload);
    }
    send() {
      this.throwIfInvalidState({
        accepted: [REQUEST_STATE.PENDING],
        next: REQUEST_STATE.SENT
      });
      this.sendRequest();
      this.sentAt = Date.now();
      this.pendingDuration = this.sentAt - this.createdAt;
      this[PRIVATE.STATE] = REQUEST_STATE.SENT;
    }
    timeoutRequest() {
      const { apiName, apiKey, apiVersion } = this.entry;
      const requestInfo = `${apiName}(key: ${apiKey}, version: ${apiVersion})`;
      const eventData = {
        broker: this.broker,
        clientId: this.clientId,
        correlationId: this.correlationId,
        createdAt: this.createdAt,
        sentAt: this.sentAt,
        pendingDuration: this.pendingDuration
      };
      this.timeoutHandler();
      this.rejected(new KafkaJSRequestTimeoutError(`Request ${requestInfo} timed out`, eventData));
      this[PRIVATE.EMIT_EVENT](events.NETWORK_REQUEST_TIMEOUT, {
        ...eventData,
        apiName,
        apiKey,
        apiVersion
      });
    }
    completed({ size, payload }) {
      this.throwIfInvalidState({
        accepted: [REQUEST_STATE.SENT],
        next: REQUEST_STATE.COMPLETED
      });
      const { entry, correlationId, broker, clientId, createdAt, sentAt, pendingDuration } = this;
      this[PRIVATE.STATE] = REQUEST_STATE.COMPLETED;
      this.duration = Date.now() - this.sentAt;
      entry.resolve({ correlationId, entry, size, payload });
      this[PRIVATE.EMIT_EVENT](events.NETWORK_REQUEST, {
        broker,
        clientId,
        correlationId,
        size,
        createdAt,
        sentAt,
        pendingDuration,
        duration: this.duration,
        apiName: entry.apiName,
        apiKey: entry.apiKey,
        apiVersion: entry.apiVersion
      });
    }
    rejected(error) {
      this.throwIfInvalidState({
        accepted: [REQUEST_STATE.PENDING, REQUEST_STATE.SENT],
        next: REQUEST_STATE.REJECTED
      });
      this[PRIVATE.STATE] = REQUEST_STATE.REJECTED;
      this.duration = Date.now() - this.sentAt;
      this.entry.reject(error);
    }
    throwIfInvalidState({ accepted, next }) {
      if (accepted.includes(this[PRIVATE.STATE])) {
        return;
      }
      const current = this[PRIVATE.STATE].toString();
      throw new KafkaJSNonRetriableError(`Invalid state, can't transition from ${current} to ${next.toString()}`);
    }
  };
});

// node_modules/kafkajs/src/network/requestQueue/index.js
var require_requestQueue = __commonJS((exports, module) => {
  var { EventEmitter } = import.meta.require("events");
  var SocketRequest = require_socketRequest();
  var events = require_instrumentationEvents();
  var { KafkaJSInvariantViolation } = require_errors();
  var PRIVATE = {
    EMIT_QUEUE_SIZE_EVENT: Symbol("private:RequestQueue:emitQueueSizeEvent"),
    EMIT_REQUEST_QUEUE_EMPTY: Symbol("private:RequestQueue:emitQueueEmpty")
  };
  var REQUEST_QUEUE_EMPTY = "requestQueueEmpty";
  var CHECK_PENDING_REQUESTS_INTERVAL = 10;
  module.exports = class RequestQueue extends EventEmitter {
    constructor({
      instrumentationEmitter = null,
      maxInFlightRequests,
      requestTimeout,
      enforceRequestTimeout,
      clientId,
      broker,
      logger,
      isConnected = () => true
    }) {
      super();
      this.instrumentationEmitter = instrumentationEmitter;
      this.maxInFlightRequests = maxInFlightRequests;
      this.requestTimeout = requestTimeout;
      this.enforceRequestTimeout = enforceRequestTimeout;
      this.clientId = clientId;
      this.broker = broker;
      this.logger = logger;
      this.isConnected = isConnected;
      this.inflight = new Map;
      this.pending = [];
      this.throttledUntil = -1;
      this.throttleCheckTimeoutId = null;
      this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY] = () => {
        if (this.pending.length === 0 && this.inflight.size === 0) {
          this.emit(REQUEST_QUEUE_EMPTY);
        }
      };
      this[PRIVATE.EMIT_QUEUE_SIZE_EVENT] = () => {
        instrumentationEmitter && instrumentationEmitter.emit(events.NETWORK_REQUEST_QUEUE_SIZE, {
          broker: this.broker,
          clientId: this.clientId,
          queueSize: this.pending.length
        });
        this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY]();
      };
    }
    scheduleRequestTimeoutCheck() {
      if (this.enforceRequestTimeout) {
        this.destroy();
        this.requestTimeoutIntervalId = setInterval(() => {
          this.inflight.forEach((request3) => {
            if (Date.now() - request3.sentAt > request3.requestTimeout) {
              request3.timeoutRequest();
            }
          });
          if (!this.isConnected()) {
            this.destroy();
          }
        }, Math.min(this.requestTimeout, 100));
      }
    }
    maybeThrottle(clientSideThrottleTime) {
      if (clientSideThrottleTime !== null && clientSideThrottleTime > 0) {
        this.logger.debug(`Client side throttling in effect for ${clientSideThrottleTime}ms`);
        const minimumThrottledUntil = Date.now() + clientSideThrottleTime;
        this.throttledUntil = Math.max(minimumThrottledUntil, this.throttledUntil);
      }
    }
    createSocketRequest(pushedRequest) {
      const { correlationId } = pushedRequest.entry;
      const defaultRequestTimeout = this.requestTimeout;
      const customRequestTimeout = pushedRequest.requestTimeout;
      const requestTimeout = Math.max(defaultRequestTimeout, customRequestTimeout || 0);
      const socketRequest = new SocketRequest({
        entry: pushedRequest.entry,
        expectResponse: pushedRequest.expectResponse,
        broker: this.broker,
        clientId: this.clientId,
        instrumentationEmitter: this.instrumentationEmitter,
        requestTimeout,
        send: () => {
          if (this.inflight.has(correlationId)) {
            throw new KafkaJSInvariantViolation("Correlation id already exists");
          }
          this.inflight.set(correlationId, socketRequest);
          pushedRequest.sendRequest();
        },
        timeout: () => {
          this.inflight.delete(correlationId);
          this.checkPendingRequests();
          this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY]();
        }
      });
      return socketRequest;
    }
    push(pushedRequest) {
      const { correlationId } = pushedRequest.entry;
      const socketRequest = this.createSocketRequest(pushedRequest);
      if (this.canSendSocketRequestImmediately()) {
        this.sendSocketRequest(socketRequest);
        return;
      }
      this.pending.push(socketRequest);
      this.scheduleCheckPendingRequests();
      this.logger.debug(`Request enqueued`, {
        clientId: this.clientId,
        broker: this.broker,
        correlationId
      });
      this[PRIVATE.EMIT_QUEUE_SIZE_EVENT]();
    }
    sendSocketRequest(socketRequest) {
      socketRequest.send();
      if (!socketRequest.expectResponse) {
        this.logger.debug(`Request does not expect a response, resolving immediately`, {
          clientId: this.clientId,
          broker: this.broker,
          correlationId: socketRequest.correlationId
        });
        this.inflight.delete(socketRequest.correlationId);
        socketRequest.completed({ size: 0, payload: null });
      }
    }
    fulfillRequest({ correlationId, payload, size }) {
      const socketRequest = this.inflight.get(correlationId);
      this.inflight.delete(correlationId);
      this.checkPendingRequests();
      if (socketRequest) {
        socketRequest.completed({ size, payload });
      } else {
        this.logger.warn(`Response without match`, {
          clientId: this.clientId,
          broker: this.broker,
          correlationId
        });
      }
      this[PRIVATE.EMIT_REQUEST_QUEUE_EMPTY]();
    }
    rejectAll(error) {
      const requests = [...this.inflight.values(), ...this.pending];
      for (const socketRequest of requests) {
        socketRequest.rejected(error);
        this.inflight.delete(socketRequest.correlationId);
      }
      this.pending = [];
      this.inflight.clear();
      this[PRIVATE.EMIT_QUEUE_SIZE_EVENT]();
    }
    waitForPendingRequests() {
      return new Promise((resolve) => {
        if (this.pending.length === 0 && this.inflight.size === 0) {
          return resolve();
        }
        this.logger.debug("Waiting for pending requests", {
          clientId: this.clientId,
          broker: this.broker,
          currentInflightRequests: this.inflight.size,
          currentPendingQueueSize: this.pending.length
        });
        this.once(REQUEST_QUEUE_EMPTY, () => resolve());
      });
    }
    destroy() {
      clearInterval(this.requestTimeoutIntervalId);
      clearTimeout(this.throttleCheckTimeoutId);
      this.throttleCheckTimeoutId = null;
    }
    canSendSocketRequestImmediately() {
      const shouldEnqueue = this.maxInFlightRequests != null && this.inflight.size >= this.maxInFlightRequests || this.throttledUntil > Date.now();
      return !shouldEnqueue;
    }
    checkPendingRequests() {
      while (this.pending.length > 0 && this.canSendSocketRequestImmediately()) {
        const pendingRequest = this.pending.shift();
        this.sendSocketRequest(pendingRequest);
        this.logger.debug(`Consumed pending request`, {
          clientId: this.clientId,
          broker: this.broker,
          correlationId: pendingRequest.correlationId,
          pendingDuration: pendingRequest.pendingDuration,
          currentPendingQueueSize: this.pending.length
        });
        this[PRIVATE.EMIT_QUEUE_SIZE_EVENT]();
      }
      this.scheduleCheckPendingRequests();
    }
    scheduleCheckPendingRequests() {
      let scheduleAt = this.throttledUntil - Date.now();
      if (!this.throttleCheckTimeoutId) {
        if (this.pending.length > 0) {
          scheduleAt = scheduleAt > 0 ? scheduleAt : CHECK_PENDING_REQUESTS_INTERVAL;
        }
        this.throttleCheckTimeoutId = setTimeout(() => {
          this.throttleCheckTimeoutId = null;
          this.checkPendingRequests();
        }, scheduleAt);
      }
    }
  };
});

// node_modules/kafkajs/src/network/connectionStatus.js
var require_connectionStatus = __commonJS((exports, module) => {
  var CONNECTION_STATUS = {
    CONNECTED: "connected",
    DISCONNECTING: "disconnecting",
    DISCONNECTED: "disconnected"
  };
  var CONNECTED_STATUS = [CONNECTION_STATUS.CONNECTED, CONNECTION_STATUS.DISCONNECTING];
  module.exports = {
    CONNECTION_STATUS,
    CONNECTED_STATUS
  };
});

// node_modules/kafkajs/src/protocol/sasl/plain/request.js
var require_request112 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var US_ASCII_NULL_CHAR = "\0";
  module.exports = ({ authorizationIdentity = null, username, password }) => ({
    encode: async () => {
      return new Encoder().writeBytes([authorizationIdentity, username, password].join(US_ASCII_NULL_CHAR)).buffer;
    }
  });
});

// node_modules/kafkajs/src/protocol/sasl/plain/response.js
var require_response111 = __commonJS((exports, module) => {
  module.exports = {
    decode: async () => true,
    parse: async () => true
  };
});

// node_modules/kafkajs/src/protocol/sasl/plain/index.js
var require_plain = __commonJS((exports, module) => {
  module.exports = {
    request: require_request112(),
    response: require_response111()
  };
});

// node_modules/kafkajs/src/broker/saslAuthenticator/plain.js
var require_plain2 = __commonJS((exports, module) => {
  var { request: request3, response } = require_plain();
  var { KafkaJSSASLAuthenticationError } = require_errors();
  var plainAuthenticatorProvider = (sasl) => ({ host, port, logger, saslAuthenticate }) => {
    return {
      authenticate: async () => {
        if (sasl.username == null || sasl.password == null) {
          throw new KafkaJSSASLAuthenticationError("SASL Plain: Invalid username or password");
        }
        const broker = `${host}:${port}`;
        try {
          logger.debug("Authenticate with SASL PLAIN", { broker });
          await saslAuthenticate({ request: request3(sasl), response });
          logger.debug("SASL PLAIN authentication successful", { broker });
        } catch (e) {
          const error = new KafkaJSSASLAuthenticationError(`SASL PLAIN authentication failed: ${e.message}`);
          logger.error(error.message, { broker });
          throw error;
        }
      }
    };
  };
  module.exports = plainAuthenticatorProvider;
});

// node_modules/kafkajs/src/protocol/sasl/scram/firstMessage/request.js
var require_request113 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  module.exports = ({ clientFirstMessage }) => ({
    encode: async () => new Encoder().writeBytes(clientFirstMessage).buffer
  });
});

// node_modules/kafkajs/src/protocol/sasl/scram/firstMessage/response.js
var require_response112 = __commonJS((exports, module) => {
  var Decoder = require_decoder();
  var ENTRY_REGEX = /^([rsiev])=(.*)$/;
  module.exports = {
    decode: async (rawData) => {
      return new Decoder(rawData).readBytes();
    },
    parse: async (data) => {
      const processed = data.toString().split(",").map((str) => {
        const [_, key, value] = str.match(ENTRY_REGEX);
        return [key, value];
      }).reduce((obj, entry) => ({ ...obj, [entry[0]]: entry[1] }), {});
      return { original: data.toString(), ...processed };
    }
  };
});

// node_modules/kafkajs/src/protocol/sasl/scram/finalMessage/request.js
var require_request114 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  module.exports = ({ finalMessage }) => ({
    encode: async () => new Encoder().writeBytes(finalMessage).buffer
  });
});

// node_modules/kafkajs/src/protocol/sasl/scram/index.js
var require_scram = __commonJS((exports, module) => {
  module.exports = {
    firstMessage: {
      request: require_request113(),
      response: require_response112()
    },
    finalMessage: {
      request: require_request114(),
      response: require_response112()
    }
  };
});

// node_modules/kafkajs/src/broker/saslAuthenticator/scram.js
var require_scram2 = __commonJS((exports, module) => {
  var crypto2 = import.meta.require("crypto");
  var scram = require_scram();
  var { KafkaJSSASLAuthenticationError, KafkaJSNonRetriableError } = require_errors();
  var GS2_HEADER = "n,,";
  var EQUAL_SIGN_REGEX = /=/g;
  var COMMA_SIGN_REGEX = /,/g;
  var URLSAFE_BASE64_PLUS_REGEX = /\+/g;
  var URLSAFE_BASE64_SLASH_REGEX = /\//g;
  var URLSAFE_BASE64_TRAILING_EQUAL_REGEX = /=+$/;
  var HMAC_CLIENT_KEY = "Client Key";
  var HMAC_SERVER_KEY = "Server Key";
  var DIGESTS = {
    SHA256: {
      length: 32,
      type: "sha256",
      minIterations: 4096
    },
    SHA512: {
      length: 64,
      type: "sha512",
      minIterations: 4096
    }
  };
  var encode64 = (str) => Buffer.from(str).toString("base64");

  class SCRAM {
    static sanitizeString(str) {
      return str.replace(EQUAL_SIGN_REGEX, "=3D").replace(COMMA_SIGN_REGEX, "=2C");
    }
    static nonce() {
      return crypto2.randomBytes(16).toString("base64").replace(URLSAFE_BASE64_PLUS_REGEX, "-").replace(URLSAFE_BASE64_SLASH_REGEX, "_").replace(URLSAFE_BASE64_TRAILING_EQUAL_REGEX, "").toString("ascii");
    }
    static hi(password, salt, iterations, digestDefinition) {
      return new Promise((resolve, reject) => {
        crypto2.pbkdf2(password, salt, iterations, digestDefinition.length, digestDefinition.type, (err, derivedKey) => err ? reject(err) : resolve(derivedKey));
      });
    }
    static xor(left, right) {
      const bufferA = Buffer.from(left);
      const bufferB = Buffer.from(right);
      const length = Buffer.byteLength(bufferA);
      if (length !== Buffer.byteLength(bufferB)) {
        throw new KafkaJSNonRetriableError("Buffers must be of the same length");
      }
      const result = [];
      for (let i = 0;i < length; i++) {
        result.push(bufferA[i] ^ bufferB[i]);
      }
      return Buffer.from(result);
    }
    constructor(sasl, host, port, logger, saslAuthenticate, digestDefinition) {
      this.sasl = sasl;
      this.host = host;
      this.port = port;
      this.logger = logger;
      this.saslAuthenticate = saslAuthenticate;
      this.digestDefinition = digestDefinition;
      const digestType = digestDefinition.type.toUpperCase();
      this.PREFIX = `SASL SCRAM ${digestType} authentication`;
      this.currentNonce = SCRAM.nonce();
    }
    async authenticate() {
      const { PREFIX } = this;
      const broker = `${this.host}:${this.port}`;
      if (this.sasl.username == null || this.sasl.password == null) {
        throw new KafkaJSSASLAuthenticationError(`${this.PREFIX}: Invalid username or password`);
      }
      try {
        this.logger.debug("Exchanging first client message", { broker });
        const clientMessageResponse = await this.sendClientFirstMessage();
        this.logger.debug("Sending final message", { broker });
        const finalResponse = await this.sendClientFinalMessage(clientMessageResponse);
        if (finalResponse.e) {
          throw new Error(finalResponse.e);
        }
        const serverKey = await this.serverKey(clientMessageResponse);
        const serverSignature = this.serverSignature(serverKey, clientMessageResponse);
        if (finalResponse.v !== serverSignature) {
          throw new Error("Invalid server signature in server final message");
        }
        this.logger.debug(`${PREFIX} successful`, { broker });
      } catch (e) {
        const error = new KafkaJSSASLAuthenticationError(`${PREFIX} failed: ${e.message}`);
        this.logger.error(error.message, { broker });
        throw error;
      }
    }
    async sendClientFirstMessage() {
      const clientFirstMessage = `${GS2_HEADER}${this.firstMessageBare()}`;
      const request3 = scram.firstMessage.request({ clientFirstMessage });
      const response = scram.firstMessage.response;
      return this.saslAuthenticate({
        request: request3,
        response
      });
    }
    async sendClientFinalMessage(clientMessageResponse) {
      const { PREFIX } = this;
      const iterations = parseInt(clientMessageResponse.i, 10);
      const { minIterations } = this.digestDefinition;
      if (!clientMessageResponse.r.startsWith(this.currentNonce)) {
        throw new KafkaJSSASLAuthenticationError(`${PREFIX} failed: Invalid server nonce, it does not start with the client nonce`);
      }
      if (iterations < minIterations) {
        throw new KafkaJSSASLAuthenticationError(`${PREFIX} failed: Requested iterations ${iterations} is less than the minimum ${minIterations}`);
      }
      const finalMessageWithoutProof = this.finalMessageWithoutProof(clientMessageResponse);
      const clientProof = await this.clientProof(clientMessageResponse);
      const finalMessage = `${finalMessageWithoutProof},p=${clientProof}`;
      const request3 = scram.finalMessage.request({ finalMessage });
      const response = scram.finalMessage.response;
      return this.saslAuthenticate({
        request: request3,
        response
      });
    }
    async clientProof(clientMessageResponse) {
      const clientKey = await this.clientKey(clientMessageResponse);
      const storedKey = this.H(clientKey);
      const clientSignature = this.clientSignature(storedKey, clientMessageResponse);
      return encode64(SCRAM.xor(clientKey, clientSignature));
    }
    async clientKey(clientMessageResponse) {
      const saltedPassword = await this.saltPassword(clientMessageResponse);
      return this.HMAC(saltedPassword, HMAC_CLIENT_KEY);
    }
    async serverKey(clientMessageResponse) {
      const saltedPassword = await this.saltPassword(clientMessageResponse);
      return this.HMAC(saltedPassword, HMAC_SERVER_KEY);
    }
    clientSignature(storedKey, clientMessageResponse) {
      return this.HMAC(storedKey, this.authMessage(clientMessageResponse));
    }
    serverSignature(serverKey, clientMessageResponse) {
      return encode64(this.HMAC(serverKey, this.authMessage(clientMessageResponse)));
    }
    authMessage(clientMessageResponse) {
      return [
        this.firstMessageBare(),
        clientMessageResponse.original,
        this.finalMessageWithoutProof(clientMessageResponse)
      ].join(",");
    }
    async saltPassword(clientMessageResponse) {
      const salt = Buffer.from(clientMessageResponse.s, "base64");
      const iterations = parseInt(clientMessageResponse.i, 10);
      return SCRAM.hi(this.encodedPassword(), salt, iterations, this.digestDefinition);
    }
    firstMessageBare() {
      return `n=${this.encodedUsername()},r=${this.currentNonce}`;
    }
    finalMessageWithoutProof(clientMessageResponse) {
      const rnonce = clientMessageResponse.r;
      return `c=${encode64(GS2_HEADER)},r=${rnonce}`;
    }
    encodedUsername() {
      const { username } = this.sasl;
      return SCRAM.sanitizeString(username).toString("utf-8");
    }
    encodedPassword() {
      const { password } = this.sasl;
      return password.toString("utf-8");
    }
    H(data) {
      return crypto2.createHash(this.digestDefinition.type).update(data).digest();
    }
    HMAC(key, data) {
      return crypto2.createHmac(this.digestDefinition.type, key).update(data).digest();
    }
  }
  module.exports = {
    DIGESTS,
    SCRAM
  };
});

// node_modules/kafkajs/src/broker/saslAuthenticator/scram256.js
var require_scram256 = __commonJS((exports, module) => {
  var { SCRAM, DIGESTS } = require_scram2();
  var scram256AuthenticatorProvider = (sasl) => ({ host, port, logger, saslAuthenticate }) => {
    const scram = new SCRAM(sasl, host, port, logger, saslAuthenticate, DIGESTS.SHA256);
    return {
      authenticate: async () => await scram.authenticate()
    };
  };
  module.exports = scram256AuthenticatorProvider;
});

// node_modules/kafkajs/src/broker/saslAuthenticator/scram512.js
var require_scram512 = __commonJS((exports, module) => {
  var { SCRAM, DIGESTS } = require_scram2();
  var scram512AuthenticatorProvider = (sasl) => ({ host, port, logger, saslAuthenticate }) => {
    const scram = new SCRAM(sasl, host, port, logger, saslAuthenticate, DIGESTS.SHA512);
    return {
      authenticate: async () => await scram.authenticate()
    };
  };
  module.exports = scram512AuthenticatorProvider;
});

// node_modules/kafkajs/src/protocol/sasl/awsIam/request.js
var require_request115 = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var US_ASCII_NULL_CHAR = "\0";
  module.exports = ({ authorizationIdentity, accessKeyId, secretAccessKey, sessionToken = "" }) => ({
    encode: async () => {
      return new Encoder().writeBytes([authorizationIdentity, accessKeyId, secretAccessKey, sessionToken].join(US_ASCII_NULL_CHAR)).buffer;
    }
  });
});

// node_modules/kafkajs/src/protocol/sasl/awsIam/response.js
var require_response113 = __commonJS((exports, module) => {
  module.exports = {
    decode: async () => true,
    parse: async () => true
  };
});

// node_modules/kafkajs/src/protocol/sasl/awsIam/index.js
var require_awsIam = __commonJS((exports, module) => {
  module.exports = {
    request: require_request115(),
    response: require_response113()
  };
});

// node_modules/kafkajs/src/broker/saslAuthenticator/awsIam.js
var require_awsIam2 = __commonJS((exports, module) => {
  var { request: request3, response } = require_awsIam();
  var { KafkaJSSASLAuthenticationError } = require_errors();
  var awsIAMAuthenticatorProvider = (sasl) => ({ host, port, logger, saslAuthenticate }) => {
    return {
      authenticate: async () => {
        if (!sasl.authorizationIdentity) {
          throw new KafkaJSSASLAuthenticationError("SASL AWS-IAM: Missing authorizationIdentity");
        }
        if (!sasl.accessKeyId) {
          throw new KafkaJSSASLAuthenticationError("SASL AWS-IAM: Missing accessKeyId");
        }
        if (!sasl.secretAccessKey) {
          throw new KafkaJSSASLAuthenticationError("SASL AWS-IAM: Missing secretAccessKey");
        }
        if (!sasl.sessionToken) {
          sasl.sessionToken = "";
        }
        const broker = `${host}:${port}`;
        try {
          logger.debug("Authenticate with SASL AWS-IAM", { broker });
          await saslAuthenticate({ request: request3(sasl), response });
          logger.debug("SASL AWS-IAM authentication successful", { broker });
        } catch (e) {
          const error = new KafkaJSSASLAuthenticationError(`SASL AWS-IAM authentication failed: ${e.message}`);
          logger.error(error.message, { broker });
          throw error;
        }
      }
    };
  };
  module.exports = awsIAMAuthenticatorProvider;
});

// node_modules/kafkajs/src/protocol/sasl/oauthBearer/request.js
var require_request116 = __commonJS((exports, module) => {
  var formatExtensions = function(extensions) {
    let msg = "";
    if (extensions == null) {
      return msg;
    }
    let prefix = "";
    for (const k in extensions) {
      msg += `${prefix}${k}=${extensions[k]}`;
      prefix = SEPARATOR;
    }
    return msg;
  };
  var Encoder = require_encoder();
  var SEPARATOR = "\x01";
  module.exports = async ({ authorizationIdentity = null }, oauthBearerToken) => {
    const authzid = authorizationIdentity == null ? "" : `"a=${authorizationIdentity}`;
    let ext = formatExtensions(oauthBearerToken.extensions);
    if (ext.length > 0) {
      ext = `${SEPARATOR}${ext}`;
    }
    const oauthMsg = `n,${authzid},${SEPARATOR}auth=Bearer ${oauthBearerToken.value}${ext}${SEPARATOR}${SEPARATOR}`;
    return {
      encode: async () => {
        return new Encoder().writeBytes(Buffer.from(oauthMsg)).buffer;
      }
    };
  };
});

// node_modules/kafkajs/src/protocol/sasl/oauthBearer/response.js
var require_response114 = __commonJS((exports, module) => {
  module.exports = {
    decode: async () => true,
    parse: async () => true
  };
});

// node_modules/kafkajs/src/protocol/sasl/oauthBearer/index.js
var require_oauthBearer = __commonJS((exports, module) => {
  module.exports = {
    request: require_request116(),
    response: require_response114()
  };
});

// node_modules/kafkajs/src/broker/saslAuthenticator/oauthBearer.js
var require_oauthBearer2 = __commonJS((exports, module) => {
  var { request: request3 } = require_oauthBearer();
  var { KafkaJSSASLAuthenticationError } = require_errors();
  var oauthBearerAuthenticatorProvider = (sasl) => ({ host, port, logger, saslAuthenticate }) => {
    return {
      authenticate: async () => {
        const { oauthBearerProvider } = sasl;
        if (oauthBearerProvider == null) {
          throw new KafkaJSSASLAuthenticationError("SASL OAUTHBEARER: Missing OAuth bearer token provider");
        }
        const oauthBearerToken = await oauthBearerProvider();
        if (oauthBearerToken.value == null) {
          throw new KafkaJSSASLAuthenticationError("SASL OAUTHBEARER: Invalid OAuth bearer token");
        }
        const broker = `${host}:${port}`;
        try {
          logger.debug("Authenticate with SASL OAUTHBEARER", { broker });
          await saslAuthenticate({ request: await request3(sasl, oauthBearerToken) });
          logger.debug("SASL OAUTHBEARER authentication successful", { broker });
        } catch (e) {
          const error = new KafkaJSSASLAuthenticationError(`SASL OAUTHBEARER authentication failed: ${e.message}`);
          logger.error(error.message, { broker });
          throw error;
        }
      }
    };
  };
  module.exports = oauthBearerAuthenticatorProvider;
});

// node_modules/kafkajs/src/broker/saslAuthenticator/index.js
var require_saslAuthenticator = __commonJS((exports, module) => {
  var { requests, lookup } = require_requests();
  var apiKeys = require_apiKeys();
  var plainAuthenticatorProvider = require_plain2();
  var scram256AuthenticatorProvider = require_scram256();
  var scram512AuthenticatorProvider = require_scram512();
  var awsIAMAuthenticatorProvider = require_awsIam2();
  var oauthBearerAuthenticatorProvider = require_oauthBearer2();
  var { KafkaJSSASLAuthenticationError } = require_errors();
  var BUILT_IN_AUTHENTICATION_PROVIDERS = {
    AWS: awsIAMAuthenticatorProvider,
    PLAIN: plainAuthenticatorProvider,
    OAUTHBEARER: oauthBearerAuthenticatorProvider,
    "SCRAM-SHA-256": scram256AuthenticatorProvider,
    "SCRAM-SHA-512": scram512AuthenticatorProvider
  };
  var UNLIMITED_SESSION_LIFETIME = "0";
  module.exports = class SASLAuthenticator {
    constructor(connection, logger, versions, supportAuthenticationProtocol) {
      this.connection = connection;
      this.logger = logger;
      this.sessionLifetime = UNLIMITED_SESSION_LIFETIME;
      const lookupRequest = lookup(versions);
      this.saslHandshake = lookupRequest(apiKeys.SaslHandshake, requests.SaslHandshake);
      this.protocolAuthentication = supportAuthenticationProtocol ? lookupRequest(apiKeys.SaslAuthenticate, requests.SaslAuthenticate) : null;
    }
    async authenticate() {
      const mechanism = this.connection.sasl.mechanism.toUpperCase();
      const handshake = await this.connection.send(this.saslHandshake({ mechanism }));
      if (!handshake.enabledMechanisms.includes(mechanism)) {
        throw new KafkaJSSASLAuthenticationError(`SASL ${mechanism} mechanism is not supported by the server`);
      }
      const saslAuthenticate = async ({ request: request3, response }) => {
        if (this.protocolAuthentication) {
          const requestAuthBytes = await request3.encode();
          const authResponse = await this.connection.send(this.protocolAuthentication({ authBytes: requestAuthBytes }));
          this.sessionLifetime = authResponse.sessionLifetimeMs || UNLIMITED_SESSION_LIFETIME;
          if (!response) {
            return;
          }
          const { authBytes: responseAuthBytes } = authResponse;
          const payloadDecoded = await response.decode(responseAuthBytes);
          return response.parse(payloadDecoded);
        }
        return this.connection.sendAuthRequest({ request: request3, response });
      };
      if (!this.connection.sasl.authenticationProvider && Object.keys(BUILT_IN_AUTHENTICATION_PROVIDERS).includes(mechanism)) {
        this.connection.sasl.authenticationProvider = BUILT_IN_AUTHENTICATION_PROVIDERS[mechanism](this.connection.sasl);
      }
      await this.connection.sasl.authenticationProvider({
        host: this.connection.host,
        port: this.connection.port,
        logger: this.logger.namespace(`SaslAuthenticator-${mechanism}`),
        saslAuthenticate
      }).authenticate();
    }
  };
});

// node_modules/kafkajs/src/network/connection.js
var require_connection = __commonJS((exports, module) => {
  var createSocket = require_socket();
  var createRequest = require_request111();
  var Decoder = require_decoder();
  var { KafkaJSConnectionError, KafkaJSConnectionClosedError } = require_errors();
  var { INT_32_MAX_VALUE } = require_constants2();
  var getEnv = require_env();
  var RequestQueue = require_requestQueue();
  var { CONNECTION_STATUS, CONNECTED_STATUS } = require_connectionStatus();
  var sharedPromiseTo = require_sharedPromiseTo();
  var Long = require_long();
  var SASLAuthenticator = require_saslAuthenticator();
  var apiKeys = require_apiKeys();
  var requestInfo = ({ apiName, apiKey, apiVersion }) => `${apiName}(key: ${apiKey}, version: ${apiVersion})`;
  var isAuthenticatedRequest = (request3) => {
    return ![apiKeys.ApiVersions, apiKeys.SaslHandshake, apiKeys.SaslAuthenticate].includes(request3.apiKey);
  };
  var PRIVATE = {
    SHOULD_REAUTHENTICATE: Symbol("private:Connection:shouldReauthenticate"),
    AUTHENTICATE: Symbol("private:Connection:authenticate")
  };
  module.exports = class Connection {
    constructor({
      host,
      port,
      logger,
      socketFactory,
      requestTimeout,
      reauthenticationThreshold = 1e4,
      rack = null,
      ssl = null,
      sasl = null,
      clientId = "kafkajs",
      connectionTimeout,
      enforceRequestTimeout = true,
      maxInFlightRequests = null,
      instrumentationEmitter = null
    }) {
      this.host = host;
      this.port = port;
      this.rack = rack;
      this.clientId = clientId;
      this.broker = `${this.host}:${this.port}`;
      this.logger = logger.namespace("Connection");
      this.socketFactory = socketFactory;
      this.ssl = ssl;
      this.sasl = sasl;
      this.requestTimeout = requestTimeout;
      this.connectionTimeout = connectionTimeout;
      this.reauthenticationThreshold = reauthenticationThreshold;
      this.bytesBuffered = 0;
      this.bytesNeeded = Decoder.int32Size();
      this.chunks = [];
      this.connectionStatus = CONNECTION_STATUS.DISCONNECTED;
      this.correlationId = 0;
      this.requestQueue = new RequestQueue({
        instrumentationEmitter,
        maxInFlightRequests,
        requestTimeout,
        enforceRequestTimeout,
        clientId,
        broker: this.broker,
        logger: logger.namespace("RequestQueue"),
        isConnected: () => this.isConnected()
      });
      this.versions = null;
      this.authHandlers = null;
      this.authExpectResponse = false;
      const log = (level) => (message, extra = {}) => {
        const logFn = this.logger[level];
        logFn(message, { broker: this.broker, clientId, ...extra });
      };
      this.logDebug = log("debug");
      this.logError = log("error");
      const env = getEnv();
      this.shouldLogBuffers = env.KAFKAJS_DEBUG_PROTOCOL_BUFFERS === "1";
      this.shouldLogFetchBuffer = this.shouldLogBuffers && env.KAFKAJS_DEBUG_EXTENDED_PROTOCOL_BUFFERS === "1";
      this.authenticatedAt = null;
      this.sessionLifetime = Long.ZERO;
      this.supportAuthenticationProtocol = null;
      this[PRIVATE.AUTHENTICATE] = sharedPromiseTo(async () => {
        if (this.sasl && !this.isAuthenticated()) {
          const authenticator = new SASLAuthenticator(this, this.logger, this.versions, this.supportAuthenticationProtocol);
          await authenticator.authenticate();
          this.authenticatedAt = process.hrtime();
          this.sessionLifetime = Long.fromValue(authenticator.sessionLifetime);
        }
      });
    }
    getSupportAuthenticationProtocol() {
      return this.supportAuthenticationProtocol;
    }
    setSupportAuthenticationProtocol(isSupported) {
      this.supportAuthenticationProtocol = isSupported;
    }
    setVersions(versions) {
      this.versions = versions;
    }
    isConnected() {
      return CONNECTED_STATUS.includes(this.connectionStatus);
    }
    connect() {
      return new Promise((resolve, reject) => {
        if (this.isConnected()) {
          return resolve(true);
        }
        this.authenticatedAt = null;
        let timeoutId;
        const onConnect = () => {
          clearTimeout(timeoutId);
          this.connectionStatus = CONNECTION_STATUS.CONNECTED;
          this.requestQueue.scheduleRequestTimeoutCheck();
          resolve(true);
        };
        const onData = (data) => {
          this.processData(data);
        };
        const onEnd = async () => {
          clearTimeout(timeoutId);
          const wasConnected = this.isConnected();
          if (this.authHandlers) {
            this.authHandlers.onError();
          } else if (wasConnected) {
            this.logDebug("Kafka server has closed connection");
            this.rejectRequests(new KafkaJSConnectionClosedError("Closed connection", {
              host: this.host,
              port: this.port
            }));
          }
          await this.disconnect();
        };
        const onError = async (e) => {
          clearTimeout(timeoutId);
          const error = new KafkaJSConnectionError(`Connection error: ${e.message}`, {
            broker: `${this.host}:${this.port}`,
            code: e.code
          });
          this.logError(error.message, { stack: e.stack });
          this.rejectRequests(error);
          await this.disconnect();
          reject(error);
        };
        const onTimeout = async () => {
          const error = new KafkaJSConnectionError("Connection timeout", {
            broker: `${this.host}:${this.port}`
          });
          this.logError(error.message);
          this.rejectRequests(error);
          await this.disconnect();
          reject(error);
        };
        this.logDebug(`Connecting`, {
          ssl: !!this.ssl,
          sasl: !!this.sasl
        });
        try {
          timeoutId = setTimeout(onTimeout, this.connectionTimeout);
          this.socket = createSocket({
            socketFactory: this.socketFactory,
            host: this.host,
            port: this.port,
            ssl: this.ssl,
            onConnect,
            onData,
            onEnd,
            onError,
            onTimeout
          });
        } catch (e) {
          clearTimeout(timeoutId);
          reject(new KafkaJSConnectionError(`Failed to connect: ${e.message}`, {
            broker: `${this.host}:${this.port}`
          }));
        }
      });
    }
    async disconnect() {
      this.authenticatedAt = null;
      this.connectionStatus = CONNECTION_STATUS.DISCONNECTING;
      this.logDebug("disconnecting...");
      await this.requestQueue.waitForPendingRequests();
      this.requestQueue.destroy();
      if (this.socket) {
        this.socket.end();
        this.socket.unref();
      }
      this.connectionStatus = CONNECTION_STATUS.DISCONNECTED;
      this.logDebug("disconnected");
      return true;
    }
    isAuthenticated() {
      return this.authenticatedAt != null && !this[PRIVATE.SHOULD_REAUTHENTICATE]();
    }
    [PRIVATE.SHOULD_REAUTHENTICATE]() {
      if (this.sessionLifetime.equals(Long.ZERO)) {
        return false;
      }
      if (this.authenticatedAt == null) {
        return true;
      }
      const [secondsSince, remainingNanosSince] = process.hrtime(this.authenticatedAt);
      const millisSince = Long.fromValue(secondsSince).multiply(1000).add(Long.fromValue(remainingNanosSince).divide(1e6));
      const reauthenticateAt = millisSince.add(this.reauthenticationThreshold);
      return reauthenticateAt.greaterThanOrEqual(this.sessionLifetime);
    }
    async authenticate() {
      await this[PRIVATE.AUTHENTICATE]();
    }
    sendAuthRequest({ request: request3, response }) {
      this.authExpectResponse = !!response;
      return new Promise(async (resolve, reject) => {
        this.authHandlers = {
          onSuccess: (rawData) => {
            this.authHandlers = null;
            this.authExpectResponse = false;
            response.decode(rawData).then((data) => response.parse(data)).then(resolve).catch(reject);
          },
          onError: () => {
            this.authHandlers = null;
            this.authExpectResponse = false;
            reject(new KafkaJSConnectionError("Connection closed by the server", {
              broker: `${this.host}:${this.port}`
            }));
          }
        };
        try {
          const requestPayload = await request3.encode();
          this.failIfNotConnected();
          this.socket.write(requestPayload, "binary");
        } catch (e) {
          reject(e);
        }
      });
    }
    async send({ request: request3, response, requestTimeout = null, logResponseError = true }) {
      if (!this.isAuthenticated() && isAuthenticatedRequest(request3)) {
        await this[PRIVATE.AUTHENTICATE]();
      }
      this.failIfNotConnected();
      const expectResponse = !request3.expectResponse || request3.expectResponse();
      const sendRequest = async () => {
        const { clientId } = this;
        const correlationId2 = this.nextCorrelationId();
        const requestPayload = await createRequest({ request: request3, correlationId: correlationId2, clientId });
        const { apiKey, apiName, apiVersion } = request3;
        this.logDebug(`Request ${requestInfo(request3)}`, {
          correlationId: correlationId2,
          expectResponse,
          size: Buffer.byteLength(requestPayload.buffer)
        });
        return new Promise((resolve, reject) => {
          try {
            this.failIfNotConnected();
            const entry2 = { apiKey, apiName, apiVersion, correlationId: correlationId2, resolve, reject };
            this.requestQueue.push({
              entry: entry2,
              expectResponse,
              requestTimeout,
              sendRequest: () => {
                this.socket.write(requestPayload.buffer, "binary");
              }
            });
          } catch (e) {
            reject(e);
          }
        });
      };
      const { correlationId, size, entry, payload } = await sendRequest();
      if (!expectResponse) {
        return;
      }
      try {
        const payloadDecoded = await response.decode(payload);
        this.requestQueue.maybeThrottle(payloadDecoded.clientSideThrottleTime);
        const data = await response.parse(payloadDecoded);
        const isFetchApi = entry.apiName === "Fetch";
        this.logDebug(`Response ${requestInfo(entry)}`, {
          correlationId,
          size,
          data: isFetchApi && !this.shouldLogFetchBuffer ? "[filtered]" : data
        });
        return data;
      } catch (e) {
        if (logResponseError) {
          this.logError(`Response ${requestInfo(entry)}`, {
            error: e.message,
            correlationId,
            size
          });
        }
        const isBuffer = Buffer.isBuffer(payload);
        this.logDebug(`Response ${requestInfo(entry)}`, {
          error: e.message,
          correlationId,
          payload: isBuffer && !this.shouldLogBuffers ? { type: "Buffer", data: "[filtered]" } : payload
        });
        throw e;
      }
    }
    failIfNotConnected() {
      if (!this.isConnected()) {
        throw new KafkaJSConnectionError("Not connected", {
          broker: `${this.host}:${this.port}`
        });
      }
    }
    nextCorrelationId() {
      if (this.correlationId >= INT_32_MAX_VALUE) {
        this.correlationId = 0;
      }
      return this.correlationId++;
    }
    processData(rawData) {
      if (this.authHandlers && !this.authExpectResponse) {
        return this.authHandlers.onSuccess(rawData);
      }
      this.chunks.push(rawData);
      this.bytesBuffered += Buffer.byteLength(rawData);
      while (this.bytesNeeded <= this.bytesBuffered) {
        const buffer = this.chunks.length > 1 ? Buffer.concat(this.chunks) : this.chunks[0];
        const decoder = new Decoder(buffer);
        const expectedResponseSize = decoder.readInt32();
        if (!decoder.canReadBytes(expectedResponseSize)) {
          this.chunks = [buffer];
          this.bytesBuffered = Buffer.byteLength(buffer);
          this.bytesNeeded = Decoder.int32Size() + expectedResponseSize;
          return;
        }
        const response = new Decoder(decoder.readBytes(expectedResponseSize));
        const remainderBuffer = decoder.readAll();
        this.chunks = [remainderBuffer];
        this.bytesBuffered = Buffer.byteLength(remainderBuffer);
        this.bytesNeeded = Decoder.int32Size();
        if (this.authHandlers) {
          const rawResponseSize = Decoder.int32Size() + expectedResponseSize;
          const rawResponseBuffer = buffer.slice(0, rawResponseSize);
          return this.authHandlers.onSuccess(rawResponseBuffer);
        }
        const correlationId = response.readInt32();
        const payload = response.readAll();
        this.requestQueue.fulfillRequest({
          size: expectedResponseSize,
          correlationId,
          payload
        });
      }
    }
    rejectRequests(error) {
      this.requestQueue.rejectAll(error);
    }
  };
});

// node_modules/kafkajs/src/network/connectionPool.js
var require_connectionPool = __commonJS((exports, module) => {
  var apiKeys = require_apiKeys();
  var Connection = require_connection();
  module.exports = class ConnectionPool {
    constructor(options) {
      this.logger = options.logger.namespace("ConnectionPool");
      this.connectionTimeout = options.connectionTimeout;
      this.host = options.host;
      this.port = options.port;
      this.rack = options.rack;
      this.ssl = options.ssl;
      this.sasl = options.sasl;
      this.clientId = options.clientId;
      this.socketFactory = options.socketFactory;
      this.pool = new Array(2).fill().map(() => new Connection(options));
    }
    isConnected() {
      return this.pool.some((c) => c.isConnected());
    }
    isAuthenticated() {
      return this.pool.some((c) => c.isAuthenticated());
    }
    setSupportAuthenticationProtocol(isSupported) {
      this.map((c) => c.setSupportAuthenticationProtocol(isSupported));
    }
    setVersions(versions) {
      this.map((c) => c.setVersions(versions));
    }
    map(callback) {
      return this.pool.map((c) => callback(c));
    }
    async send(protocolRequest) {
      const connection = await this.getConnectionByRequest(protocolRequest);
      return connection.send(protocolRequest);
    }
    getConnectionByRequest({ request: { apiKey } }) {
      const index = { [apiKeys.Fetch]: 1 }[apiKey] || 0;
      return this.getConnection(index);
    }
    async getConnection(index = 0) {
      const connection = this.pool[index];
      if (!connection.isConnected()) {
        await connection.connect();
      }
      return connection;
    }
    async destroy() {
      await Promise.all(this.map((c) => c.disconnect()));
    }
  };
});

// node_modules/kafkajs/src/cluster/connectionPoolBuilder.js
var require_connectionPoolBuilder = __commonJS((exports, module) => {
  var { KafkaJSConnectionError, KafkaJSNonRetriableError } = require_errors();
  var ConnectionPool = require_connectionPool();
  module.exports = ({
    socketFactory,
    brokers,
    ssl,
    sasl,
    clientId,
    requestTimeout,
    enforceRequestTimeout,
    connectionTimeout,
    maxInFlightRequests,
    logger,
    instrumentationEmitter = null,
    reauthenticationThreshold
  }) => {
    let index = 0;
    const isValidBroker = (broker) => {
      return broker && typeof broker === "string" && broker.length > 0;
    };
    const validateBrokers = (brokers2) => {
      if (!brokers2) {
        throw new KafkaJSNonRetriableError(`Failed to connect: brokers should not be null`);
      }
      if (Array.isArray(brokers2)) {
        if (!brokers2.length) {
          throw new KafkaJSNonRetriableError(`Failed to connect: brokers array is empty`);
        }
        brokers2.forEach((broker, index2) => {
          if (!isValidBroker(broker)) {
            throw new KafkaJSNonRetriableError(`Failed to connect: broker at index ${index2} is invalid "${typeof broker}"`);
          }
        });
      }
    };
    const getBrokers = async () => {
      let list;
      if (typeof brokers === "function") {
        try {
          list = await brokers();
        } catch (e) {
          const wrappedError = new KafkaJSConnectionError(`Failed to connect: "config.brokers" threw: ${e.message}`);
          wrappedError.stack = `${wrappedError.name}\n  Caused by: ${e.stack}`;
          throw wrappedError;
        }
      } else {
        list = brokers;
      }
      validateBrokers(list);
      return list;
    };
    return {
      build: async ({ host, port, rack } = {}) => {
        if (!host) {
          const list = await getBrokers();
          const randomBroker = list[index++ % list.length];
          host = randomBroker.split(":")[0];
          port = Number(randomBroker.split(":")[1]);
        }
        return new ConnectionPool({
          host,
          port,
          rack,
          sasl,
          ssl,
          clientId,
          socketFactory,
          connectionTimeout,
          requestTimeout,
          enforceRequestTimeout,
          maxInFlightRequests,
          instrumentationEmitter,
          logger,
          reauthenticationThreshold
        });
      }
    };
  };
});

// node_modules/kafkajs/src/cluster/index.js
var require_cluster = __commonJS((exports, module) => {
  var BrokerPool = require_brokerPool();
  var Lock = require_lock();
  var sharedPromiseTo = require_sharedPromiseTo();
  var createRetry = require_retry();
  var connectionPoolBuilder = require_connectionPoolBuilder();
  var { EARLIEST_OFFSET, LATEST_OFFSET } = require_constants2();
  var {
    KafkaJSError,
    KafkaJSBrokerNotFound,
    KafkaJSMetadataNotLoaded,
    KafkaJSTopicMetadataNotLoaded,
    KafkaJSGroupCoordinatorNotFound
  } = require_errors();
  var COORDINATOR_TYPES = require_coordinatorTypes();
  var { keys } = Object;
  var mergeTopics = (obj, { topic, partitions }) => ({
    ...obj,
    [topic]: [...obj[topic] || [], ...partitions]
  });
  var PRIVATE = {
    CONNECT: Symbol("private:Cluster:connect"),
    REFRESH_METADATA: Symbol("private:Cluster:refreshMetadata"),
    REFRESH_METADATA_IF_NECESSARY: Symbol("private:Cluster:refreshMetadataIfNecessary"),
    FIND_CONTROLLER_BROKER: Symbol("private:Cluster:findControllerBroker")
  };
  module.exports = class Cluster {
    constructor({
      logger: rootLogger,
      socketFactory,
      brokers,
      ssl,
      sasl,
      clientId,
      connectionTimeout,
      authenticationTimeout,
      reauthenticationThreshold,
      requestTimeout = 30000,
      enforceRequestTimeout,
      metadataMaxAge,
      retry,
      allowAutoTopicCreation,
      maxInFlightRequests,
      isolationLevel,
      instrumentationEmitter = null,
      offsets = new Map
    }) {
      this.rootLogger = rootLogger;
      this.logger = rootLogger.namespace("Cluster");
      this.retrier = createRetry(retry);
      this.connectionPoolBuilder = connectionPoolBuilder({
        logger: rootLogger,
        instrumentationEmitter,
        socketFactory,
        brokers,
        ssl,
        sasl,
        clientId,
        connectionTimeout,
        requestTimeout,
        enforceRequestTimeout,
        maxInFlightRequests,
        reauthenticationThreshold
      });
      this.targetTopics = new Set;
      this.mutatingTargetTopics = new Lock({
        description: `updating target topics`,
        timeout: requestTimeout
      });
      this.isolationLevel = isolationLevel;
      this.brokerPool = new BrokerPool({
        connectionPoolBuilder: this.connectionPoolBuilder,
        logger: this.rootLogger,
        retry,
        allowAutoTopicCreation,
        authenticationTimeout,
        metadataMaxAge
      });
      this.committedOffsetsByGroup = offsets;
      this[PRIVATE.CONNECT] = sharedPromiseTo(async () => {
        return await this.brokerPool.connect();
      });
      this[PRIVATE.REFRESH_METADATA] = sharedPromiseTo(async () => {
        return await this.brokerPool.refreshMetadata(Array.from(this.targetTopics));
      });
      this[PRIVATE.REFRESH_METADATA_IF_NECESSARY] = sharedPromiseTo(async () => {
        return await this.brokerPool.refreshMetadataIfNecessary(Array.from(this.targetTopics));
      });
      this[PRIVATE.FIND_CONTROLLER_BROKER] = sharedPromiseTo(async () => {
        const { metadata } = this.brokerPool;
        if (!metadata || metadata.controllerId == null) {
          throw new KafkaJSMetadataNotLoaded("Topic metadata not loaded");
        }
        const broker = await this.findBroker({ nodeId: metadata.controllerId });
        if (!broker) {
          throw new KafkaJSBrokerNotFound(`Controller broker with id ${metadata.controllerId} not found in the cached metadata`);
        }
        return broker;
      });
    }
    isConnected() {
      return this.brokerPool.hasConnectedBrokers();
    }
    async connect() {
      await this[PRIVATE.CONNECT]();
    }
    async disconnect() {
      await this.brokerPool.disconnect();
    }
    removeBroker({ host, port }) {
      this.brokerPool.removeBroker({ host, port });
    }
    async refreshMetadata() {
      await this[PRIVATE.REFRESH_METADATA]();
    }
    async refreshMetadataIfNecessary() {
      await this[PRIVATE.REFRESH_METADATA_IF_NECESSARY]();
    }
    async metadata({ topics = [] } = {}) {
      return this.retrier(async (bail, retryCount, retryTime) => {
        try {
          await this.brokerPool.refreshMetadataIfNecessary(topics);
          return this.brokerPool.withBroker(async ({ broker }) => broker.metadata(topics));
        } catch (e) {
          if (e.type === "LEADER_NOT_AVAILABLE") {
            throw e;
          }
          bail(e);
        }
      });
    }
    async addTargetTopic(topic) {
      return this.addMultipleTargetTopics([topic]);
    }
    async addMultipleTargetTopics(topics) {
      await this.mutatingTargetTopics.acquire();
      try {
        const previousSize = this.targetTopics.size;
        const previousTopics = new Set(this.targetTopics);
        for (const topic of topics) {
          this.targetTopics.add(topic);
        }
        const hasChanged = previousSize !== this.targetTopics.size || !this.brokerPool.metadata;
        if (hasChanged) {
          try {
            await this.refreshMetadata();
          } catch (e) {
            if (e.type === "INVALID_TOPIC_EXCEPTION" || e.type === "UNKNOWN_TOPIC_OR_PARTITION" || e.type === "TOPIC_AUTHORIZATION_FAILED") {
              this.targetTopics = previousTopics;
            }
            throw e;
          }
        }
      } finally {
        await this.mutatingTargetTopics.release();
      }
    }
    getNodeIds() {
      return this.brokerPool.getNodeIds();
    }
    async findBroker({ nodeId }) {
      try {
        return await this.brokerPool.findBroker({ nodeId });
      } catch (e) {
        if (e.name === "KafkaJSBrokerNotFound" || e.name === "KafkaJSLockTimeout" || e.name === "KafkaJSConnectionError") {
          await this.refreshMetadata();
        }
        throw e;
      }
    }
    async findControllerBroker() {
      return await this[PRIVATE.FIND_CONTROLLER_BROKER]();
    }
    findTopicPartitionMetadata(topic) {
      const { metadata } = this.brokerPool;
      if (!metadata || !metadata.topicMetadata) {
        throw new KafkaJSTopicMetadataNotLoaded("Topic metadata not loaded", { topic });
      }
      const topicMetadata = metadata.topicMetadata.find((t) => t.topic === topic);
      return topicMetadata ? topicMetadata.partitionMetadata : [];
    }
    findLeaderForPartitions(topic, partitions) {
      const partitionMetadata = this.findTopicPartitionMetadata(topic);
      return partitions.reduce((result, id) => {
        const partitionId = parseInt(id, 10);
        const metadata = partitionMetadata.find((p) => p.partitionId === partitionId);
        if (!metadata) {
          return result;
        }
        if (metadata.leader === null || metadata.leader === undefined) {
          throw new KafkaJSError("Invalid partition metadata", { topic, partitionId, metadata });
        }
        const { leader } = metadata;
        const current = result[leader] || [];
        return { ...result, [leader]: [...current, partitionId] };
      }, {});
    }
    async findGroupCoordinator({ groupId, coordinatorType = COORDINATOR_TYPES.GROUP }) {
      return this.retrier(async (bail, retryCount, retryTime) => {
        try {
          const { coordinator } = await this.findGroupCoordinatorMetadata({
            groupId,
            coordinatorType
          });
          return await this.findBroker({ nodeId: coordinator.nodeId });
        } catch (e) {
          if (e.name === "KafkaJSBrokerNotFound" || e.type === "GROUP_COORDINATOR_NOT_AVAILABLE") {
            this.logger.debug(`${e.message}, refreshing metadata and trying again...`, {
              groupId,
              retryCount,
              retryTime
            });
            await this.refreshMetadata();
            throw e;
          }
          if (e.code === "ECONNREFUSED") {
            throw e;
          }
          bail(e);
        }
      });
    }
    async findGroupCoordinatorMetadata({ groupId, coordinatorType }) {
      const brokerMetadata = await this.brokerPool.withBroker(async ({ nodeId, broker }) => {
        return await this.retrier(async (bail, retryCount, retryTime) => {
          try {
            const brokerMetadata2 = await broker.findGroupCoordinator({ groupId, coordinatorType });
            this.logger.debug("Found group coordinator", {
              broker: brokerMetadata2.host,
              nodeId: brokerMetadata2.coordinator.nodeId
            });
            return brokerMetadata2;
          } catch (e) {
            this.logger.debug("Tried to find group coordinator", {
              nodeId,
              groupId,
              error: e
            });
            if (e.type === "GROUP_COORDINATOR_NOT_AVAILABLE") {
              this.logger.debug("Group coordinator not available, retrying...", {
                nodeId,
                retryCount,
                retryTime
              });
              throw e;
            }
            bail(e);
          }
        });
      });
      if (brokerMetadata) {
        return brokerMetadata;
      }
      throw new KafkaJSGroupCoordinatorNotFound("Failed to find group coordinator");
    }
    defaultOffset({ fromBeginning }) {
      return fromBeginning ? EARLIEST_OFFSET : LATEST_OFFSET;
    }
    async fetchTopicsOffset(topics) {
      const partitionsPerBroker = {};
      const topicConfigurations = {};
      const addDefaultOffset = (topic) => (partition) => {
        const { timestamp } = topicConfigurations[topic];
        return { ...partition, timestamp };
      };
      for (const topicData of topics) {
        const { topic, partitions, fromBeginning, fromTimestamp } = topicData;
        const partitionsPerLeader = this.findLeaderForPartitions(topic, partitions.map((p) => p.partition));
        const timestamp = fromTimestamp != null ? fromTimestamp : this.defaultOffset({ fromBeginning });
        topicConfigurations[topic] = { timestamp };
        keys(partitionsPerLeader).forEach((nodeId) => {
          partitionsPerBroker[nodeId] = partitionsPerBroker[nodeId] || {};
          partitionsPerBroker[nodeId][topic] = partitions.filter((p) => partitionsPerLeader[nodeId].includes(p.partition));
        });
      }
      const requests = keys(partitionsPerBroker).map(async (nodeId) => {
        const broker = await this.findBroker({ nodeId });
        const partitions = partitionsPerBroker[nodeId];
        const { responses: topicOffsets } = await broker.listOffsets({
          isolationLevel: this.isolationLevel,
          topics: keys(partitions).map((topic) => ({
            topic,
            partitions: partitions[topic].map(addDefaultOffset(topic))
          }))
        });
        return topicOffsets;
      });
      const responses = await Promise.all(requests);
      const partitionsPerTopic = responses.flat().reduce(mergeTopics, {});
      return keys(partitionsPerTopic).map((topic) => ({
        topic,
        partitions: partitionsPerTopic[topic].map(({ partition, offset }) => ({
          partition,
          offset
        }))
      }));
    }
    committedOffsets({ groupId }) {
      if (!this.committedOffsetsByGroup.has(groupId)) {
        this.committedOffsetsByGroup.set(groupId, {});
      }
      return this.committedOffsetsByGroup.get(groupId);
    }
    markOffsetAsCommitted({ groupId, topic, partition, offset }) {
      const committedOffsets = this.committedOffsets({ groupId });
      committedOffsets[topic] = committedOffsets[topic] || {};
      committedOffsets[topic][partition] = offset;
    }
  };
});

// node_modules/kafkajs/src/producer/partitioners/default/murmur2.js
var require_murmur2 = __commonJS((exports, module) => {
  var Long = require_long();
  var SEED = Long.fromValue(2538058380);
  var M = Long.fromValue(1540483477);
  var R = Long.fromValue(24);
  module.exports = (key) => {
    const data = Buffer.isBuffer(key) ? key : Buffer.from(String(key));
    const length = data.length;
    let h = Long.fromValue(SEED.xor(length));
    let length4 = Math.floor(length / 4);
    for (let i = 0;i < length4; i++) {
      const i4 = i * 4;
      let k = (data[i4 + 0] & 255) + ((data[i4 + 1] & 255) << 8) + ((data[i4 + 2] & 255) << 16) + ((data[i4 + 3] & 255) << 24);
      k = Long.fromValue(k);
      k = k.multiply(M);
      k = k.xor(k.toInt() >>> R);
      k = Long.fromValue(k).multiply(M);
      h = h.multiply(M);
      h = h.xor(k);
    }
    switch (length % 4) {
      case 3:
        h = h.xor((data[(length & ~3) + 2] & 255) << 16);
      case 2:
        h = h.xor((data[(length & ~3) + 1] & 255) << 8);
      case 1:
        h = h.xor(data[length & ~3] & 255);
        h = h.multiply(M);
    }
    h = h.xor(h.toInt() >>> 13);
    h = h.multiply(M);
    h = h.xor(h.toInt() >>> 15);
    return h.toInt();
  };
});

// node_modules/kafkajs/src/producer/partitioners/legacy/randomBytes.js
var require_randomBytes = __commonJS((exports, module) => {
  var { KafkaJSNonRetriableError } = require_errors();
  var toNodeCompatible = (crypto2) => ({
    randomBytes: (size) => crypto2.getRandomValues(Buffer.allocUnsafe(size))
  });
  var cryptoImplementation = null;
  if (global && global.crypto) {
    cryptoImplementation = global.crypto.randomBytes === undefined ? toNodeCompatible(global.crypto) : global.crypto;
  } else if (global && global.msCrypto) {
    cryptoImplementation = toNodeCompatible(global.msCrypto);
  } else if (global && !global.crypto) {
    cryptoImplementation = import.meta.require("crypto");
  }
  var MAX_BYTES = 65536;
  module.exports = (size) => {
    if (size > MAX_BYTES) {
      throw new KafkaJSNonRetriableError(`Byte length (${size}) exceeds the max number of bytes of entropy available (${MAX_BYTES})`);
    }
    if (!cryptoImplementation) {
      throw new KafkaJSNonRetriableError("No available crypto implementation");
    }
    return cryptoImplementation.randomBytes(size);
  };
});

// node_modules/kafkajs/src/producer/partitioners/legacy/partitioner.js
var require_partitioner = __commonJS((exports, module) => {
  var randomBytes = require_randomBytes();
  var toPositive = (x) => x & 2147483647;
  module.exports = (murmur2) => () => {
    const counters = {};
    return ({ topic, partitionMetadata, message }) => {
      if (!(topic in counters)) {
        counters[topic] = randomBytes(32).readUInt32BE(0);
      }
      const numPartitions = partitionMetadata.length;
      const availablePartitions = partitionMetadata.filter((p) => p.leader >= 0);
      const numAvailablePartitions = availablePartitions.length;
      if (message.partition !== null && message.partition !== undefined) {
        return message.partition;
      }
      if (message.key !== null && message.key !== undefined) {
        return toPositive(murmur2(message.key)) % numPartitions;
      }
      if (numAvailablePartitions > 0) {
        const i = toPositive(++counters[topic]) % numAvailablePartitions;
        return availablePartitions[i].partitionId;
      }
      return toPositive(++counters[topic]) % numPartitions;
    };
  };
});

// node_modules/kafkajs/src/producer/partitioners/default/index.js
var require_default = __commonJS((exports, module) => {
  var murmur2 = require_murmur2();
  var createDefaultPartitioner = require_partitioner();
  module.exports = createDefaultPartitioner(murmur2);
});

// node_modules/kafkajs/src/producer/partitioners/legacy/murmur2.js
var require_murmur22 = __commonJS((exports, module) => {
  var SEED = 2538058380;
  var M = 1540483477;
  var R = 24;
  module.exports = (key) => {
    const data = Buffer.isBuffer(key) ? key : Buffer.from(String(key));
    const length = data.length;
    let h = SEED ^ length;
    let length4 = length / 4;
    for (let i = 0;i < length4; i++) {
      const i4 = i * 4;
      let k = (data[i4 + 0] & 255) + ((data[i4 + 1] & 255) << 8) + ((data[i4 + 2] & 255) << 16) + ((data[i4 + 3] & 255) << 24);
      k *= M;
      k ^= k >>> R;
      k *= M;
      h *= M;
      h ^= k;
    }
    switch (length % 4) {
      case 3:
        h ^= (data[(length & ~3) + 2] & 255) << 16;
      case 2:
        h ^= (data[(length & ~3) + 1] & 255) << 8;
      case 1:
        h ^= data[length & ~3] & 255;
        h *= M;
    }
    h ^= h >>> 13;
    h *= M;
    h ^= h >>> 15;
    return h;
  };
});

// node_modules/kafkajs/src/producer/partitioners/legacy/index.js
var require_legacy = __commonJS((exports, module) => {
  var murmur2 = require_murmur22();
  var createLegacyPartitioner = require_partitioner();
  module.exports = createLegacyPartitioner(murmur2);
});

// node_modules/kafkajs/src/producer/partitioners/index.js
var require_partitioners = __commonJS((exports, module) => {
  var DefaultPartitioner = require_default();
  var LegacyPartitioner = require_legacy();
  module.exports = {
    DefaultPartitioner,
    LegacyPartitioner,
    JavaCompatiblePartitioner: DefaultPartitioner
  };
});

// node_modules/kafkajs/src/producer/eosManager/transactionStates.js
var require_transactionStates = __commonJS((exports, module) => {
  module.exports = {
    UNINITIALIZED: "UNINITIALIZED",
    READY: "READY",
    TRANSACTING: "TRANSACTING",
    COMMITTING: "COMMITTING",
    ABORTING: "ABORTING"
  };
});

// node_modules/kafkajs/src/producer/eosManager/transactionStateMachine.js
var require_transactionStateMachine = __commonJS((exports, module) => {
  var { EventEmitter } = import.meta.require("events");
  var { KafkaJSNonRetriableError } = require_errors();
  var STATES = require_transactionStates();
  var VALID_STATE_TRANSITIONS = {
    [STATES.UNINITIALIZED]: [STATES.READY],
    [STATES.READY]: [STATES.READY, STATES.TRANSACTING],
    [STATES.TRANSACTING]: [STATES.COMMITTING, STATES.ABORTING],
    [STATES.COMMITTING]: [STATES.READY],
    [STATES.ABORTING]: [STATES.READY]
  };
  module.exports = ({ logger, initialState = STATES.UNINITIALIZED }) => {
    let currentState = initialState;
    const guard = (object, method, { legalStates, async: isAsync = true }) => {
      if (!object[method]) {
        throw new KafkaJSNonRetriableError(`Cannot add guard on missing method "${method}"`);
      }
      return (...args) => {
        const fn = object[method];
        if (!legalStates.includes(currentState)) {
          const error = new KafkaJSNonRetriableError(`Transaction state exception: Cannot call "${method}" in state "${currentState}"`);
          if (isAsync) {
            return Promise.reject(error);
          } else {
            throw error;
          }
        }
        return fn.apply(object, args);
      };
    };
    const stateMachine = Object.assign(new EventEmitter, {
      createGuarded(object, methodStateMapping) {
        const guardedMethods = Object.keys(methodStateMapping).reduce((guards, method) => {
          guards[method] = guard(object, method, methodStateMapping[method]);
          return guards;
        }, {});
        return { ...object, ...guardedMethods };
      },
      transitionTo(state) {
        logger.debug(`Transaction state transition ${currentState} --> ${state}`);
        if (!VALID_STATE_TRANSITIONS[currentState].includes(state)) {
          throw new KafkaJSNonRetriableError(`Transaction state exception: Invalid transition ${currentState} --> ${state}`);
        }
        stateMachine.emit("transition", { to: state, from: currentState });
        currentState = state;
      },
      state() {
        return currentState;
      }
    });
    return stateMachine;
  };
});

// node_modules/kafkajs/src/producer/eosManager/index.js
var require_eosManager = __commonJS((exports, module) => {
  var createRetry = require_retry();
  var Lock = require_lock();
  var { KafkaJSNonRetriableError } = require_errors();
  var COORDINATOR_TYPES = require_coordinatorTypes();
  var createStateMachine = require_transactionStateMachine();
  var { INT_32_MAX_VALUE } = require_constants2();
  var assert = import.meta.require("assert");
  var STATES = require_transactionStates();
  var NO_PRODUCER_ID = -1;
  var SEQUENCE_START = 0;
  var INIT_PRODUCER_RETRIABLE_PROTOCOL_ERRORS = [
    "NOT_COORDINATOR_FOR_GROUP",
    "GROUP_COORDINATOR_NOT_AVAILABLE",
    "GROUP_LOAD_IN_PROGRESS",
    "CONCURRENT_TRANSACTIONS"
  ];
  var COMMIT_RETRIABLE_PROTOCOL_ERRORS = [
    "UNKNOWN_TOPIC_OR_PARTITION",
    "COORDINATOR_LOAD_IN_PROGRESS"
  ];
  var COMMIT_STALE_COORDINATOR_PROTOCOL_ERRORS = ["COORDINATOR_NOT_AVAILABLE", "NOT_COORDINATOR"];
  module.exports = ({
    logger,
    cluster,
    transactionTimeout = 60000,
    transactional,
    transactionalId
  }) => {
    if (transactional && !transactionalId) {
      throw new KafkaJSNonRetriableError("Cannot manage transactions without a transactionalId");
    }
    const retrier = createRetry(cluster.retry);
    let producerId = NO_PRODUCER_ID;
    let producerEpoch = 0;
    let producerSequence = {};
    let brokerMutexLocks = {};
    let transactionTopicPartitions = {};
    let hasOffsetsAddedToTransaction = false;
    const stateMachine = createStateMachine({ logger });
    stateMachine.on("transition", ({ to }) => {
      if (to === STATES.READY) {
        transactionTopicPartitions = {};
        hasOffsetsAddedToTransaction = false;
      }
    });
    const findTransactionCoordinator = () => {
      return cluster.findGroupCoordinator({
        groupId: transactionalId,
        coordinatorType: COORDINATOR_TYPES.TRANSACTION
      });
    };
    const transactionalGuard = () => {
      if (!transactional) {
        throw new KafkaJSNonRetriableError("Method unavailable if non-transactional");
      }
    };
    const isOngoing = () => {
      return hasOffsetsAddedToTransaction || Object.entries(transactionTopicPartitions).some(([, partitions]) => {
        return Object.entries(partitions).some(([, isPartitionAddedToTransaction]) => isPartitionAddedToTransaction);
      });
    };
    const eosManager = stateMachine.createGuarded({
      getProducerId() {
        return producerId;
      },
      getProducerEpoch() {
        return producerEpoch;
      },
      getTransactionalId() {
        return transactionalId;
      },
      async initProducerId() {
        return retrier(async (bail, retryCount, retryTime) => {
          try {
            await cluster.refreshMetadataIfNecessary();
            const broker = await (transactional ? findTransactionCoordinator() : cluster.findControllerBroker());
            const result = await broker.initProducerId({
              transactionalId: transactional ? transactionalId : undefined,
              transactionTimeout
            });
            stateMachine.transitionTo(STATES.READY);
            producerId = result.producerId;
            producerEpoch = result.producerEpoch;
            producerSequence = {};
            brokerMutexLocks = {};
            logger.debug("Initialized producer id & epoch", { producerId, producerEpoch });
          } catch (e) {
            if (INIT_PRODUCER_RETRIABLE_PROTOCOL_ERRORS.includes(e.type)) {
              if (e.type === "CONCURRENT_TRANSACTIONS") {
                logger.debug("There is an ongoing transaction on this transactionId, retrying", {
                  error: e.message,
                  stack: e.stack,
                  transactionalId,
                  retryCount,
                  retryTime
                });
              }
              throw e;
            }
            bail(e);
          }
        });
      },
      getSequence(topic, partition) {
        if (!eosManager.isInitialized()) {
          return SEQUENCE_START;
        }
        producerSequence[topic] = producerSequence[topic] || {};
        producerSequence[topic][partition] = producerSequence[topic][partition] || SEQUENCE_START;
        return producerSequence[topic][partition];
      },
      updateSequence(topic, partition, increment) {
        if (!eosManager.isInitialized()) {
          return;
        }
        const previous = eosManager.getSequence(topic, partition);
        let sequence = previous + increment;
        if (sequence >= INT_32_MAX_VALUE) {
          logger.debug(`Sequence for ${topic} ${partition} exceeds max value (${sequence}). Rotating to 0.`);
          sequence = 0;
        }
        producerSequence[topic][partition] = sequence;
      },
      beginTransaction() {
        transactionalGuard();
        stateMachine.transitionTo(STATES.TRANSACTING);
      },
      async addPartitionsToTransaction(topicData) {
        transactionalGuard();
        const newTopicPartitions = {};
        topicData.forEach(({ topic, partitions }) => {
          transactionTopicPartitions[topic] = transactionTopicPartitions[topic] || {};
          partitions.forEach(({ partition }) => {
            if (!transactionTopicPartitions[topic][partition]) {
              newTopicPartitions[topic] = newTopicPartitions[topic] || [];
              newTopicPartitions[topic].push(partition);
            }
          });
        });
        const topics = Object.keys(newTopicPartitions).map((topic) => ({
          topic,
          partitions: newTopicPartitions[topic]
        }));
        if (topics.length) {
          const broker = await findTransactionCoordinator();
          await broker.addPartitionsToTxn({ transactionalId, producerId, producerEpoch, topics });
        }
        topics.forEach(({ topic, partitions }) => {
          partitions.forEach((partition) => {
            transactionTopicPartitions[topic][partition] = true;
          });
        });
      },
      async commit() {
        transactionalGuard();
        stateMachine.transitionTo(STATES.COMMITTING);
        if (!isOngoing()) {
          logger.debug("No partitions or offsets registered, not sending EndTxn");
          stateMachine.transitionTo(STATES.READY);
          return;
        }
        const broker = await findTransactionCoordinator();
        await broker.endTxn({
          producerId,
          producerEpoch,
          transactionalId,
          transactionResult: true
        });
        stateMachine.transitionTo(STATES.READY);
      },
      async abort() {
        transactionalGuard();
        stateMachine.transitionTo(STATES.ABORTING);
        if (!isOngoing()) {
          logger.debug("No partitions or offsets registered, not sending EndTxn");
          stateMachine.transitionTo(STATES.READY);
          return;
        }
        const broker = await findTransactionCoordinator();
        await broker.endTxn({
          producerId,
          producerEpoch,
          transactionalId,
          transactionResult: false
        });
        stateMachine.transitionTo(STATES.READY);
      },
      isInitialized() {
        return producerId !== NO_PRODUCER_ID;
      },
      isTransactional() {
        return transactional;
      },
      isInTransaction() {
        return stateMachine.state() === STATES.TRANSACTING;
      },
      async acquireBrokerLock(broker) {
        if (this.isInitialized()) {
          brokerMutexLocks[broker.nodeId] = brokerMutexLocks[broker.nodeId] || new Lock({ timeout: 65535 });
          await brokerMutexLocks[broker.nodeId].acquire();
        }
      },
      releaseBrokerLock(broker) {
        if (this.isInitialized())
          brokerMutexLocks[broker.nodeId].release();
      },
      async sendOffsets({ consumerGroupId, topics }) {
        assert(consumerGroupId, "Missing consumerGroupId");
        assert(topics, "Missing offset topics");
        const transactionCoordinator = await findTransactionCoordinator();
        await transactionCoordinator.addOffsetsToTxn({
          transactionalId,
          producerId,
          producerEpoch,
          groupId: consumerGroupId
        });
        hasOffsetsAddedToTransaction = true;
        let groupCoordinator = await cluster.findGroupCoordinator({
          groupId: consumerGroupId,
          coordinatorType: COORDINATOR_TYPES.GROUP
        });
        return retrier(async (bail, retryCount, retryTime) => {
          try {
            await groupCoordinator.txnOffsetCommit({
              transactionalId,
              producerId,
              producerEpoch,
              groupId: consumerGroupId,
              topics
            });
          } catch (e) {
            if (COMMIT_RETRIABLE_PROTOCOL_ERRORS.includes(e.type)) {
              logger.debug("Group coordinator is not ready yet, retrying", {
                error: e.message,
                stack: e.stack,
                transactionalId,
                retryCount,
                retryTime
              });
              throw e;
            }
            if (COMMIT_STALE_COORDINATOR_PROTOCOL_ERRORS.includes(e.type) || e.code === "ECONNREFUSED") {
              logger.debug("Invalid group coordinator, finding new group coordinator and retrying", {
                error: e.message,
                stack: e.stack,
                transactionalId,
                retryCount,
                retryTime
              });
              groupCoordinator = await cluster.findGroupCoordinator({
                groupId: consumerGroupId,
                coordinatorType: COORDINATOR_TYPES.GROUP
              });
              throw e;
            }
            bail(e);
          }
        });
      }
    }, {
      initProducerId: { legalStates: [STATES.UNINITIALIZED, STATES.READY] },
      beginTransaction: { legalStates: [STATES.READY], async: false },
      addPartitionsToTransaction: { legalStates: [STATES.TRANSACTING] },
      sendOffsets: { legalStates: [STATES.TRANSACTING] },
      commit: { legalStates: [STATES.TRANSACTING] },
      abort: { legalStates: [STATES.TRANSACTING] }
    });
    return eosManager;
  };
});

// node_modules/kafkajs/src/producer/groupMessagesPerPartition.js
var require_groupMessagesPerPartition = __commonJS((exports, module) => {
  module.exports = ({ topic, partitionMetadata, messages, partitioner }) => {
    if (partitionMetadata.length === 0) {
      return {};
    }
    return messages.reduce((result, message) => {
      const partition = partitioner({ topic, partitionMetadata, message });
      const current = result[partition] || [];
      return Object.assign(result, { [partition]: [...current, message] });
    }, {});
  };
});

// node_modules/kafkajs/src/producer/createTopicData.js
var require_createTopicData = __commonJS((exports, module) => {
  module.exports = (topicDataForBroker) => {
    return topicDataForBroker.map(({ topic, partitions, messagesPerPartition, sequencePerPartition }) => ({
      topic,
      partitions: partitions.map((partition) => ({
        partition,
        messages: messagesPerPartition[partition]
      }))
    }));
  };
});

// node_modules/kafkajs/src/producer/responseSerializer.js
var require_responseSerializer = __commonJS((exports, module) => {
  module.exports = ({ topics }) => topics.flatMap(({ topicName, partitions }) => partitions.map((partition) => ({ topicName, ...partition })));
});

// node_modules/kafkajs/src/producer/sendMessages.js
var require_sendMessages = __commonJS((exports, module) => {
  var { KafkaJSMetadataNotLoaded } = require_errors();
  var { staleMetadata } = require_error();
  var groupMessagesPerPartition = require_groupMessagesPerPartition();
  var createTopicData = require_createTopicData();
  var responseSerializer = require_responseSerializer();
  var { keys } = Object;
  module.exports = ({ logger, cluster, partitioner, eosManager, retrier }) => {
    return async ({ acks, timeout, compression, topicMessages }) => {
      const responsePerBroker = new Map;
      const createProducerRequests = async (responsePerBroker2) => {
        const topicMetadata = new Map;
        await cluster.refreshMetadataIfNecessary();
        for (const { topic, messages } of topicMessages) {
          const partitionMetadata = cluster.findTopicPartitionMetadata(topic);
          if (partitionMetadata.length === 0) {
            logger.debug("Producing to topic without metadata", {
              topic,
              targetTopics: Array.from(cluster.targetTopics)
            });
            throw new KafkaJSMetadataNotLoaded("Producing to topic without metadata");
          }
          const messagesPerPartition = groupMessagesPerPartition({
            topic,
            partitionMetadata,
            messages,
            partitioner
          });
          const partitions = keys(messagesPerPartition);
          const partitionsPerLeader = cluster.findLeaderForPartitions(topic, partitions);
          const leaders = keys(partitionsPerLeader);
          topicMetadata.set(topic, {
            partitionsPerLeader,
            messagesPerPartition
          });
          for (const nodeId of leaders) {
            const broker = await cluster.findBroker({ nodeId });
            if (!responsePerBroker2.has(broker)) {
              responsePerBroker2.set(broker, null);
            }
          }
        }
        const brokers = Array.from(responsePerBroker2.keys());
        const brokersWithoutResponse = brokers.filter((broker) => !responsePerBroker2.get(broker));
        return brokersWithoutResponse.map(async (broker) => {
          const entries = Array.from(topicMetadata.entries());
          const topicDataForBroker = entries.filter(([_, { partitionsPerLeader }]) => !!partitionsPerLeader[broker.nodeId]).map(([topic, { partitionsPerLeader, messagesPerPartition, sequencePerPartition }]) => ({
            topic,
            partitions: partitionsPerLeader[broker.nodeId],
            messagesPerPartition
          }));
          const topicData = createTopicData(topicDataForBroker);
          await eosManager.acquireBrokerLock(broker);
          try {
            if (eosManager.isTransactional()) {
              await eosManager.addPartitionsToTransaction(topicData);
            }
            topicData.forEach(({ topic, partitions }) => {
              partitions.forEach((entry) => {
                entry["firstSequence"] = eosManager.getSequence(topic, entry.partition);
                eosManager.updateSequence(topic, entry.partition, entry.messages.length);
              });
            });
            let response;
            try {
              response = await broker.produce({
                transactionalId: eosManager.isTransactional() ? eosManager.getTransactionalId() : undefined,
                producerId: eosManager.getProducerId(),
                producerEpoch: eosManager.getProducerEpoch(),
                acks,
                timeout,
                compression,
                topicData
              });
            } catch (e) {
              topicData.forEach(({ topic, partitions }) => {
                partitions.forEach((entry) => {
                  eosManager.updateSequence(topic, entry.partition, -entry.messages.length);
                });
              });
              throw e;
            }
            const expectResponse = acks !== 0;
            const formattedResponse = expectResponse ? responseSerializer(response) : [];
            responsePerBroker2.set(broker, formattedResponse);
          } catch (e) {
            responsePerBroker2.delete(broker);
            throw e;
          } finally {
            await eosManager.releaseBrokerLock(broker);
          }
        });
      };
      return retrier(async (bail, retryCount, retryTime) => {
        const topics = topicMessages.map(({ topic }) => topic);
        await cluster.addMultipleTargetTopics(topics);
        try {
          const requests = await createProducerRequests(responsePerBroker);
          await Promise.all(requests);
          return Array.from(responsePerBroker.values()).flat();
        } catch (e) {
          if (e.name === "KafkaJSConnectionClosedError") {
            cluster.removeBroker({ host: e.host, port: e.port });
          }
          if (!cluster.isConnected()) {
            logger.debug(`Cluster has disconnected, reconnecting: ${e.message}`, {
              retryCount,
              retryTime
            });
            await cluster.connect();
            await cluster.refreshMetadata();
            throw e;
          }
          if (staleMetadata(e) || e.name === "KafkaJSMetadataNotLoaded" || e.name === "KafkaJSConnectionError" || e.name === "KafkaJSConnectionClosedError" || e.name === "KafkaJSProtocolError" && e.retriable) {
            logger.error(`Failed to send messages: ${e.message}`, { retryCount, retryTime });
            await cluster.refreshMetadata();
            throw e;
          }
          logger.error(`${e.message}`, { retryCount, retryTime });
          if (e.retriable)
            throw e;
          bail(e);
        }
      });
    };
  };
});

// node_modules/kafkajs/src/producer/messageProducer.js
var require_messageProducer = __commonJS((exports, module) => {
  var createSendMessages = require_sendMessages();
  var { KafkaJSError, KafkaJSNonRetriableError } = require_errors();
  var { CONNECTION_STATUS } = require_connectionStatus();
  module.exports = ({
    logger,
    cluster,
    partitioner,
    eosManager,
    idempotent,
    retrier,
    getConnectionStatus
  }) => {
    const sendMessages = createSendMessages({
      logger,
      cluster,
      retrier,
      partitioner,
      eosManager
    });
    const validateConnectionStatus = () => {
      const connectionStatus = getConnectionStatus();
      switch (connectionStatus) {
        case CONNECTION_STATUS.DISCONNECTING:
          throw new KafkaJSNonRetriableError(`The producer is disconnecting; therefore, it can't safely accept messages anymore`);
        case CONNECTION_STATUS.DISCONNECTED:
          throw new KafkaJSError("The producer is disconnected");
      }
    };
    const sendBatch = async ({ acks = -1, timeout, compression, topicMessages = [] }) => {
      if (topicMessages.some(({ topic }) => !topic)) {
        throw new KafkaJSNonRetriableError(`Invalid topic`);
      }
      if (idempotent && acks !== -1) {
        throw new KafkaJSNonRetriableError(`Not requiring ack for all messages invalidates the idempotent producer's EoS guarantees`);
      }
      for (const { topic, messages } of topicMessages) {
        if (!messages) {
          throw new KafkaJSNonRetriableError(`Invalid messages array [${messages}] for topic "${topic}"`);
        }
        const messageWithoutValue = messages.find((message) => message.value === undefined);
        if (messageWithoutValue) {
          throw new KafkaJSNonRetriableError(`Invalid message without value for topic "${topic}": ${JSON.stringify(messageWithoutValue)}`);
        }
      }
      validateConnectionStatus();
      const mergedTopicMessages = topicMessages.reduce((merged, { topic, messages }) => {
        const index = merged.findIndex(({ topic: mergedTopic }) => topic === mergedTopic);
        if (index === -1) {
          merged.push({ topic, messages });
        } else {
          merged[index].messages = [...merged[index].messages, ...messages];
        }
        return merged;
      }, []);
      return await sendMessages({
        acks,
        timeout,
        compression,
        topicMessages: mergedTopicMessages
      });
    };
    const send = async ({ acks, timeout, compression, topic, messages }) => {
      const topicMessage = { topic, messages };
      return sendBatch({
        acks,
        timeout,
        compression,
        topicMessages: [topicMessage]
      });
    };
    return {
      send,
      sendBatch
    };
  };
});

// node_modules/kafkajs/src/utils/swapObject.js
var require_swapObject = __commonJS((exports, module) => {
  var { keys } = Object;
  module.exports = (object) => keys(object).reduce((result, key) => ({ ...result, [object[key]]: key }), {});
});

// node_modules/kafkajs/src/producer/instrumentationEvents.js
var require_instrumentationEvents2 = __commonJS((exports, module) => {
  var swapObject = require_swapObject();
  var networkEvents = require_instrumentationEvents();
  var InstrumentationEventType = require_eventType();
  var producerType = InstrumentationEventType("producer");
  var events = {
    CONNECT: producerType("connect"),
    DISCONNECT: producerType("disconnect"),
    REQUEST: producerType(networkEvents.NETWORK_REQUEST),
    REQUEST_TIMEOUT: producerType(networkEvents.NETWORK_REQUEST_TIMEOUT),
    REQUEST_QUEUE_SIZE: producerType(networkEvents.NETWORK_REQUEST_QUEUE_SIZE)
  };
  var wrappedEvents = {
    [events.REQUEST]: networkEvents.NETWORK_REQUEST,
    [events.REQUEST_TIMEOUT]: networkEvents.NETWORK_REQUEST_TIMEOUT,
    [events.REQUEST_QUEUE_SIZE]: networkEvents.NETWORK_REQUEST_QUEUE_SIZE
  };
  var reversedWrappedEvents = swapObject(wrappedEvents);
  var unwrap = (eventName) => wrappedEvents[eventName] || eventName;
  var wrap = (eventName) => reversedWrappedEvents[eventName] || eventName;
  module.exports = {
    events,
    wrap,
    unwrap
  };
});

// node_modules/kafkajs/src/producer/index.js
var require_producer = __commonJS((exports, module) => {
  var createRetry = require_retry();
  var { CONNECTION_STATUS } = require_connectionStatus();
  var { DefaultPartitioner } = require_partitioners();
  var InstrumentationEventEmitter = require_emitter();
  var createEosManager = require_eosManager();
  var createMessageProducer = require_messageProducer();
  var { events, wrap: wrapEvent, unwrap: unwrapEvent } = require_instrumentationEvents2();
  var { KafkaJSNonRetriableError } = require_errors();
  var { values, keys } = Object;
  var eventNames = values(events);
  var eventKeys = keys(events).map((key) => `producer.events.${key}`).join(", ");
  var { CONNECT, DISCONNECT } = events;
  module.exports = ({
    cluster,
    logger: rootLogger,
    createPartitioner = DefaultPartitioner,
    retry,
    idempotent = false,
    transactionalId,
    transactionTimeout,
    instrumentationEmitter: rootInstrumentationEmitter
  }) => {
    let connectionStatus = CONNECTION_STATUS.DISCONNECTED;
    retry = retry || { retries: idempotent ? Number.MAX_SAFE_INTEGER : 5 };
    if (idempotent && retry.retries < 1) {
      throw new KafkaJSNonRetriableError("Idempotent producer must allow retries to protect against transient errors");
    }
    const logger = rootLogger.namespace("Producer");
    if (idempotent && retry.retries < Number.MAX_SAFE_INTEGER) {
      logger.warn("Limiting retries for the idempotent producer may invalidate EoS guarantees");
    }
    const partitioner = createPartitioner();
    const retrier = createRetry(Object.assign({}, cluster.retry, retry));
    const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter;
    const idempotentEosManager = createEosManager({
      logger,
      cluster,
      transactionTimeout,
      transactional: false,
      transactionalId
    });
    const { send, sendBatch } = createMessageProducer({
      logger,
      cluster,
      partitioner,
      eosManager: idempotentEosManager,
      idempotent,
      retrier,
      getConnectionStatus: () => connectionStatus
    });
    let transactionalEosManager;
    const on = (eventName, listener) => {
      if (!eventNames.includes(eventName)) {
        throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`);
      }
      return instrumentationEmitter.addListener(unwrapEvent(eventName), (event) => {
        event.type = wrapEvent(event.type);
        Promise.resolve(listener(event)).catch((e) => {
          logger.error(`Failed to execute listener: ${e.message}`, {
            eventName,
            stack: e.stack
          });
        });
      });
    };
    const transaction = async () => {
      if (!transactionalId) {
        throw new KafkaJSNonRetriableError("Must provide transactional id for transactional producer");
      }
      let transactionDidEnd = false;
      transactionalEosManager = transactionalEosManager || createEosManager({
        logger,
        cluster,
        transactionTimeout,
        transactional: true,
        transactionalId
      });
      if (transactionalEosManager.isInTransaction()) {
        throw new KafkaJSNonRetriableError("There is already an ongoing transaction for this producer. Please end the transaction before beginning another.");
      }
      if (!transactionalEosManager.isInitialized()) {
        await transactionalEosManager.initProducerId();
      }
      transactionalEosManager.beginTransaction();
      const { send: sendTxn, sendBatch: sendBatchTxn } = createMessageProducer({
        logger,
        cluster,
        partitioner,
        retrier,
        eosManager: transactionalEosManager,
        idempotent: true,
        getConnectionStatus: () => connectionStatus
      });
      const isActive = () => transactionalEosManager.isInTransaction() && !transactionDidEnd;
      const transactionGuard = (fn) => (...args) => {
        if (!isActive()) {
          return Promise.reject(new KafkaJSNonRetriableError("Cannot continue to use transaction once ended"));
        }
        return fn(...args);
      };
      return {
        sendBatch: transactionGuard(sendBatchTxn),
        send: transactionGuard(sendTxn),
        abort: transactionGuard(async () => {
          await transactionalEosManager.abort();
          transactionDidEnd = true;
        }),
        commit: transactionGuard(async () => {
          await transactionalEosManager.commit();
          transactionDidEnd = true;
        }),
        sendOffsets: transactionGuard(async ({ consumerGroupId, topics }) => {
          await transactionalEosManager.sendOffsets({ consumerGroupId, topics });
          for (const topicOffsets of topics) {
            const { topic, partitions } = topicOffsets;
            for (const { partition, offset } of partitions) {
              cluster.markOffsetAsCommitted({
                groupId: consumerGroupId,
                topic,
                partition,
                offset
              });
            }
          }
        }),
        isActive
      };
    };
    const getLogger = () => logger;
    return {
      connect: async () => {
        await cluster.connect();
        connectionStatus = CONNECTION_STATUS.CONNECTED;
        instrumentationEmitter.emit(CONNECT);
        if (idempotent && !idempotentEosManager.isInitialized()) {
          await idempotentEosManager.initProducerId();
        }
      },
      disconnect: async () => {
        connectionStatus = CONNECTION_STATUS.DISCONNECTING;
        await cluster.disconnect();
        connectionStatus = CONNECTION_STATUS.DISCONNECTED;
        instrumentationEmitter.emit(DISCONNECT);
      },
      isIdempotent: () => {
        return idempotent;
      },
      events,
      on,
      send,
      sendBatch,
      transaction,
      logger: getLogger
    };
  };
});

// node_modules/kafkajs/src/utils/sleep.js
var require_sleep = __commonJS((exports, module) => {
  module.exports = (timeInMs) => new Promise((resolve) => {
    setTimeout(resolve, timeInMs);
  });
});

// node_modules/kafkajs/src/consumer/offsetManager/isInvalidOffset.js
var require_isInvalidOffset = __commonJS((exports, module) => {
  var Long = require_long();
  module.exports = (offset) => !offset && offset !== 0 || Long.fromValue(offset).isNegative();
});

// node_modules/kafkajs/src/consumer/offsetManager/initializeConsumerOffsets.js
var require_initializeConsumerOffsets = __commonJS((exports, module) => {
  var isInvalidOffset = require_isInvalidOffset();
  var { keys, assign } = Object;
  var indexPartitions = (obj, { partition, offset }) => assign(obj, { [partition]: offset });
  var indexTopics = (obj, { topic, partitions }) => assign(obj, { [topic]: partitions.reduce(indexPartitions, {}) });
  module.exports = (consumerOffsets, topicOffsets) => {
    const indexedConsumerOffsets = consumerOffsets.reduce(indexTopics, {});
    const indexedTopicOffsets = topicOffsets.reduce(indexTopics, {});
    return keys(indexedConsumerOffsets).map((topic) => {
      const partitions = indexedConsumerOffsets[topic];
      return {
        topic,
        partitions: keys(partitions).map((partition) => {
          const offset = partitions[partition];
          const resolvedOffset = isInvalidOffset(offset) ? indexedTopicOffsets[topic][partition] : offset;
          return { partition: Number(partition), offset: resolvedOffset };
        })
      };
    });
  };
});

// node_modules/kafkajs/src/consumer/instrumentationEvents.js
var require_instrumentationEvents3 = __commonJS((exports, module) => {
  var swapObject = require_swapObject();
  var InstrumentationEventType = require_eventType();
  var networkEvents = require_instrumentationEvents();
  var consumerType = InstrumentationEventType("consumer");
  var events = {
    HEARTBEAT: consumerType("heartbeat"),
    COMMIT_OFFSETS: consumerType("commit_offsets"),
    GROUP_JOIN: consumerType("group_join"),
    FETCH: consumerType("fetch"),
    FETCH_START: consumerType("fetch_start"),
    START_BATCH_PROCESS: consumerType("start_batch_process"),
    END_BATCH_PROCESS: consumerType("end_batch_process"),
    CONNECT: consumerType("connect"),
    DISCONNECT: consumerType("disconnect"),
    STOP: consumerType("stop"),
    CRASH: consumerType("crash"),
    REBALANCING: consumerType("rebalancing"),
    RECEIVED_UNSUBSCRIBED_TOPICS: consumerType("received_unsubscribed_topics"),
    REQUEST: consumerType(networkEvents.NETWORK_REQUEST),
    REQUEST_TIMEOUT: consumerType(networkEvents.NETWORK_REQUEST_TIMEOUT),
    REQUEST_QUEUE_SIZE: consumerType(networkEvents.NETWORK_REQUEST_QUEUE_SIZE)
  };
  var wrappedEvents = {
    [events.REQUEST]: networkEvents.NETWORK_REQUEST,
    [events.REQUEST_TIMEOUT]: networkEvents.NETWORK_REQUEST_TIMEOUT,
    [events.REQUEST_QUEUE_SIZE]: networkEvents.NETWORK_REQUEST_QUEUE_SIZE
  };
  var reversedWrappedEvents = swapObject(wrappedEvents);
  var unwrap = (eventName) => wrappedEvents[eventName] || eventName;
  var wrap = (eventName) => reversedWrappedEvents[eventName] || eventName;
  module.exports = {
    events,
    wrap,
    unwrap
  };
});

// node_modules/kafkajs/src/consumer/offsetManager/index.js
var require_offsetManager = __commonJS((exports, module) => {
  var Long = require_long();
  var isInvalidOffset = require_isInvalidOffset();
  var initializeConsumerOffsets = require_initializeConsumerOffsets();
  var {
    events: { COMMIT_OFFSETS }
  } = require_instrumentationEvents3();
  var { keys, assign } = Object;
  var indexTopics = (topics) => topics.reduce((obj, topic) => assign(obj, { [topic]: {} }), {});
  var PRIVATE = {
    COMMITTED_OFFSETS: Symbol("private:OffsetManager:committedOffsets")
  };
  module.exports = class OffsetManager {
    constructor({
      cluster,
      coordinator,
      memberAssignment,
      autoCommit,
      autoCommitInterval,
      autoCommitThreshold,
      topicConfigurations,
      instrumentationEmitter,
      groupId,
      generationId,
      memberId
    }) {
      this.cluster = cluster;
      this.coordinator = coordinator;
      this.memberAssignment = memberAssignment;
      this.topicConfigurations = topicConfigurations;
      this.instrumentationEmitter = instrumentationEmitter;
      this.groupId = groupId;
      this.generationId = generationId;
      this.memberId = memberId;
      this.autoCommit = autoCommit;
      this.autoCommitInterval = autoCommitInterval;
      this.autoCommitThreshold = autoCommitThreshold;
      this.lastCommit = Date.now();
      this.topics = keys(memberAssignment);
      this.clearAllOffsets();
    }
    nextOffset(topic, partition) {
      if (!this.resolvedOffsets[topic][partition]) {
        this.resolvedOffsets[topic][partition] = this.committedOffsets()[topic][partition];
      }
      let offset = this.resolvedOffsets[topic][partition];
      if (isInvalidOffset(offset)) {
        offset = "0";
      }
      return Long.fromValue(offset);
    }
    async getCoordinator() {
      if (!this.coordinator.isConnected()) {
        this.coordinator = await this.cluster.findBroker(this.coordinator);
      }
      return this.coordinator;
    }
    resetOffset({ topic, partition }) {
      this.resolvedOffsets[topic][partition] = this.committedOffsets()[topic][partition];
    }
    resolveOffset({ topic, partition, offset }) {
      this.resolvedOffsets[topic][partition] = Long.fromValue(offset).add(1).toString();
    }
    countResolvedOffsets() {
      const committedOffsets = this.committedOffsets();
      const subtractOffsets = (resolvedOffset, committedOffset) => {
        const resolvedOffsetLong = Long.fromValue(resolvedOffset);
        return isInvalidOffset(committedOffset) ? resolvedOffsetLong : resolvedOffsetLong.subtract(Long.fromValue(committedOffset));
      };
      const subtractPartitionOffsets = (resolvedTopicOffsets, committedTopicOffsets) => keys(resolvedTopicOffsets).map((partition) => subtractOffsets(resolvedTopicOffsets[partition], committedTopicOffsets[partition]));
      const subtractTopicOffsets = (topic) => subtractPartitionOffsets(this.resolvedOffsets[topic], committedOffsets[topic]);
      const offsetsDiff = this.topics.flatMap(subtractTopicOffsets);
      return offsetsDiff.reduce((sum, offset) => sum.add(offset), Long.fromValue(0));
    }
    async setDefaultOffset({ topic, partition }) {
      const { groupId, generationId, memberId } = this;
      const defaultOffset = this.cluster.defaultOffset(this.topicConfigurations[topic]);
      const coordinator = await this.getCoordinator();
      await coordinator.offsetCommit({
        groupId,
        memberId,
        groupGenerationId: generationId,
        topics: [
          {
            topic,
            partitions: [{ partition, offset: defaultOffset }]
          }
        ]
      });
      this.clearOffsets({ topic, partition });
    }
    async seek({ topic, partition, offset }) {
      if (!this.memberAssignment[topic] || !this.memberAssignment[topic].includes(partition)) {
        return;
      }
      if (!this.autoCommit) {
        this.resolveOffset({
          topic,
          partition,
          offset: Long.fromValue(offset).subtract(1).toString()
        });
        return;
      }
      const { groupId, generationId, memberId } = this;
      const coordinator = await this.getCoordinator();
      await coordinator.offsetCommit({
        groupId,
        memberId,
        groupGenerationId: generationId,
        topics: [
          {
            topic,
            partitions: [{ partition, offset }]
          }
        ]
      });
      this.clearOffsets({ topic, partition });
    }
    async commitOffsetsIfNecessary() {
      const now = Date.now();
      const timeoutReached = this.autoCommitInterval != null && now >= this.lastCommit + this.autoCommitInterval;
      const thresholdReached = this.autoCommitThreshold != null && this.countResolvedOffsets().gte(Long.fromValue(this.autoCommitThreshold));
      if (timeoutReached || thresholdReached) {
        return this.commitOffsets();
      }
    }
    uncommittedOffsets() {
      const offsets = (topic) => keys(this.resolvedOffsets[topic]);
      const emptyPartitions = ({ partitions }) => partitions.length > 0;
      const toPartitions = (topic) => (partition) => ({
        partition,
        offset: this.resolvedOffsets[topic][partition]
      });
      const changedOffsets = (topic) => ({ partition, offset }) => {
        return offset !== this.committedOffsets()[topic][partition] && Long.fromValue(offset).greaterThanOrEqual(0);
      };
      const topicsWithPartitionsToCommit = this.topics.map((topic) => ({
        topic,
        partitions: offsets(topic).map(toPartitions(topic)).filter(changedOffsets(topic))
      })).filter(emptyPartitions);
      return { topics: topicsWithPartitionsToCommit };
    }
    async commitOffsets(offsets = {}) {
      const { groupId, generationId, memberId } = this;
      const { topics = this.uncommittedOffsets().topics } = offsets;
      if (topics.length === 0) {
        this.lastCommit = Date.now();
        return;
      }
      const payload = {
        groupId,
        memberId,
        groupGenerationId: generationId,
        topics
      };
      try {
        const coordinator = await this.getCoordinator();
        await coordinator.offsetCommit(payload);
        this.instrumentationEmitter.emit(COMMIT_OFFSETS, payload);
        topics.forEach(({ topic, partitions }) => {
          const updatedOffsets = partitions.reduce((obj, { partition, offset }) => assign(obj, { [partition]: offset }), {});
          this[PRIVATE.COMMITTED_OFFSETS][topic] = assign({}, this.committedOffsets()[topic], updatedOffsets);
        });
        this.lastCommit = Date.now();
      } catch (e) {
        if (e.type === "NOT_COORDINATOR_FOR_GROUP") {
          await this.cluster.refreshMetadata();
        }
        throw e;
      }
    }
    async resolveOffsets() {
      const { groupId } = this;
      const invalidOffset = (topic) => (partition) => {
        return isInvalidOffset(this.committedOffsets()[topic][partition]);
      };
      const pendingPartitions = this.topics.map((topic) => ({
        topic,
        partitions: this.memberAssignment[topic].filter(invalidOffset(topic)).map((partition) => ({ partition }))
      })).filter((t) => t.partitions.length > 0);
      if (pendingPartitions.length === 0) {
        return;
      }
      const coordinator = await this.getCoordinator();
      const { responses: consumerOffsets } = await coordinator.offsetFetch({
        groupId,
        topics: pendingPartitions
      });
      const unresolvedPartitions = consumerOffsets.map(({ topic, partitions }) => assign({
        topic,
        partitions: partitions.filter(({ offset }) => isInvalidOffset(offset)).map(({ partition }) => assign({ partition }))
      }, this.topicConfigurations[topic]));
      const indexPartitions = (obj, { partition, offset }) => {
        return assign(obj, { [partition]: offset });
      };
      const hasUnresolvedPartitions = () => unresolvedPartitions.some((t) => t.partitions.length > 0);
      let offsets = consumerOffsets;
      if (hasUnresolvedPartitions()) {
        const topicOffsets = await this.cluster.fetchTopicsOffset(unresolvedPartitions);
        offsets = initializeConsumerOffsets(consumerOffsets, topicOffsets);
      }
      offsets.forEach(({ topic, partitions }) => {
        this.committedOffsets()[topic] = partitions.reduce(indexPartitions, {
          ...this.committedOffsets()[topic]
        });
      });
    }
    clearOffsets({ topic, partition }) {
      delete this.committedOffsets()[topic][partition];
      delete this.resolvedOffsets[topic][partition];
    }
    clearAllOffsets() {
      const committedOffsets = this.committedOffsets();
      for (const topic in committedOffsets) {
        delete committedOffsets[topic];
      }
      for (const topic of this.topics) {
        committedOffsets[topic] = {};
      }
      this.resolvedOffsets = indexTopics(this.topics);
    }
    committedOffsets() {
      if (!this[PRIVATE.COMMITTED_OFFSETS]) {
        this[PRIVATE.COMMITTED_OFFSETS] = this.groupId ? this.cluster.committedOffsets({ groupId: this.groupId }) : {};
      }
      return this[PRIVATE.COMMITTED_OFFSETS];
    }
  };
});

// node_modules/kafkajs/src/consumer/filterAbortedMessages.js
var require_filterAbortedMessages = __commonJS((exports, module) => {
  var Long = require_long();
  var ABORTED_MESSAGE_KEY = Buffer.from([0, 0, 0, 0]);
  var isAbortMarker = ({ key }) => {
    if (!key)
      return false;
    return Buffer.from(key).equals(ABORTED_MESSAGE_KEY);
  };
  module.exports = ({ messages, abortedTransactions }) => {
    const currentAbortedTransactions = new Map;
    if (!abortedTransactions || !abortedTransactions.length) {
      return messages;
    }
    const remainingAbortedTransactions = [...abortedTransactions];
    return messages.filter((message) => {
      if (remainingAbortedTransactions.length && Long.fromValue(message.offset).gte(remainingAbortedTransactions[0].firstOffset)) {
        const { producerId: producerId2 } = remainingAbortedTransactions.shift();
        currentAbortedTransactions.set(producerId2, true);
      }
      const { producerId, inTransaction } = message.batchContext;
      if (isAbortMarker(message)) {
        currentAbortedTransactions.delete(producerId);
      } else if (currentAbortedTransactions.has(producerId) && inTransaction) {
        return false;
      }
      return true;
    });
  };
});

// node_modules/kafkajs/src/consumer/batch.js
var require_batch = __commonJS((exports, module) => {
  var Long = require_long();
  var filterAbortedMessages = require_filterAbortedMessages();
  module.exports = class Batch {
    constructor(topic, fetchedOffset, partitionData) {
      this.fetchedOffset = fetchedOffset;
      const longFetchedOffset = Long.fromValue(this.fetchedOffset);
      const { abortedTransactions, messages } = partitionData;
      this.topic = topic;
      this.partition = partitionData.partition;
      this.highWatermark = partitionData.highWatermark;
      this.rawMessages = messages;
      this.messagesWithinOffset = this.rawMessages.filter((message) => Long.fromValue(message.offset).gte(longFetchedOffset));
      this.messages = filterAbortedMessages({
        messages: this.messagesWithinOffset,
        abortedTransactions
      }).filter((message) => !message.isControlRecord);
    }
    isEmpty() {
      return this.messages.length === 0;
    }
    isEmptyIncludingFiltered() {
      return this.messagesWithinOffset.length === 0;
    }
    isEmptyDueToFiltering() {
      return this.isEmpty() && this.rawMessages.length > 0;
    }
    isEmptyControlRecord() {
      return this.isEmpty() && this.messagesWithinOffset.some(({ isControlRecord }) => isControlRecord);
    }
    isEmptyDueToLogCompactedMessages() {
      const hasMessages = this.rawMessages.length > 0;
      return hasMessages && this.isEmptyIncludingFiltered();
    }
    firstOffset() {
      return this.isEmptyIncludingFiltered() ? null : this.messagesWithinOffset[0].offset;
    }
    lastOffset() {
      if (this.isEmptyDueToLogCompactedMessages()) {
        return this.fetchedOffset;
      }
      if (this.isEmptyIncludingFiltered()) {
        return Long.fromValue(this.highWatermark).add(-1).toString();
      }
      return this.messagesWithinOffset[this.messagesWithinOffset.length - 1].offset;
    }
    offsetLag() {
      const lastOffsetOfPartition = Long.fromValue(this.highWatermark).add(-1);
      const lastConsumedOffset = Long.fromValue(this.lastOffset());
      return lastOffsetOfPartition.add(lastConsumedOffset.multiply(-1)).toString();
    }
    offsetLagLow() {
      if (this.isEmptyIncludingFiltered()) {
        return "0";
      }
      const lastOffsetOfPartition = Long.fromValue(this.highWatermark).add(-1);
      const firstConsumedOffset = Long.fromValue(this.firstOffset());
      return lastOffsetOfPartition.add(firstConsumedOffset.multiply(-1)).toString();
    }
  };
});

// node_modules/kafkajs/src/consumer/seekOffsets.js
var require_seekOffsets = __commonJS((exports, module) => {
  module.exports = class SeekOffsets extends Map {
    getKey(topic, partition) {
      return JSON.stringify([topic, partition]);
    }
    set(topic, partition, offset) {
      const key = this.getKey(topic, partition);
      super.set(key, offset);
    }
    has(topic, partition) {
      const key = this.getKey(topic, partition);
      return super.has(key);
    }
    pop(topic, partition) {
      if (this.size === 0 || !this.has(topic, partition)) {
        return;
      }
      const key = this.getKey(topic, partition);
      const offset = this.get(key);
      this.delete(key);
      return { topic, partition, offset };
    }
  };
});

// node_modules/kafkajs/src/consumer/subscriptionState.js
var require_subscriptionState = __commonJS((exports, module) => {
  var createState = (topic) => ({
    topic,
    paused: new Set,
    pauseAll: false,
    resumed: new Set
  });
  module.exports = class SubscriptionState {
    constructor() {
      this.assignedPartitionsByTopic = {};
      this.subscriptionStatesByTopic = {};
    }
    assign(topicPartitions = []) {
      this.assignedPartitionsByTopic = topicPartitions.reduce((assigned, { topic, partitions = [] }) => {
        return { ...assigned, [topic]: { topic, partitions } };
      }, {});
    }
    pause(topicPartitions = []) {
      topicPartitions.forEach(({ topic, partitions }) => {
        const state = this.subscriptionStatesByTopic[topic] || createState(topic);
        if (typeof partitions === "undefined") {
          state.paused.clear();
          state.resumed.clear();
          state.pauseAll = true;
        } else if (Array.isArray(partitions)) {
          partitions.forEach((partition) => {
            state.paused.add(partition);
            state.resumed.delete(partition);
          });
          state.pauseAll = false;
        }
        this.subscriptionStatesByTopic[topic] = state;
      });
    }
    resume(topicPartitions = []) {
      topicPartitions.forEach(({ topic, partitions }) => {
        const state = this.subscriptionStatesByTopic[topic] || createState(topic);
        if (typeof partitions === "undefined") {
          state.paused.clear();
          state.resumed.clear();
          state.pauseAll = false;
        } else if (Array.isArray(partitions)) {
          partitions.forEach((partition) => {
            state.paused.delete(partition);
            if (state.pauseAll) {
              state.resumed.add(partition);
            }
          });
        }
        this.subscriptionStatesByTopic[topic] = state;
      });
    }
    assigned() {
      return Object.values(this.assignedPartitionsByTopic).map(({ topic, partitions }) => ({
        topic,
        partitions: partitions.sort()
      }));
    }
    active() {
      return Object.values(this.assignedPartitionsByTopic).map(({ topic, partitions }) => ({
        topic,
        partitions: partitions.filter((partition) => !this.isPaused(topic, partition)).sort()
      }));
    }
    paused() {
      return Object.values(this.assignedPartitionsByTopic).map(({ topic, partitions }) => ({
        topic,
        partitions: partitions.filter((partition) => this.isPaused(topic, partition)).sort()
      })).filter(({ partitions }) => partitions.length !== 0);
    }
    isPaused(topic, partition) {
      const state = this.subscriptionStatesByTopic[topic];
      if (!state) {
        return false;
      }
      const partitionResumed = state.resumed.has(partition);
      const partitionPaused = state.paused.has(partition);
      return state.pauseAll && !partitionResumed || partitionPaused;
    }
  };
});

// node_modules/kafkajs/src/consumer/assignerProtocol.js
var require_assignerProtocol = __commonJS((exports, module) => {
  var Encoder = require_encoder();
  var Decoder = require_decoder();
  var MemberMetadata = {
    encode({ version, topics, userData = Buffer.alloc(0) }) {
      return new Encoder().writeInt16(version).writeArray(topics).writeBytes(userData).buffer;
    },
    decode(buffer) {
      const decoder = new Decoder(buffer);
      return {
        version: decoder.readInt16(),
        topics: decoder.readArray((d) => d.readString()),
        userData: decoder.readBytes()
      };
    }
  };
  var MemberAssignment = {
    encode({ version, assignment, userData = Buffer.alloc(0) }) {
      return new Encoder().writeInt16(version).writeArray(Object.keys(assignment).map((topic) => new Encoder().writeString(topic).writeArray(assignment[topic]))).writeBytes(userData).buffer;
    },
    decode(buffer) {
      const decoder = new Decoder(buffer);
      const decodePartitions = (d) => d.readInt32();
      const decodeAssignment = (d) => ({
        topic: d.readString(),
        partitions: d.readArray(decodePartitions)
      });
      const indexAssignment = (obj, { topic, partitions }) => Object.assign(obj, { [topic]: partitions });
      if (!decoder.canReadInt16()) {
        return null;
      }
      return {
        version: decoder.readInt16(),
        assignment: decoder.readArray(decodeAssignment).reduce(indexAssignment, {}),
        userData: decoder.readBytes()
      };
    }
  };
  module.exports = {
    MemberMetadata,
    MemberAssignment
  };
});

// node_modules/kafkajs/src/consumer/consumerGroup.js
var require_consumerGroup = __commonJS((exports, module) => {
  var sleep = require_sleep();
  var websiteUrl = require_websiteUrl();
  var arrayDiff = require_arrayDiff();
  var createRetry = require_retry();
  var sharedPromiseTo = require_sharedPromiseTo();
  var OffsetManager = require_offsetManager();
  var Batch = require_batch();
  var SeekOffsets = require_seekOffsets();
  var SubscriptionState = require_subscriptionState();
  var {
    events: { GROUP_JOIN, HEARTBEAT, CONNECT, RECEIVED_UNSUBSCRIBED_TOPICS }
  } = require_instrumentationEvents3();
  var { MemberAssignment } = require_assignerProtocol();
  var {
    KafkaJSError,
    KafkaJSNonRetriableError,
    KafkaJSStaleTopicMetadataAssignment,
    isRebalancing
  } = require_errors();
  var { keys } = Object;
  var STALE_METADATA_ERRORS = [
    "LEADER_NOT_AVAILABLE",
    "NOT_LEADER_FOR_PARTITION",
    "FENCED_LEADER_EPOCH",
    "UNKNOWN_LEADER_EPOCH",
    "UNKNOWN_TOPIC_OR_PARTITION"
  ];
  var PRIVATE = {
    JOIN: Symbol("private:ConsumerGroup:join"),
    SYNC: Symbol("private:ConsumerGroup:sync"),
    SHARED_HEARTBEAT: Symbol("private:ConsumerGroup:sharedHeartbeat")
  };
  module.exports = class ConsumerGroup {
    constructor({
      retry,
      cluster,
      groupId,
      topics,
      topicConfigurations,
      logger,
      instrumentationEmitter,
      assigners,
      sessionTimeout,
      rebalanceTimeout,
      maxBytesPerPartition,
      minBytes,
      maxBytes,
      maxWaitTimeInMs,
      autoCommit,
      autoCommitInterval,
      autoCommitThreshold,
      isolationLevel,
      rackId,
      metadataMaxAge
    }) {
      this.cluster = cluster;
      this.groupId = groupId;
      this.topics = topics;
      this.topicsSubscribed = topics;
      this.topicConfigurations = topicConfigurations;
      this.logger = logger.namespace("ConsumerGroup");
      this.instrumentationEmitter = instrumentationEmitter;
      this.retrier = createRetry(Object.assign({}, retry));
      this.assigners = assigners;
      this.sessionTimeout = sessionTimeout;
      this.rebalanceTimeout = rebalanceTimeout;
      this.maxBytesPerPartition = maxBytesPerPartition;
      this.minBytes = minBytes;
      this.maxBytes = maxBytes;
      this.maxWaitTime = maxWaitTimeInMs;
      this.autoCommit = autoCommit;
      this.autoCommitInterval = autoCommitInterval;
      this.autoCommitThreshold = autoCommitThreshold;
      this.isolationLevel = isolationLevel;
      this.rackId = rackId;
      this.metadataMaxAge = metadataMaxAge;
      this.seekOffset = new SeekOffsets;
      this.coordinator = null;
      this.generationId = null;
      this.leaderId = null;
      this.memberId = null;
      this.members = null;
      this.groupProtocol = null;
      this.partitionsPerSubscribedTopic = null;
      this.preferredReadReplicasPerTopicPartition = {};
      this.offsetManager = null;
      this.subscriptionState = new SubscriptionState;
      this.lastRequest = Date.now();
      this[PRIVATE.SHARED_HEARTBEAT] = sharedPromiseTo(async ({ interval }) => {
        const { groupId: groupId2, generationId, memberId } = this;
        const now = Date.now();
        if (memberId && now >= this.lastRequest + interval) {
          const payload = {
            groupId: groupId2,
            memberId,
            groupGenerationId: generationId
          };
          await this.coordinator.heartbeat(payload);
          this.instrumentationEmitter.emit(HEARTBEAT, payload);
          this.lastRequest = Date.now();
        }
      });
    }
    isLeader() {
      return this.leaderId && this.memberId === this.leaderId;
    }
    getNodeIds() {
      return this.cluster.getNodeIds();
    }
    async connect() {
      await this.cluster.connect();
      this.instrumentationEmitter.emit(CONNECT);
      await this.cluster.refreshMetadataIfNecessary();
    }
    async[PRIVATE.JOIN]() {
      const { groupId, sessionTimeout, rebalanceTimeout } = this;
      this.coordinator = await this.cluster.findGroupCoordinator({ groupId });
      const groupData = await this.coordinator.joinGroup({
        groupId,
        sessionTimeout,
        rebalanceTimeout,
        memberId: this.memberId || "",
        groupProtocols: this.assigners.map((assigner) => assigner.protocol({
          topics: this.topicsSubscribed
        }))
      });
      this.generationId = groupData.generationId;
      this.leaderId = groupData.leaderId;
      this.memberId = groupData.memberId;
      this.members = groupData.members;
      this.groupProtocol = groupData.groupProtocol;
    }
    async leave() {
      const { groupId, memberId } = this;
      if (memberId) {
        await this.coordinator.leaveGroup({ groupId, memberId });
        this.memberId = null;
      }
    }
    async[PRIVATE.SYNC]() {
      let assignment = [];
      const {
        groupId,
        generationId,
        memberId,
        members,
        groupProtocol,
        topics,
        topicsSubscribed,
        coordinator
      } = this;
      if (this.isLeader()) {
        this.logger.debug("Chosen as group leader", { groupId, generationId, memberId, topics });
        const assigner = this.assigners.find(({ name }) => name === groupProtocol);
        if (!assigner) {
          throw new KafkaJSNonRetriableError(`Unsupported partition assigner "${groupProtocol}", the assigner wasn't found in the assigners list`);
        }
        await this.cluster.refreshMetadata();
        assignment = await assigner.assign({ members, topics: topicsSubscribed });
        this.logger.debug("Group assignment", {
          groupId,
          generationId,
          groupProtocol,
          assignment,
          topics: topicsSubscribed
        });
      }
      this.partitionsPerSubscribedTopic = this.generatePartitionsPerSubscribedTopic();
      const { memberAssignment } = await this.coordinator.syncGroup({
        groupId,
        generationId,
        memberId,
        groupAssignment: assignment
      });
      const decodedMemberAssignment = MemberAssignment.decode(memberAssignment);
      const decodedAssignment = decodedMemberAssignment != null ? decodedMemberAssignment.assignment : {};
      this.logger.debug("Received assignment", {
        groupId,
        generationId,
        memberId,
        memberAssignment: decodedAssignment
      });
      const assignedTopics = keys(decodedAssignment);
      const topicsNotSubscribed = arrayDiff(assignedTopics, topicsSubscribed);
      if (topicsNotSubscribed.length > 0) {
        const payload = {
          groupId,
          generationId,
          memberId,
          assignedTopics,
          topicsSubscribed,
          topicsNotSubscribed
        };
        this.instrumentationEmitter.emit(RECEIVED_UNSUBSCRIBED_TOPICS, payload);
        this.logger.warn("Consumer group received unsubscribed topics", {
          ...payload,
          helpUrl: websiteUrl("docs/faq", "why-am-i-receiving-messages-for-topics-i-m-not-subscribed-to")
        });
      }
      const safeAssignment = arrayDiff(assignedTopics, topicsNotSubscribed);
      const currentMemberAssignment = safeAssignment.map((topic) => ({
        topic,
        partitions: decodedAssignment[topic]
      }));
      for (const assignment2 of currentMemberAssignment) {
        const { topic, partitions: assignedPartitions } = assignment2;
        const knownPartitions = this.partitionsPerSubscribedTopic.get(topic);
        const isAwareOfAllAssignedPartitions = assignedPartitions.every((partition) => knownPartitions.includes(partition));
        if (!isAwareOfAllAssignedPartitions) {
          this.logger.warn("Consumer is not aware of all assigned partitions, refreshing metadata", {
            groupId,
            generationId,
            memberId,
            topic,
            knownPartitions,
            assignedPartitions
          });
          await this.cluster.refreshMetadata();
          this.partitionsPerSubscribedTopic = this.generatePartitionsPerSubscribedTopic();
          break;
        }
      }
      this.topics = currentMemberAssignment.map(({ topic }) => topic);
      this.subscriptionState.assign(currentMemberAssignment);
      this.offsetManager = new OffsetManager({
        cluster: this.cluster,
        topicConfigurations: this.topicConfigurations,
        instrumentationEmitter: this.instrumentationEmitter,
        memberAssignment: currentMemberAssignment.reduce((partitionsByTopic, { topic, partitions }) => ({
          ...partitionsByTopic,
          [topic]: partitions
        }), {}),
        autoCommit: this.autoCommit,
        autoCommitInterval: this.autoCommitInterval,
        autoCommitThreshold: this.autoCommitThreshold,
        coordinator,
        groupId,
        generationId,
        memberId
      });
    }
    joinAndSync() {
      const startJoin = Date.now();
      return this.retrier(async (bail) => {
        try {
          await this[PRIVATE.JOIN]();
          await this[PRIVATE.SYNC]();
          const memberAssignment = this.assigned().reduce((result, { topic, partitions }) => ({ ...result, [topic]: partitions }), {});
          const payload = {
            groupId: this.groupId,
            memberId: this.memberId,
            leaderId: this.leaderId,
            isLeader: this.isLeader(),
            memberAssignment,
            groupProtocol: this.groupProtocol,
            duration: Date.now() - startJoin
          };
          this.instrumentationEmitter.emit(GROUP_JOIN, payload);
          this.logger.info("Consumer has joined the group", payload);
        } catch (e) {
          if (isRebalancing(e)) {
            throw new KafkaJSError(e);
          }
          if (e.type === "UNKNOWN_MEMBER_ID") {
            this.memberId = null;
            throw new KafkaJSError(e);
          }
          bail(e);
        }
      });
    }
    resetOffset({ topic, partition }) {
      this.offsetManager.resetOffset({ topic, partition });
    }
    resolveOffset({ topic, partition, offset }) {
      this.offsetManager.resolveOffset({ topic, partition, offset });
    }
    seek({ topic, partition, offset }) {
      this.seekOffset.set(topic, partition, offset);
    }
    pause(topicPartitions) {
      this.logger.info(`Pausing fetching from ${topicPartitions.length} topics`, {
        topicPartitions
      });
      this.subscriptionState.pause(topicPartitions);
    }
    resume(topicPartitions) {
      this.logger.info(`Resuming fetching from ${topicPartitions.length} topics`, {
        topicPartitions
      });
      this.subscriptionState.resume(topicPartitions);
    }
    assigned() {
      return this.subscriptionState.assigned();
    }
    paused() {
      return this.subscriptionState.paused();
    }
    isPaused(topic, partition) {
      return this.subscriptionState.isPaused(topic, partition);
    }
    async commitOffsetsIfNecessary() {
      await this.offsetManager.commitOffsetsIfNecessary();
    }
    async commitOffsets(offsets) {
      await this.offsetManager.commitOffsets(offsets);
    }
    uncommittedOffsets() {
      return this.offsetManager.uncommittedOffsets();
    }
    async heartbeat({ interval }) {
      return this[PRIVATE.SHARED_HEARTBEAT]({ interval });
    }
    async fetch(nodeId) {
      try {
        await this.cluster.refreshMetadataIfNecessary();
        this.checkForStaleAssignment();
        let topicPartitions = this.subscriptionState.assigned();
        topicPartitions = this.filterPartitionsByNode(nodeId, topicPartitions);
        await this.seekOffsets(topicPartitions);
        const committedOffsets = this.offsetManager.committedOffsets();
        const activeTopicPartitions = this.getActiveTopicPartitions();
        const requests = topicPartitions.map(({ topic, partitions }) => ({
          topic,
          partitions: partitions.filter((partition) => committedOffsets[topic][partition] != null && activeTopicPartitions[topic].has(partition)).map((partition) => ({
            partition,
            fetchOffset: this.offsetManager.nextOffset(topic, partition).toString(),
            maxBytes: this.maxBytesPerPartition
          }))
        })).filter(({ partitions }) => partitions.length);
        if (!requests.length) {
          await sleep(this.maxWaitTime);
          return [];
        }
        const broker = await this.cluster.findBroker({ nodeId });
        const { responses } = await broker.fetch({
          maxWaitTime: this.maxWaitTime,
          minBytes: this.minBytes,
          maxBytes: this.maxBytes,
          isolationLevel: this.isolationLevel,
          topics: requests,
          rackId: this.rackId
        });
        return responses.flatMap(({ topicName, partitions }) => {
          const topicRequestData = requests.find(({ topic }) => topic === topicName);
          let preferredReadReplicas = this.preferredReadReplicasPerTopicPartition[topicName];
          if (!preferredReadReplicas) {
            this.preferredReadReplicasPerTopicPartition[topicName] = preferredReadReplicas = {};
          }
          return partitions.filter(({ partition }) => !this.seekOffset.has(topicName, partition) && !this.subscriptionState.isPaused(topicName, partition)).map((partitionData) => {
            const { partition, preferredReadReplica } = partitionData;
            if (preferredReadReplica != null && preferredReadReplica !== -1) {
              const { nodeId: currentPreferredReadReplica } = preferredReadReplicas[partition] || {};
              if (currentPreferredReadReplica !== preferredReadReplica) {
                this.logger.info(`Preferred read replica is now ${preferredReadReplica}`, {
                  groupId: this.groupId,
                  memberId: this.memberId,
                  topic: topicName,
                  partition
                });
              }
              preferredReadReplicas[partition] = {
                nodeId: preferredReadReplica,
                expireAt: Date.now() + this.metadataMaxAge
              };
            }
            const partitionRequestData = topicRequestData.partitions.find(({ partition: partition2 }) => partition2 === partitionData.partition);
            const fetchedOffset = partitionRequestData.fetchOffset;
            return new Batch(topicName, fetchedOffset, partitionData);
          });
        });
      } catch (e) {
        await this.recoverFromFetch(e);
        return [];
      }
    }
    async recoverFromFetch(e) {
      if (STALE_METADATA_ERRORS.includes(e.type) || e.name === "KafkaJSTopicMetadataNotLoaded") {
        this.logger.debug("Stale cluster metadata, refreshing...", {
          groupId: this.groupId,
          memberId: this.memberId,
          error: e.message
        });
        await this.cluster.refreshMetadata();
        await this.joinAndSync();
        return;
      }
      if (e.name === "KafkaJSStaleTopicMetadataAssignment") {
        this.logger.warn(`${e.message}, resync group`, {
          groupId: this.groupId,
          memberId: this.memberId,
          topic: e.topic,
          unknownPartitions: e.unknownPartitions
        });
        await this.joinAndSync();
        return;
      }
      if (e.name === "KafkaJSOffsetOutOfRange") {
        await this.recoverFromOffsetOutOfRange(e);
        return;
      }
      if (e.name === "KafkaJSConnectionClosedError") {
        this.cluster.removeBroker({ host: e.host, port: e.port });
        return;
      }
      if (e.name === "KafkaJSBrokerNotFound" || e.name === "KafkaJSConnectionClosedError") {
        this.logger.debug(`${e.message}, refreshing metadata and retrying...`);
        await this.cluster.refreshMetadata();
        return;
      }
      throw e;
    }
    async recoverFromOffsetOutOfRange(e) {
      const preferredReadReplicas = this.preferredReadReplicasPerTopicPartition[e.topic];
      if (preferredReadReplicas && typeof preferredReadReplicas[e.partition] === "number") {
        this.logger.info("Offset out of range while fetching from follower, retrying with leader", {
          topic: e.topic,
          partition: e.partition,
          groupId: this.groupId,
          memberId: this.memberId
        });
        delete preferredReadReplicas[e.partition];
      } else {
        this.logger.error("Offset out of range, resetting to default offset", {
          topic: e.topic,
          partition: e.partition,
          groupId: this.groupId,
          memberId: this.memberId
        });
        await this.offsetManager.setDefaultOffset({
          topic: e.topic,
          partition: e.partition
        });
      }
    }
    generatePartitionsPerSubscribedTopic() {
      const map = new Map;
      for (const topic of this.topicsSubscribed) {
        const partitions = this.cluster.findTopicPartitionMetadata(topic).map((m) => m.partitionId).sort();
        map.set(topic, partitions);
      }
      return map;
    }
    checkForStaleAssignment() {
      if (!this.partitionsPerSubscribedTopic) {
        return;
      }
      const newPartitionsPerSubscribedTopic = this.generatePartitionsPerSubscribedTopic();
      for (const [topic, partitions] of newPartitionsPerSubscribedTopic) {
        const diff = arrayDiff(partitions, this.partitionsPerSubscribedTopic.get(topic));
        if (diff.length > 0) {
          throw new KafkaJSStaleTopicMetadataAssignment("Topic has been updated", {
            topic,
            unknownPartitions: diff
          });
        }
      }
    }
    async seekOffsets(topicPartitions) {
      for (const { topic, partitions } of topicPartitions) {
        for (const partition of partitions) {
          const seekEntry = this.seekOffset.pop(topic, partition);
          if (!seekEntry) {
            continue;
          }
          this.logger.debug("Seek offset", {
            groupId: this.groupId,
            memberId: this.memberId,
            seek: seekEntry
          });
          await this.offsetManager.seek(seekEntry);
        }
      }
      await this.offsetManager.resolveOffsets();
    }
    hasSeekOffset({ topic, partition }) {
      return this.seekOffset.has(topic, partition);
    }
    findReadReplicaForPartitions(topic, partitions) {
      const partitionMetadata = this.cluster.findTopicPartitionMetadata(topic);
      const preferredReadReplicas = this.preferredReadReplicasPerTopicPartition[topic];
      return partitions.reduce((result, id) => {
        const partitionId = parseInt(id, 10);
        const metadata = partitionMetadata.find((p) => p.partitionId === partitionId);
        if (!metadata) {
          return result;
        }
        if (metadata.leader == null) {
          throw new KafkaJSError("Invalid partition metadata", { topic, partitionId, metadata });
        }
        let nodeId = metadata.leader;
        if (preferredReadReplicas) {
          const { nodeId: preferredReadReplica, expireAt } = preferredReadReplicas[partitionId] || {};
          if (Date.now() >= expireAt) {
            this.logger.debug("Preferred read replica information has expired, using leader", {
              topic,
              partitionId,
              groupId: this.groupId,
              memberId: this.memberId,
              preferredReadReplica,
              leader: metadata.leader
            });
            delete preferredReadReplicas[partitionId];
          } else if (preferredReadReplica != null) {
            const offlineReplicas = metadata.offlineReplicas;
            if (Array.isArray(offlineReplicas) && offlineReplicas.includes(nodeId)) {
              this.logger.debug("Preferred read replica is offline, using leader", {
                topic,
                partitionId,
                groupId: this.groupId,
                memberId: this.memberId,
                preferredReadReplica,
                leader: metadata.leader
              });
            } else {
              nodeId = preferredReadReplica;
            }
          }
        }
        const current = result[nodeId] || [];
        return { ...result, [nodeId]: [...current, partitionId] };
      }, {});
    }
    filterPartitionsByNode(nodeId, topicPartitions) {
      return topicPartitions.map(({ topic, partitions }) => ({
        topic,
        partitions: this.findReadReplicaForPartitions(topic, partitions)[nodeId] || []
      }));
    }
    getActiveTopicPartitions() {
      const activeSubscriptionState = this.subscriptionState.active();
      const activeTopicPartitions = {};
      activeSubscriptionState.forEach(({ topic, partitions }) => {
        activeTopicPartitions[topic] = new Set(partitions);
      });
      return activeTopicPartitions;
    }
  };
});

// node_modules/kafkajs/src/utils/seq.js
var require_seq = __commonJS((exports, module) => {
  var seq = (count, callback = (x) => x) => new Array(count).fill(0).map((_, index) => callback(index));
  module.exports = seq;
});

// node_modules/kafkajs/src/consumer/fetcher.js
var require_fetcher = __commonJS((exports, module) => {
  var EventEmitter = import.meta.require("events");
  var createFetcher = ({
    nodeId,
    workerQueue,
    partitionAssignments,
    fetch: fetch2,
    logger: rootLogger
  }) => {
    const logger = rootLogger.namespace(`Fetcher ${nodeId}`);
    const emitter = new EventEmitter;
    let isRunning = false;
    const getWorkerQueue = () => workerQueue;
    const assignmentKey = ({ topic, partition }) => `${topic}|${partition}`;
    const getAssignedFetcher = (batch) => partitionAssignments.get(assignmentKey(batch));
    const assignTopicPartition = (batch) => partitionAssignments.set(assignmentKey(batch), nodeId);
    const unassignTopicPartition = (batch) => partitionAssignments.delete(assignmentKey(batch));
    const filterUnassignedBatches = (batches) => batches.filter((batch) => {
      const assignedFetcher = getAssignedFetcher(batch);
      if (assignedFetcher != null && assignedFetcher !== nodeId) {
        logger.info("Filtering out batch due to partition already being processed by another fetcher", {
          topic: batch.topic,
          partition: batch.partition,
          assignedFetcher,
          fetcher: nodeId
        });
        return false;
      }
      return true;
    });
    const start = async () => {
      if (isRunning)
        return;
      isRunning = true;
      while (isRunning) {
        try {
          const batches = await fetch2(nodeId);
          if (isRunning) {
            const availableBatches = filterUnassignedBatches(batches);
            if (availableBatches.length > 0) {
              availableBatches.forEach(assignTopicPartition);
              try {
                await workerQueue.push(...availableBatches);
              } finally {
                availableBatches.forEach(unassignTopicPartition);
              }
            }
          }
        } catch (error) {
          isRunning = false;
          emitter.emit("end");
          throw error;
        }
      }
      emitter.emit("end");
    };
    const stop = async () => {
      if (!isRunning)
        return;
      isRunning = false;
      await new Promise((resolve) => emitter.once("end", () => resolve()));
    };
    return { start, stop, getWorkerQueue };
  };
  module.exports = createFetcher;
});

// node_modules/kafkajs/src/consumer/worker.js
var require_worker = __commonJS((exports, module) => {
  var sharedPromiseTo = require_sharedPromiseTo();
  var createWorker = ({ handler, workerId }) => {
    const run = sharedPromiseTo(async ({ next }) => {
      while (true) {
        const item = next();
        if (!item)
          break;
        const { batch, resolve, reject } = item;
        try {
          await handler(batch, { workerId });
          resolve();
        } catch (error) {
          reject(error);
        }
      }
    });
    return { run };
  };
  module.exports = createWorker;
});

// node_modules/kafkajs/src/consumer/workerQueue.js
var require_workerQueue = __commonJS((exports, module) => {
  var createWorkerQueue = ({ workers }) => {
    const queue = [];
    const getWorkers = () => workers;
    const push = async (...batches) => {
      const promises = batches.map((batch) => new Promise((resolve, reject) => queue.push({ batch, resolve, reject })));
      workers.forEach((worker) => worker.run({ next: () => queue.shift() }));
      const results = await Promise.allSettled(promises);
      const rejected = results.find((result) => result.status === "rejected");
      if (rejected) {
        throw rejected.reason;
      }
    };
    return { push, getWorkers };
  };
  module.exports = createWorkerQueue;
});

// node_modules/kafkajs/src/consumer/fetchManager.js
var require_fetchManager = __commonJS((exports, module) => {
  var seq = require_seq();
  var createFetcher = require_fetcher();
  var createWorker = require_worker();
  var createWorkerQueue = require_workerQueue();
  var { KafkaJSFetcherRebalanceError, KafkaJSNoBrokerAvailableError } = require_errors();
  var createFetchManager = ({
    logger: rootLogger,
    getNodeIds,
    fetch: fetch2,
    handler,
    concurrency = 1
  }) => {
    const logger = rootLogger.namespace("FetchManager");
    const workers = seq(concurrency, (workerId) => createWorker({ handler, workerId }));
    const workerQueue = createWorkerQueue({ workers });
    let fetchers = [];
    const getFetchers = () => fetchers;
    const createFetchers = () => {
      const nodeIds = getNodeIds();
      const partitionAssignments = new Map;
      if (nodeIds.length === 0) {
        throw new KafkaJSNoBrokerAvailableError;
      }
      const validateShouldRebalance = () => {
        const current = getNodeIds();
        const hasChanged = nodeIds.length !== current.length || nodeIds.some((nodeId) => !current.includes(nodeId));
        if (hasChanged && current.length !== 0) {
          throw new KafkaJSFetcherRebalanceError;
        }
      };
      const fetchers2 = nodeIds.map((nodeId) => createFetcher({
        nodeId,
        workerQueue,
        partitionAssignments,
        fetch: async (nodeId2) => {
          validateShouldRebalance();
          return fetch2(nodeId2);
        },
        logger
      }));
      logger.debug(`Created ${fetchers2.length} fetchers`, { nodeIds, concurrency });
      return fetchers2;
    };
    const start = async () => {
      logger.debug("Starting...");
      while (true) {
        fetchers = createFetchers();
        try {
          await Promise.all(fetchers.map((fetcher) => fetcher.start()));
        } catch (error) {
          await stop();
          if (error instanceof KafkaJSFetcherRebalanceError) {
            logger.debug("Rebalancing fetchers...");
            continue;
          }
          throw error;
        }
        break;
      }
    };
    const stop = async () => {
      logger.debug("Stopping fetchers...");
      await Promise.all(fetchers.map((fetcher) => fetcher.stop()));
      logger.debug("Stopped fetchers");
    };
    return { start, stop, getFetchers };
  };
  module.exports = createFetchManager;
});

// node_modules/kafkajs/src/consumer/runner.js
var require_runner = __commonJS((exports, module) => {
  var { EventEmitter } = import.meta.require("events");
  var Long = require_long();
  var createRetry = require_retry();
  var { isKafkaJSError, isRebalancing } = require_errors();
  var {
    events: { FETCH, FETCH_START, START_BATCH_PROCESS, END_BATCH_PROCESS, REBALANCING }
  } = require_instrumentationEvents3();
  var createFetchManager = require_fetchManager();
  var isSameOffset = (offsetA, offsetB) => Long.fromValue(offsetA).equals(Long.fromValue(offsetB));
  var CONSUMING_START = "consuming-start";
  var CONSUMING_STOP = "consuming-stop";
  module.exports = class Runner extends EventEmitter {
    constructor({
      logger,
      consumerGroup,
      instrumentationEmitter,
      eachBatchAutoResolve = true,
      concurrency,
      eachBatch,
      eachMessage,
      heartbeatInterval,
      onCrash,
      retry,
      autoCommit = true
    }) {
      super();
      this.logger = logger.namespace("Runner");
      this.consumerGroup = consumerGroup;
      this.instrumentationEmitter = instrumentationEmitter;
      this.eachBatchAutoResolve = eachBatchAutoResolve;
      this.eachBatch = eachBatch;
      this.eachMessage = eachMessage;
      this.heartbeatInterval = heartbeatInterval;
      this.retrier = createRetry(Object.assign({}, retry));
      this.onCrash = onCrash;
      this.autoCommit = autoCommit;
      this.fetchManager = createFetchManager({
        logger: this.logger,
        getNodeIds: () => this.consumerGroup.getNodeIds(),
        fetch: (nodeId) => this.fetch(nodeId),
        handler: (batch) => this.handleBatch(batch),
        concurrency
      });
      this.running = false;
      this.consuming = false;
    }
    get consuming() {
      return this._consuming;
    }
    set consuming(value) {
      if (this._consuming !== value) {
        this._consuming = value;
        this.emit(value ? CONSUMING_START : CONSUMING_STOP);
      }
    }
    async start() {
      if (this.running) {
        return;
      }
      try {
        await this.consumerGroup.connect();
        await this.consumerGroup.joinAndSync();
      } catch (e) {
        return this.onCrash(e);
      }
      this.running = true;
      this.scheduleFetchManager();
    }
    scheduleFetchManager() {
      if (!this.running) {
        this.consuming = false;
        this.logger.info("consumer not running, exiting", {
          groupId: this.consumerGroup.groupId,
          memberId: this.consumerGroup.memberId
        });
        return;
      }
      this.consuming = true;
      this.retrier(async (bail, retryCount, retryTime) => {
        if (!this.running) {
          return;
        }
        try {
          await this.fetchManager.start();
        } catch (e) {
          if (isRebalancing(e)) {
            this.logger.warn("The group is rebalancing, re-joining", {
              groupId: this.consumerGroup.groupId,
              memberId: this.consumerGroup.memberId,
              error: e.message
            });
            this.instrumentationEmitter.emit(REBALANCING, {
              groupId: this.consumerGroup.groupId,
              memberId: this.consumerGroup.memberId
            });
            await this.consumerGroup.joinAndSync();
            return;
          }
          if (e.type === "UNKNOWN_MEMBER_ID") {
            this.logger.error("The coordinator is not aware of this member, re-joining the group", {
              groupId: this.consumerGroup.groupId,
              memberId: this.consumerGroup.memberId,
              error: e.message
            });
            this.consumerGroup.memberId = null;
            await this.consumerGroup.joinAndSync();
            return;
          }
          if (e.name === "KafkaJSNotImplemented") {
            return bail(e);
          }
          if (e.name === "KafkaJSNoBrokerAvailableError") {
            return bail(e);
          }
          this.logger.debug("Error while scheduling fetch manager, trying again...", {
            groupId: this.consumerGroup.groupId,
            memberId: this.consumerGroup.memberId,
            error: e.message,
            stack: e.stack,
            retryCount,
            retryTime
          });
          throw e;
        }
      }).then(() => {
        this.scheduleFetchManager();
      }).catch((e) => {
        this.onCrash(e);
        this.consuming = false;
        this.running = false;
      });
    }
    async stop() {
      if (!this.running) {
        return;
      }
      this.logger.debug("stop consumer group", {
        groupId: this.consumerGroup.groupId,
        memberId: this.consumerGroup.memberId
      });
      this.running = false;
      try {
        await this.fetchManager.stop();
        await this.waitForConsumer();
        await this.consumerGroup.leave();
      } catch (e) {
      }
    }
    waitForConsumer() {
      return new Promise((resolve) => {
        if (!this.consuming) {
          return resolve();
        }
        this.logger.debug("waiting for consumer to finish...", {
          groupId: this.consumerGroup.groupId,
          memberId: this.consumerGroup.memberId
        });
        this.once(CONSUMING_STOP, () => resolve());
      });
    }
    async heartbeat() {
      try {
        await this.consumerGroup.heartbeat({ interval: this.heartbeatInterval });
      } catch (e) {
        if (isRebalancing(e)) {
          await this.autoCommitOffsets();
        }
        throw e;
      }
    }
    async processEachMessage(batch) {
      const { topic, partition } = batch;
      const pause = () => {
        this.consumerGroup.pause([{ topic, partitions: [partition] }]);
        return () => this.consumerGroup.resume([{ topic, partitions: [partition] }]);
      };
      for (const message of batch.messages) {
        if (!this.running || this.consumerGroup.hasSeekOffset({ topic, partition })) {
          break;
        }
        try {
          await this.eachMessage({
            topic,
            partition,
            message,
            heartbeat: () => this.heartbeat(),
            pause
          });
        } catch (e) {
          if (!isKafkaJSError(e)) {
            this.logger.error(`Error when calling eachMessage`, {
              topic,
              partition,
              offset: message.offset,
              stack: e.stack,
              error: e
            });
          }
          await this.autoCommitOffsets();
          throw e;
        }
        this.consumerGroup.resolveOffset({ topic, partition, offset: message.offset });
        await this.heartbeat();
        await this.autoCommitOffsetsIfNecessary();
        if (this.consumerGroup.isPaused(topic, partition)) {
          break;
        }
      }
    }
    async processEachBatch(batch) {
      const { topic, partition } = batch;
      const lastFilteredMessage = batch.messages[batch.messages.length - 1];
      const pause = () => {
        this.consumerGroup.pause([{ topic, partitions: [partition] }]);
        return () => this.consumerGroup.resume([{ topic, partitions: [partition] }]);
      };
      try {
        await this.eachBatch({
          batch,
          resolveOffset: (offset) => {
            const offsetToResolve = lastFilteredMessage && isSameOffset(offset, lastFilteredMessage.offset) ? batch.lastOffset() : offset;
            this.consumerGroup.resolveOffset({ topic, partition, offset: offsetToResolve });
          },
          heartbeat: () => this.heartbeat(),
          pause,
          commitOffsetsIfNecessary: async (offsets) => {
            return offsets ? this.consumerGroup.commitOffsets(offsets) : this.consumerGroup.commitOffsetsIfNecessary();
          },
          uncommittedOffsets: () => this.consumerGroup.uncommittedOffsets(),
          isRunning: () => this.running,
          isStale: () => this.consumerGroup.hasSeekOffset({ topic, partition })
        });
      } catch (e) {
        if (!isKafkaJSError(e)) {
          this.logger.error(`Error when calling eachBatch`, {
            topic,
            partition,
            offset: batch.firstOffset(),
            stack: e.stack,
            error: e
          });
        }
        await this.autoCommitOffsets();
        throw e;
      }
      if (this.eachBatchAutoResolve) {
        this.consumerGroup.resolveOffset({ topic, partition, offset: batch.lastOffset() });
      }
    }
    async fetch(nodeId) {
      if (!this.running) {
        this.logger.debug("consumer not running, exiting", {
          groupId: this.consumerGroup.groupId,
          memberId: this.consumerGroup.memberId
        });
        return [];
      }
      const startFetch = Date.now();
      this.instrumentationEmitter.emit(FETCH_START, { nodeId });
      const batches = await this.consumerGroup.fetch(nodeId);
      this.instrumentationEmitter.emit(FETCH, {
        numberOfBatches: 0,
        duration: Date.now() - startFetch,
        nodeId
      });
      if (batches.length === 0) {
        await this.heartbeat();
      }
      return batches;
    }
    async handleBatch(batch) {
      if (!this.running) {
        this.logger.debug("consumer not running, exiting", {
          groupId: this.consumerGroup.groupId,
          memberId: this.consumerGroup.memberId
        });
        return;
      }
      const onBatch = async (batch2) => {
        const startBatchProcess = Date.now();
        const payload = {
          topic: batch2.topic,
          partition: batch2.partition,
          highWatermark: batch2.highWatermark,
          offsetLag: batch2.offsetLag(),
          offsetLagLow: batch2.offsetLagLow(),
          batchSize: batch2.messages.length,
          firstOffset: batch2.firstOffset(),
          lastOffset: batch2.lastOffset()
        };
        if (batch2.isEmptyDueToFiltering()) {
          this.instrumentationEmitter.emit(START_BATCH_PROCESS, payload);
          this.consumerGroup.resolveOffset({
            topic: batch2.topic,
            partition: batch2.partition,
            offset: batch2.lastOffset()
          });
          await this.autoCommitOffsetsIfNecessary();
          this.instrumentationEmitter.emit(END_BATCH_PROCESS, {
            ...payload,
            duration: Date.now() - startBatchProcess
          });
          await this.heartbeat();
          return;
        }
        if (batch2.isEmpty()) {
          await this.heartbeat();
          return;
        }
        this.instrumentationEmitter.emit(START_BATCH_PROCESS, payload);
        if (this.eachMessage) {
          await this.processEachMessage(batch2);
        } else if (this.eachBatch) {
          await this.processEachBatch(batch2);
        }
        this.instrumentationEmitter.emit(END_BATCH_PROCESS, {
          ...payload,
          duration: Date.now() - startBatchProcess
        });
        await this.autoCommitOffsets();
        await this.heartbeat();
      };
      await onBatch(batch);
    }
    autoCommitOffsets() {
      if (this.autoCommit) {
        return this.consumerGroup.commitOffsets();
      }
    }
    autoCommitOffsetsIfNecessary() {
      if (this.autoCommit) {
        return this.consumerGroup.commitOffsetsIfNecessary();
      }
    }
    commitOffsets(offsets) {
      if (!this.running) {
        this.logger.debug("consumer not running, exiting", {
          groupId: this.consumerGroup.groupId,
          memberId: this.consumerGroup.memberId,
          offsets
        });
        return;
      }
      return this.retrier(async (bail, retryCount, retryTime) => {
        try {
          await this.consumerGroup.commitOffsets(offsets);
        } catch (e) {
          if (!this.running) {
            this.logger.debug("consumer not running, exiting", {
              error: e.message,
              groupId: this.consumerGroup.groupId,
              memberId: this.consumerGroup.memberId,
              offsets
            });
            return;
          }
          if (e.name === "KafkaJSNotImplemented") {
            return bail(e);
          }
          this.logger.debug("Error while committing offsets, trying again...", {
            groupId: this.consumerGroup.groupId,
            memberId: this.consumerGroup.memberId,
            error: e.message,
            stack: e.stack,
            retryCount,
            retryTime,
            offsets
          });
          throw e;
        }
      });
    }
  };
});

// node_modules/kafkajs/src/consumer/assigners/roundRobinAssigner/index.js
var require_roundRobinAssigner = __commonJS((exports, module) => {
  var { MemberMetadata, MemberAssignment } = require_assignerProtocol();
  module.exports = ({ cluster }) => ({
    name: "RoundRobinAssigner",
    version: 0,
    async assign({ members, topics }) {
      const membersCount = members.length;
      const sortedMembers = members.map(({ memberId }) => memberId).sort();
      const assignment = {};
      const topicsPartitions = topics.flatMap((topic) => {
        const partitionMetadata = cluster.findTopicPartitionMetadata(topic);
        return partitionMetadata.map((m) => ({ topic, partitionId: m.partitionId }));
      });
      topicsPartitions.forEach((topicPartition, i) => {
        const assignee = sortedMembers[i % membersCount];
        if (!assignment[assignee]) {
          assignment[assignee] = Object.create(null);
        }
        if (!assignment[assignee][topicPartition.topic]) {
          assignment[assignee][topicPartition.topic] = [];
        }
        assignment[assignee][topicPartition.topic].push(topicPartition.partitionId);
      });
      return Object.keys(assignment).map((memberId) => ({
        memberId,
        memberAssignment: MemberAssignment.encode({
          version: this.version,
          assignment: assignment[memberId]
        })
      }));
    },
    protocol({ topics }) {
      return {
        name: this.name,
        metadata: MemberMetadata.encode({
          version: this.version,
          topics
        })
      };
    }
  });
});

// node_modules/kafkajs/src/consumer/assigners/index.js
var require_assigners = __commonJS((exports, module) => {
  var roundRobin = require_roundRobinAssigner();
  module.exports = {
    roundRobin
  };
});

// node_modules/kafkajs/src/consumer/index.js
var require_consumer = __commonJS((exports, module) => {
  var Long = require_long();
  var createRetry = require_retry();
  var { initialRetryTime } = require_defaults();
  var ConsumerGroup = require_consumerGroup();
  var Runner = require_runner();
  var { events, wrap: wrapEvent, unwrap: unwrapEvent } = require_instrumentationEvents3();
  var InstrumentationEventEmitter = require_emitter();
  var { KafkaJSNonRetriableError } = require_errors();
  var { roundRobin } = require_assigners();
  var { EARLIEST_OFFSET, LATEST_OFFSET } = require_constants2();
  var ISOLATION_LEVEL = require_isolationLevel();
  var sharedPromiseTo = require_sharedPromiseTo();
  var { keys, values } = Object;
  var { CONNECT, DISCONNECT, STOP, CRASH } = events;
  var eventNames = values(events);
  var eventKeys = keys(events).map((key) => `consumer.events.${key}`).join(", ");
  var specialOffsets = [
    Long.fromValue(EARLIEST_OFFSET).toString(),
    Long.fromValue(LATEST_OFFSET).toString()
  ];
  module.exports = ({
    cluster,
    groupId,
    retry,
    logger: rootLogger,
    partitionAssigners = [roundRobin],
    sessionTimeout = 30000,
    rebalanceTimeout = 60000,
    heartbeatInterval = 3000,
    maxBytesPerPartition = 1048576,
    minBytes = 1,
    maxBytes = 10485760,
    maxWaitTimeInMs = 5000,
    isolationLevel = ISOLATION_LEVEL.READ_COMMITTED,
    rackId = "",
    instrumentationEmitter: rootInstrumentationEmitter,
    metadataMaxAge
  }) => {
    if (!groupId) {
      throw new KafkaJSNonRetriableError("Consumer groupId must be a non-empty string.");
    }
    const logger = rootLogger.namespace("Consumer");
    const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter;
    const assigners = partitionAssigners.map((createAssigner) => createAssigner({ groupId, logger, cluster }));
    const topics = {};
    let runner = null;
    let consumerGroup = null;
    let restartTimeout = null;
    if (heartbeatInterval >= sessionTimeout) {
      throw new KafkaJSNonRetriableError(`Consumer heartbeatInterval (${heartbeatInterval}) must be lower than sessionTimeout (${sessionTimeout}). It is recommended to set heartbeatInterval to approximately a third of the sessionTimeout.`);
    }
    const connect = async () => {
      await cluster.connect();
      instrumentationEmitter.emit(CONNECT);
    };
    const disconnect = async () => {
      try {
        await stop();
        logger.debug("consumer has stopped, disconnecting", { groupId });
        await cluster.disconnect();
        instrumentationEmitter.emit(DISCONNECT);
      } catch (e) {
        logger.error(`Caught error when disconnecting the consumer: ${e.message}`, {
          stack: e.stack,
          groupId
        });
        throw e;
      }
    };
    const stop = sharedPromiseTo(async () => {
      try {
        if (runner) {
          await runner.stop();
          runner = null;
          consumerGroup = null;
          instrumentationEmitter.emit(STOP);
        }
        clearTimeout(restartTimeout);
        logger.info("Stopped", { groupId });
      } catch (e) {
        logger.error(`Caught error when stopping the consumer: ${e.message}`, {
          stack: e.stack,
          groupId
        });
        throw e;
      }
    });
    const subscribe = async ({ topic, topics: subscriptionTopics, fromBeginning = false }) => {
      if (consumerGroup) {
        throw new KafkaJSNonRetriableError("Cannot subscribe to topic while consumer is running");
      }
      if (!topic && !subscriptionTopics) {
        throw new KafkaJSNonRetriableError('Missing required argument "topics"');
      }
      if (subscriptionTopics != null && !Array.isArray(subscriptionTopics)) {
        throw new KafkaJSNonRetriableError('Argument "topics" must be an array');
      }
      const subscriptions = subscriptionTopics || [topic];
      for (const subscription of subscriptions) {
        if (typeof subscription !== "string" && !(subscription instanceof RegExp)) {
          throw new KafkaJSNonRetriableError(`Invalid topic ${subscription} (${typeof subscription}), the topic name has to be a String or a RegExp`);
        }
      }
      const hasRegexSubscriptions = subscriptions.some((subscription) => subscription instanceof RegExp);
      const metadata = hasRegexSubscriptions ? await cluster.metadata() : undefined;
      const topicsToSubscribe = [];
      for (const subscription of subscriptions) {
        const isRegExp = subscription instanceof RegExp;
        if (isRegExp) {
          const topicRegExp = subscription;
          const matchedTopics = metadata.topicMetadata.map(({ topic: topicName }) => topicName).filter((topicName) => topicRegExp.test(topicName));
          logger.debug("Subscription based on RegExp", {
            groupId,
            topicRegExp: topicRegExp.toString(),
            matchedTopics
          });
          topicsToSubscribe.push(...matchedTopics);
        } else {
          topicsToSubscribe.push(subscription);
        }
      }
      for (const t of topicsToSubscribe) {
        topics[t] = { fromBeginning };
      }
      await cluster.addMultipleTargetTopics(topicsToSubscribe);
    };
    const run = async ({
      autoCommit = true,
      autoCommitInterval = null,
      autoCommitThreshold = null,
      eachBatchAutoResolve = true,
      partitionsConsumedConcurrently: concurrency = 1,
      eachBatch = null,
      eachMessage = null
    } = {}) => {
      if (consumerGroup) {
        logger.warn("consumer#run was called, but the consumer is already running", { groupId });
        return;
      }
      const start = async (onCrash2) => {
        logger.info("Starting", { groupId });
        consumerGroup = new ConsumerGroup({
          logger: rootLogger,
          topics: keys(topics),
          topicConfigurations: topics,
          retry,
          cluster,
          groupId,
          assigners,
          sessionTimeout,
          rebalanceTimeout,
          maxBytesPerPartition,
          minBytes,
          maxBytes,
          maxWaitTimeInMs,
          instrumentationEmitter,
          isolationLevel,
          rackId,
          metadataMaxAge,
          autoCommit,
          autoCommitInterval,
          autoCommitThreshold
        });
        runner = new Runner({
          logger: rootLogger,
          consumerGroup,
          instrumentationEmitter,
          heartbeatInterval,
          retry,
          autoCommit,
          eachBatchAutoResolve,
          eachBatch,
          eachMessage,
          onCrash: onCrash2,
          concurrency
        });
        await runner.start();
      };
      const onCrash = async (e) => {
        logger.error(`Crash: ${e.name}: ${e.message}`, {
          groupId,
          retryCount: e.retryCount,
          stack: e.stack
        });
        if (e.name === "KafkaJSConnectionClosedError") {
          cluster.removeBroker({ host: e.host, port: e.port });
        }
        await disconnect();
        const getOriginalCause = (error) => {
          if (error.cause) {
            return getOriginalCause(error.cause);
          }
          return error;
        };
        const isErrorRetriable = e.name === "KafkaJSNumberOfRetriesExceeded" || getOriginalCause(e).retriable === true;
        const shouldRestart = isErrorRetriable && (!retry || !retry.restartOnFailure || await retry.restartOnFailure(e).catch((error) => {
          logger.error('Caught error when invoking user-provided "restartOnFailure" callback. Defaulting to restarting.', {
            error: error.message || error,
            cause: e.message || e,
            groupId
          });
          return true;
        }));
        instrumentationEmitter.emit(CRASH, {
          error: e,
          groupId,
          restart: shouldRestart
        });
        if (shouldRestart) {
          const retryTime = e.retryTime || retry && retry.initialRetryTime || initialRetryTime;
          logger.error(`Restarting the consumer in ${retryTime}ms`, {
            retryCount: e.retryCount,
            retryTime,
            groupId
          });
          restartTimeout = setTimeout(() => start(onCrash), retryTime);
        }
      };
      await start(onCrash);
    };
    const on = (eventName, listener) => {
      if (!eventNames.includes(eventName)) {
        throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`);
      }
      return instrumentationEmitter.addListener(unwrapEvent(eventName), (event) => {
        event.type = wrapEvent(event.type);
        Promise.resolve(listener(event)).catch((e) => {
          logger.error(`Failed to execute listener: ${e.message}`, {
            eventName,
            stack: e.stack
          });
        });
      });
    };
    const commitOffsets = async (topicPartitions = []) => {
      const commitsByTopic = topicPartitions.reduce((payload, { topic, partition, offset, metadata = null }) => {
        if (!topic) {
          throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`);
        }
        if (isNaN(partition)) {
          throw new KafkaJSNonRetriableError(`Invalid partition, expected a number received ${partition}`);
        }
        let commitOffset;
        try {
          commitOffset = Long.fromValue(offset);
        } catch (_) {
          throw new KafkaJSNonRetriableError(`Invalid offset, expected a long received ${offset}`);
        }
        if (commitOffset.lessThan(0)) {
          throw new KafkaJSNonRetriableError("Offset must not be a negative number");
        }
        if (metadata !== null && typeof metadata !== "string") {
          throw new KafkaJSNonRetriableError(`Invalid offset metadata, expected string or null, received ${metadata}`);
        }
        const topicCommits = payload[topic] || [];
        topicCommits.push({ partition, offset: commitOffset, metadata });
        return { ...payload, [topic]: topicCommits };
      }, {});
      if (!consumerGroup) {
        throw new KafkaJSNonRetriableError("Consumer group was not initialized, consumer#run must be called first");
      }
      const topics2 = Object.keys(commitsByTopic);
      return runner.commitOffsets({
        topics: topics2.map((topic) => {
          return {
            topic,
            partitions: commitsByTopic[topic]
          };
        })
      });
    };
    const seek = ({ topic, partition, offset }) => {
      if (!topic) {
        throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`);
      }
      if (isNaN(partition)) {
        throw new KafkaJSNonRetriableError(`Invalid partition, expected a number received ${partition}`);
      }
      let seekOffset;
      try {
        seekOffset = Long.fromValue(offset);
      } catch (_) {
        throw new KafkaJSNonRetriableError(`Invalid offset, expected a long received ${offset}`);
      }
      if (seekOffset.lessThan(0) && !specialOffsets.includes(seekOffset.toString())) {
        throw new KafkaJSNonRetriableError("Offset must not be a negative number");
      }
      if (!consumerGroup) {
        throw new KafkaJSNonRetriableError("Consumer group was not initialized, consumer#run must be called first");
      }
      consumerGroup.seek({ topic, partition, offset: seekOffset.toString() });
    };
    const describeGroup = async () => {
      const coordinator = await cluster.findGroupCoordinator({ groupId });
      const retrier = createRetry(retry);
      return retrier(async () => {
        const { groups } = await coordinator.describeGroups({ groupIds: [groupId] });
        return groups.find((group) => group.groupId === groupId);
      });
    };
    const pause = (topicPartitions = []) => {
      for (const topicPartition of topicPartitions) {
        if (!topicPartition || !topicPartition.topic) {
          throw new KafkaJSNonRetriableError(`Invalid topic ${topicPartition && topicPartition.topic || topicPartition}`);
        } else if (typeof topicPartition.partitions !== "undefined" && (!Array.isArray(topicPartition.partitions) || topicPartition.partitions.some(isNaN))) {
          throw new KafkaJSNonRetriableError(`Array of valid partitions required to pause specific partitions instead of ${topicPartition.partitions}`);
        }
      }
      if (!consumerGroup) {
        throw new KafkaJSNonRetriableError("Consumer group was not initialized, consumer#run must be called first");
      }
      consumerGroup.pause(topicPartitions);
    };
    const paused = () => {
      if (!consumerGroup) {
        return [];
      }
      return consumerGroup.paused();
    };
    const resume = (topicPartitions = []) => {
      for (const topicPartition of topicPartitions) {
        if (!topicPartition || !topicPartition.topic) {
          throw new KafkaJSNonRetriableError(`Invalid topic ${topicPartition && topicPartition.topic || topicPartition}`);
        } else if (typeof topicPartition.partitions !== "undefined" && (!Array.isArray(topicPartition.partitions) || topicPartition.partitions.some(isNaN))) {
          throw new KafkaJSNonRetriableError(`Array of valid partitions required to resume specific partitions instead of ${topicPartition.partitions}`);
        }
      }
      if (!consumerGroup) {
        throw new KafkaJSNonRetriableError("Consumer group was not initialized, consumer#run must be called first");
      }
      consumerGroup.resume(topicPartitions);
    };
    const getLogger = () => logger;
    return {
      connect,
      disconnect,
      subscribe,
      stop,
      run,
      commitOffsets,
      seek,
      describeGroup,
      pause,
      paused,
      resume,
      on,
      events,
      logger: getLogger
    };
  };
});

// node_modules/kafkajs/src/utils/waitFor.js
var require_waitFor = __commonJS((exports, module) => {
  var sleep = require_sleep();
  var { KafkaJSTimeout } = require_errors();
  module.exports = (fn, { delay = 50, maxWait = 1e4, timeoutMessage = "Timeout", ignoreTimeout = false } = {}) => {
    let timeoutId;
    let totalWait = 0;
    let fulfilled = false;
    const checkCondition = async (resolve, reject) => {
      totalWait += delay;
      if (fulfilled) {
        return;
      }
      await sleep(delay);
      try {
        const result = await fn(totalWait);
        if (result) {
          fulfilled = true;
          clearTimeout(timeoutId);
          return resolve(result);
        }
        checkCondition(resolve, reject);
      } catch (e) {
        fulfilled = true;
        clearTimeout(timeoutId);
        reject(e);
      }
    };
    return new Promise((resolve, reject) => {
      checkCondition(resolve, reject);
      if (ignoreTimeout) {
        return;
      }
      timeoutId = setTimeout(() => {
        if (!fulfilled) {
          fulfilled = true;
          return reject(new KafkaJSTimeout(timeoutMessage));
        }
      }, maxWait);
    });
  };
});

// node_modules/kafkajs/src/utils/groupBy.js
var require_groupBy = __commonJS((exports, module) => {
  module.exports = async (array, groupFn) => {
    const result = new Map;
    for (const item of array) {
      const group = await Promise.resolve(groupFn(item));
      result.set(group, result.has(group) ? [...result.get(group), item] : [item]);
    }
    return result;
  };
});

// node_modules/kafkajs/src/admin/instrumentationEvents.js
var require_instrumentationEvents4 = __commonJS((exports, module) => {
  var swapObject = require_swapObject();
  var networkEvents = require_instrumentationEvents();
  var InstrumentationEventType = require_eventType();
  var adminType = InstrumentationEventType("admin");
  var events = {
    CONNECT: adminType("connect"),
    DISCONNECT: adminType("disconnect"),
    REQUEST: adminType(networkEvents.NETWORK_REQUEST),
    REQUEST_TIMEOUT: adminType(networkEvents.NETWORK_REQUEST_TIMEOUT),
    REQUEST_QUEUE_SIZE: adminType(networkEvents.NETWORK_REQUEST_QUEUE_SIZE)
  };
  var wrappedEvents = {
    [events.REQUEST]: networkEvents.NETWORK_REQUEST,
    [events.REQUEST_TIMEOUT]: networkEvents.NETWORK_REQUEST_TIMEOUT,
    [events.REQUEST_QUEUE_SIZE]: networkEvents.NETWORK_REQUEST_QUEUE_SIZE
  };
  var reversedWrappedEvents = swapObject(wrappedEvents);
  var unwrap = (eventName) => wrappedEvents[eventName] || eventName;
  var wrap = (eventName) => reversedWrappedEvents[eventName] || eventName;
  module.exports = {
    events,
    wrap,
    unwrap
  };
});

// node_modules/kafkajs/src/protocol/aclResourceTypes.js
var require_aclResourceTypes = __commonJS((exports, module) => {
  module.exports = {
    UNKNOWN: 0,
    ANY: 1,
    TOPIC: 2,
    GROUP: 3,
    CLUSTER: 4,
    TRANSACTIONAL_ID: 5,
    DELEGATION_TOKEN: 6
  };
});

// node_modules/kafkajs/src/protocol/aclOperationTypes.js
var require_aclOperationTypes = __commonJS((exports, module) => {
  module.exports = {
    UNKNOWN: 0,
    ANY: 1,
    ALL: 2,
    READ: 3,
    WRITE: 4,
    CREATE: 5,
    DELETE: 6,
    ALTER: 7,
    DESCRIBE: 8,
    CLUSTER_ACTION: 9,
    DESCRIBE_CONFIGS: 10,
    ALTER_CONFIGS: 11,
    IDEMPOTENT_WRITE: 12
  };
});

// node_modules/kafkajs/src/protocol/aclPermissionTypes.js
var require_aclPermissionTypes = __commonJS((exports, module) => {
  module.exports = {
    UNKNOWN: 0,
    ANY: 1,
    DENY: 2,
    ALLOW: 3
  };
});

// node_modules/kafkajs/src/protocol/resourcePatternTypes.js
var require_resourcePatternTypes = __commonJS((exports, module) => {
  module.exports = {
    UNKNOWN: 0,
    ANY: 1,
    MATCH: 2,
    LITERAL: 3,
    PREFIXED: 4
  };
});

// node_modules/kafkajs/src/admin/index.js
var require_admin = __commonJS((exports, module) => {
  var createRetry = require_retry();
  var waitFor = require_waitFor();
  var groupBy = require_groupBy();
  var createConsumer = require_consumer();
  var InstrumentationEventEmitter = require_emitter();
  var { events, wrap: wrapEvent, unwrap: unwrapEvent } = require_instrumentationEvents4();
  var { LEVELS } = require_loggers();
  var {
    KafkaJSNonRetriableError,
    KafkaJSDeleteGroupsError,
    KafkaJSBrokerNotFound,
    KafkaJSDeleteTopicRecordsError,
    KafkaJSAggregateError
  } = require_errors();
  var { staleMetadata } = require_error();
  var CONFIG_RESOURCE_TYPES = require_configResourceTypes();
  var ACL_RESOURCE_TYPES = require_aclResourceTypes();
  var ACL_OPERATION_TYPES = require_aclOperationTypes();
  var ACL_PERMISSION_TYPES = require_aclPermissionTypes();
  var RESOURCE_PATTERN_TYPES = require_resourcePatternTypes();
  var { EARLIEST_OFFSET, LATEST_OFFSET } = require_constants2();
  var { CONNECT, DISCONNECT } = events;
  var NO_CONTROLLER_ID = -1;
  var { values, keys, entries } = Object;
  var eventNames = values(events);
  var eventKeys = keys(events).map((key) => `admin.events.${key}`).join(", ");
  var retryOnLeaderNotAvailable = (fn, opts = {}) => {
    const callback = async () => {
      try {
        return await fn();
      } catch (e) {
        if (e.type !== "LEADER_NOT_AVAILABLE") {
          throw e;
        }
        return false;
      }
    };
    return waitFor(callback, opts);
  };
  var isConsumerGroupRunning = (description) => ["Empty", "Dead"].includes(description.state);
  var findTopicPartitions = async (cluster, topic) => {
    await cluster.addTargetTopic(topic);
    await cluster.refreshMetadataIfNecessary();
    return cluster.findTopicPartitionMetadata(topic).map(({ partitionId }) => partitionId).sort();
  };
  var indexByPartition = (array) => array.reduce((obj, { partition, ...props }) => Object.assign(obj, { [partition]: { ...props } }), {});
  module.exports = ({
    logger: rootLogger,
    instrumentationEmitter: rootInstrumentationEmitter,
    retry,
    cluster
  }) => {
    const logger = rootLogger.namespace("Admin");
    const instrumentationEmitter = rootInstrumentationEmitter || new InstrumentationEventEmitter;
    const connect = async () => {
      await cluster.connect();
      instrumentationEmitter.emit(CONNECT);
    };
    const disconnect = async () => {
      await cluster.disconnect();
      instrumentationEmitter.emit(DISCONNECT);
    };
    const listTopics = async () => {
      const { topicMetadata } = await cluster.metadata();
      const topics = topicMetadata.map((t) => t.topic);
      return topics;
    };
    const createTopics = async ({ topics, validateOnly, timeout, waitForLeaders = true }) => {
      if (!topics || !Array.isArray(topics)) {
        throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`);
      }
      if (topics.filter(({ topic }) => typeof topic !== "string").length > 0) {
        throw new KafkaJSNonRetriableError("Invalid topics array, the topic names have to be a valid string");
      }
      const topicNames = new Set(topics.map(({ topic }) => topic));
      if (topicNames.size < topics.length) {
        throw new KafkaJSNonRetriableError("Invalid topics array, it cannot have multiple entries for the same topic");
      }
      for (const { topic, configEntries } of topics) {
        if (configEntries == null) {
          continue;
        }
        if (!Array.isArray(configEntries)) {
          throw new KafkaJSNonRetriableError(`Invalid configEntries for topic "${topic}", must be an array`);
        }
        configEntries.forEach((entry, index) => {
          if (typeof entry !== "object" || entry == null) {
            throw new KafkaJSNonRetriableError(`Invalid configEntries for topic "${topic}". Entry ${index} must be an object`);
          }
          for (const requiredProperty of ["name", "value"]) {
            if (!Object.prototype.hasOwnProperty.call(entry, requiredProperty) || typeof entry[requiredProperty] !== "string") {
              throw new KafkaJSNonRetriableError(`Invalid configEntries for topic "${topic}". Entry ${index} must have a valid "${requiredProperty}" property`);
            }
          }
        });
      }
      const retrier = createRetry(retry);
      return retrier(async (bail, retryCount, retryTime) => {
        try {
          await cluster.refreshMetadata();
          const broker = await cluster.findControllerBroker();
          await broker.createTopics({ topics, validateOnly, timeout });
          if (waitForLeaders) {
            const topicNamesArray = Array.from(topicNames.values());
            await retryOnLeaderNotAvailable(async () => await broker.metadata(topicNamesArray), {
              delay: 100,
              maxWait: timeout,
              timeoutMessage: "Timed out while waiting for topic leaders"
            });
          }
          return true;
        } catch (e) {
          if (e.type === "NOT_CONTROLLER") {
            logger.warn("Could not create topics", { error: e.message, retryCount, retryTime });
            throw e;
          }
          if (e instanceof KafkaJSAggregateError) {
            if (e.errors.every((error) => error.type === "TOPIC_ALREADY_EXISTS")) {
              return false;
            }
          }
          bail(e);
        }
      });
    };
    const createPartitions = async ({ topicPartitions, validateOnly, timeout }) => {
      if (!topicPartitions || !Array.isArray(topicPartitions)) {
        throw new KafkaJSNonRetriableError(`Invalid topic partitions array ${topicPartitions}`);
      }
      if (topicPartitions.length === 0) {
        throw new KafkaJSNonRetriableError(`Empty topic partitions array`);
      }
      if (topicPartitions.filter(({ topic }) => typeof topic !== "string").length > 0) {
        throw new KafkaJSNonRetriableError("Invalid topic partitions array, the topic names have to be a valid string");
      }
      const topicNames = new Set(topicPartitions.map(({ topic }) => topic));
      if (topicNames.size < topicPartitions.length) {
        throw new KafkaJSNonRetriableError("Invalid topic partitions array, it cannot have multiple entries for the same topic");
      }
      const retrier = createRetry(retry);
      return retrier(async (bail, retryCount, retryTime) => {
        try {
          await cluster.refreshMetadata();
          const broker = await cluster.findControllerBroker();
          await broker.createPartitions({ topicPartitions, validateOnly, timeout });
        } catch (e) {
          if (e.type === "NOT_CONTROLLER") {
            logger.warn("Could not create topics", { error: e.message, retryCount, retryTime });
            throw e;
          }
          bail(e);
        }
      });
    };
    const deleteTopics = async ({ topics, timeout }) => {
      if (!topics || !Array.isArray(topics)) {
        throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`);
      }
      if (topics.filter((topic) => typeof topic !== "string").length > 0) {
        throw new KafkaJSNonRetriableError("Invalid topics array, the names must be a valid string");
      }
      const retrier = createRetry(retry);
      return retrier(async (bail, retryCount, retryTime) => {
        try {
          await cluster.refreshMetadata();
          const broker = await cluster.findControllerBroker();
          await broker.deleteTopics({ topics, timeout });
          for (const topic of topics) {
            cluster.targetTopics.delete(topic);
          }
          await cluster.refreshMetadata();
        } catch (e) {
          if (["NOT_CONTROLLER", "UNKNOWN_TOPIC_OR_PARTITION"].includes(e.type)) {
            logger.warn("Could not delete topics", { error: e.message, retryCount, retryTime });
            throw e;
          }
          if (e.type === "REQUEST_TIMED_OUT") {
            logger.error('Could not delete topics, check if "delete.topic.enable" is set to "true" (the default value is "false") or increase the timeout', {
              error: e.message,
              retryCount,
              retryTime
            });
          }
          bail(e);
        }
      });
    };
    const fetchTopicOffsets = async (topic) => {
      if (!topic || typeof topic !== "string") {
        throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`);
      }
      const retrier = createRetry(retry);
      return retrier(async (bail, retryCount, retryTime) => {
        try {
          await cluster.addTargetTopic(topic);
          await cluster.refreshMetadataIfNecessary();
          const metadata = cluster.findTopicPartitionMetadata(topic);
          const high = await cluster.fetchTopicsOffset([
            {
              topic,
              fromBeginning: false,
              partitions: metadata.map((p) => ({ partition: p.partitionId }))
            }
          ]);
          const low = await cluster.fetchTopicsOffset([
            {
              topic,
              fromBeginning: true,
              partitions: metadata.map((p) => ({ partition: p.partitionId }))
            }
          ]);
          const { partitions: highPartitions } = high.pop();
          const { partitions: lowPartitions } = low.pop();
          return highPartitions.map(({ partition, offset }) => ({
            partition,
            offset,
            high: offset,
            low: lowPartitions.find(({ partition: lowPartition }) => lowPartition === partition).offset
          }));
        } catch (e) {
          if (e.type === "UNKNOWN_TOPIC_OR_PARTITION") {
            await cluster.refreshMetadata();
            throw e;
          }
          bail(e);
        }
      });
    };
    const fetchTopicOffsetsByTimestamp = async (topic, timestamp) => {
      if (!topic || typeof topic !== "string") {
        throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`);
      }
      const retrier = createRetry(retry);
      return retrier(async (bail, retryCount, retryTime) => {
        try {
          await cluster.addTargetTopic(topic);
          await cluster.refreshMetadataIfNecessary();
          const metadata = cluster.findTopicPartitionMetadata(topic);
          const partitions = metadata.map((p) => ({ partition: p.partitionId }));
          const high = await cluster.fetchTopicsOffset([
            {
              topic,
              fromBeginning: false,
              partitions
            }
          ]);
          const { partitions: highPartitions } = high.pop();
          const offsets = await cluster.fetchTopicsOffset([
            {
              topic,
              fromTimestamp: timestamp,
              partitions
            }
          ]);
          const { partitions: lowPartitions } = offsets.pop();
          return lowPartitions.map(({ partition, offset }) => ({
            partition,
            offset: parseInt(offset, 10) >= 0 ? offset : highPartitions.find(({ partition: highPartition }) => highPartition === partition).offset
          }));
        } catch (e) {
          if (e.type === "UNKNOWN_TOPIC_OR_PARTITION") {
            await cluster.refreshMetadata();
            throw e;
          }
          bail(e);
        }
      });
    };
    const fetchOffsets = async ({ groupId, topics, resolveOffsets = false }) => {
      if (!groupId) {
        throw new KafkaJSNonRetriableError(`Invalid groupId ${groupId}`);
      }
      if (!topics) {
        topics = [];
      }
      if (!Array.isArray(topics)) {
        throw new KafkaJSNonRetriableError("Expected topics array to be set");
      }
      const coordinator = await cluster.findGroupCoordinator({ groupId });
      const topicsToFetch = await Promise.all(topics.map(async (topic) => {
        const partitions = await findTopicPartitions(cluster, topic);
        const partitionsToFetch = partitions.map((partition) => ({ partition }));
        return { topic, partitions: partitionsToFetch };
      }));
      let { responses: consumerOffsets } = await coordinator.offsetFetch({
        groupId,
        topics: topicsToFetch
      });
      if (resolveOffsets) {
        consumerOffsets = await Promise.all(consumerOffsets.map(async ({ topic, partitions }) => {
          const indexedOffsets = indexByPartition(await fetchTopicOffsets(topic));
          const recalculatedPartitions = partitions.map(({ offset, partition, ...props }) => {
            let resolvedOffset = offset;
            if (Number(offset) === EARLIEST_OFFSET) {
              resolvedOffset = indexedOffsets[partition].low;
            }
            if (Number(offset) === LATEST_OFFSET) {
              resolvedOffset = indexedOffsets[partition].high;
            }
            return {
              partition,
              offset: resolvedOffset,
              ...props
            };
          });
          await setOffsets({ groupId, topic, partitions: recalculatedPartitions });
          return {
            topic,
            partitions: recalculatedPartitions
          };
        }));
      }
      return consumerOffsets.map(({ topic, partitions }) => {
        const completePartitions = partitions.map(({ partition, offset, metadata }) => ({
          partition,
          offset,
          metadata: metadata || null
        }));
        return { topic, partitions: completePartitions };
      });
    };
    const resetOffsets = async ({ groupId, topic, earliest = false }) => {
      if (!groupId) {
        throw new KafkaJSNonRetriableError(`Invalid groupId ${groupId}`);
      }
      if (!topic) {
        throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`);
      }
      const partitions = await findTopicPartitions(cluster, topic);
      const partitionsToSeek = partitions.map((partition) => ({
        partition,
        offset: cluster.defaultOffset({ fromBeginning: earliest })
      }));
      return setOffsets({ groupId, topic, partitions: partitionsToSeek });
    };
    const setOffsets = async ({ groupId, topic, partitions }) => {
      if (!groupId) {
        throw new KafkaJSNonRetriableError(`Invalid groupId ${groupId}`);
      }
      if (!topic) {
        throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`);
      }
      if (!partitions || partitions.length === 0) {
        throw new KafkaJSNonRetriableError(`Invalid partitions`);
      }
      const consumer = createConsumer({
        logger: rootLogger.namespace("Admin", LEVELS.NOTHING),
        cluster,
        groupId
      });
      await consumer.subscribe({ topic, fromBeginning: true });
      const description = await consumer.describeGroup();
      if (!isConsumerGroupRunning(description)) {
        throw new KafkaJSNonRetriableError(`The consumer group must have no running instances, current state: ${description.state}`);
      }
      return new Promise((resolve, reject) => {
        consumer.on(consumer.events.FETCH, async () => consumer.stop().then(resolve).catch(reject));
        consumer.run({
          eachBatchAutoResolve: false,
          eachBatch: async () => true
        }).catch(reject);
        consumer.pause([{ topic }]);
        for (const seekData of partitions) {
          consumer.seek({ topic, ...seekData });
        }
      });
    };
    const isBrokerConfig = (type) => [CONFIG_RESOURCE_TYPES.BROKER, CONFIG_RESOURCE_TYPES.BROKER_LOGGER].includes(type);
    const groupResourcesByBroker = ({ resources, defaultBroker }) => groupBy(resources, async ({ type, name: nodeId }) => {
      return isBrokerConfig(type) ? await cluster.findBroker({ nodeId: String(nodeId) }) : defaultBroker;
    });
    const describeConfigs = async ({ resources, includeSynonyms }) => {
      if (!resources || !Array.isArray(resources)) {
        throw new KafkaJSNonRetriableError(`Invalid resources array ${resources}`);
      }
      if (resources.length === 0) {
        throw new KafkaJSNonRetriableError("Resources array cannot be empty");
      }
      const validResourceTypes = Object.values(CONFIG_RESOURCE_TYPES);
      const invalidType = resources.find((r) => !validResourceTypes.includes(r.type));
      if (invalidType) {
        throw new KafkaJSNonRetriableError(`Invalid resource type ${invalidType.type}: ${JSON.stringify(invalidType)}`);
      }
      const invalidName = resources.find((r) => !r.name || typeof r.name !== "string");
      if (invalidName) {
        throw new KafkaJSNonRetriableError(`Invalid resource name ${invalidName.name}: ${JSON.stringify(invalidName)}`);
      }
      const invalidConfigs = resources.find((r) => !Array.isArray(r.configNames) && r.configNames != null);
      if (invalidConfigs) {
        const { configNames } = invalidConfigs;
        throw new KafkaJSNonRetriableError(`Invalid resource configNames ${configNames}: ${JSON.stringify(invalidConfigs)}`);
      }
      const retrier = createRetry(retry);
      return retrier(async (bail, retryCount, retryTime) => {
        try {
          await cluster.refreshMetadata();
          const controller = await cluster.findControllerBroker();
          const resourcerByBroker = await groupResourcesByBroker({
            resources,
            defaultBroker: controller
          });
          const describeConfigsAction = async (broker) => {
            const targetBroker = broker || controller;
            return targetBroker.describeConfigs({
              resources: resourcerByBroker.get(targetBroker),
              includeSynonyms
            });
          };
          const brokers = Array.from(resourcerByBroker.keys());
          const responses = await Promise.all(brokers.map(describeConfigsAction));
          const responseResources = responses.reduce((result, { resources: resources2 }) => [...result, ...resources2], []);
          return { resources: responseResources };
        } catch (e) {
          if (e.type === "NOT_CONTROLLER") {
            logger.warn("Could not describe configs", { error: e.message, retryCount, retryTime });
            throw e;
          }
          bail(e);
        }
      });
    };
    const alterConfigs = async ({ resources, validateOnly }) => {
      if (!resources || !Array.isArray(resources)) {
        throw new KafkaJSNonRetriableError(`Invalid resources array ${resources}`);
      }
      if (resources.length === 0) {
        throw new KafkaJSNonRetriableError("Resources array cannot be empty");
      }
      const validResourceTypes = Object.values(CONFIG_RESOURCE_TYPES);
      const invalidType = resources.find((r) => !validResourceTypes.includes(r.type));
      if (invalidType) {
        throw new KafkaJSNonRetriableError(`Invalid resource type ${invalidType.type}: ${JSON.stringify(invalidType)}`);
      }
      const invalidName = resources.find((r) => !r.name || typeof r.name !== "string");
      if (invalidName) {
        throw new KafkaJSNonRetriableError(`Invalid resource name ${invalidName.name}: ${JSON.stringify(invalidName)}`);
      }
      const invalidConfigs = resources.find((r) => !Array.isArray(r.configEntries));
      if (invalidConfigs) {
        const { configEntries } = invalidConfigs;
        throw new KafkaJSNonRetriableError(`Invalid resource configEntries ${configEntries}: ${JSON.stringify(invalidConfigs)}`);
      }
      const invalidConfigValue = resources.find((r) => r.configEntries.some((e) => typeof e.name !== "string" || typeof e.value !== "string"));
      if (invalidConfigValue) {
        throw new KafkaJSNonRetriableError(`Invalid resource config value: ${JSON.stringify(invalidConfigValue)}`);
      }
      const retrier = createRetry(retry);
      return retrier(async (bail, retryCount, retryTime) => {
        try {
          await cluster.refreshMetadata();
          const controller = await cluster.findControllerBroker();
          const resourcerByBroker = await groupResourcesByBroker({
            resources,
            defaultBroker: controller
          });
          const alterConfigsAction = async (broker) => {
            const targetBroker = broker || controller;
            return targetBroker.alterConfigs({
              resources: resourcerByBroker.get(targetBroker),
              validateOnly: !!validateOnly
            });
          };
          const brokers = Array.from(resourcerByBroker.keys());
          const responses = await Promise.all(brokers.map(alterConfigsAction));
          const responseResources = responses.reduce((result, { resources: resources2 }) => [...result, ...resources2], []);
          return { resources: responseResources };
        } catch (e) {
          if (e.type === "NOT_CONTROLLER") {
            logger.warn("Could not alter configs", { error: e.message, retryCount, retryTime });
            throw e;
          }
          bail(e);
        }
      });
    };
    const fetchTopicMetadata = async ({ topics = [] } = {}) => {
      if (topics) {
        topics.forEach((topic) => {
          if (!topic || typeof topic !== "string") {
            throw new KafkaJSNonRetriableError(`Invalid topic ${topic}`);
          }
        });
      }
      const metadata = await cluster.metadata({ topics });
      return {
        topics: metadata.topicMetadata.map((topicMetadata) => ({
          name: topicMetadata.topic,
          partitions: topicMetadata.partitionMetadata
        }))
      };
    };
    const describeCluster = async () => {
      const { brokers: nodes, clusterId, controllerId } = await cluster.metadata({ topics: [] });
      const brokers = nodes.map(({ nodeId, host, port }) => ({
        nodeId,
        host,
        port
      }));
      const controller = controllerId == null || controllerId === NO_CONTROLLER_ID ? null : controllerId;
      return {
        brokers,
        controller,
        clusterId
      };
    };
    const listGroups = async () => {
      await cluster.refreshMetadata();
      let groups = [];
      for (var nodeId in cluster.brokerPool.brokers) {
        const broker = await cluster.findBroker({ nodeId });
        const response = await broker.listGroups();
        groups = groups.concat(response.groups);
      }
      return { groups };
    };
    const describeGroups = async (groupIds) => {
      const coordinatorsForGroup = await Promise.all(groupIds.map(async (groupId) => {
        const coordinator = await cluster.findGroupCoordinator({ groupId });
        return {
          coordinator,
          groupId
        };
      }));
      const groupsByCoordinator = Object.values(coordinatorsForGroup.reduce((coordinators, { coordinator, groupId }) => {
        const group = coordinators[coordinator.nodeId];
        if (group) {
          coordinators[coordinator.nodeId] = {
            ...group,
            groupIds: [...group.groupIds, groupId]
          };
        } else {
          coordinators[coordinator.nodeId] = { coordinator, groupIds: [groupId] };
        }
        return coordinators;
      }, {}));
      const responses = await Promise.all(groupsByCoordinator.map(async ({ coordinator, groupIds: groupIds2 }) => {
        const retrier = createRetry(retry);
        const { groups: groups2 } = await retrier(() => coordinator.describeGroups({ groupIds: groupIds2 }));
        return groups2;
      }));
      const groups = [].concat.apply([], responses);
      return { groups };
    };
    const deleteGroups = async (groupIds) => {
      if (!groupIds || !Array.isArray(groupIds)) {
        throw new KafkaJSNonRetriableError(`Invalid groupIds array ${groupIds}`);
      }
      const invalidGroupId = groupIds.some((g) => typeof g !== "string");
      if (invalidGroupId) {
        throw new KafkaJSNonRetriableError(`Invalid groupId name: ${JSON.stringify(invalidGroupId)}`);
      }
      const retrier = createRetry(retry);
      let results = [];
      let clonedGroupIds = groupIds.slice();
      return retrier(async (bail, retryCount, retryTime) => {
        try {
          if (clonedGroupIds.length === 0)
            return [];
          await cluster.refreshMetadata();
          const brokersPerGroups = {};
          const brokersPerNode = {};
          for (const groupId of clonedGroupIds) {
            const broker = await cluster.findGroupCoordinator({ groupId });
            if (brokersPerGroups[broker.nodeId] === undefined)
              brokersPerGroups[broker.nodeId] = [];
            brokersPerGroups[broker.nodeId].push(groupId);
            brokersPerNode[broker.nodeId] = broker;
          }
          const res = await Promise.all(Object.keys(brokersPerNode).map(async (nodeId) => await brokersPerNode[nodeId].deleteGroups(brokersPerGroups[nodeId])));
          const errors = res.flatMap(({ results: results2 }) => results2.map(({ groupId, errorCode, error }) => {
            return { groupId, errorCode, error };
          })).filter(({ errorCode }) => errorCode !== 0);
          clonedGroupIds = errors.map(({ groupId }) => groupId);
          if (errors.length > 0)
            throw new KafkaJSDeleteGroupsError("Error in DeleteGroups", errors);
          results = res.flatMap(({ results: results2 }) => results2);
          return results;
        } catch (e) {
          if (e.type === "NOT_CONTROLLER" || e.type === "COORDINATOR_NOT_AVAILABLE") {
            logger.warn("Could not delete groups", { error: e.message, retryCount, retryTime });
            throw e;
          }
          bail(e);
        }
      });
    };
    const deleteTopicRecords = async ({ topic, partitions }) => {
      if (!topic || typeof topic !== "string") {
        throw new KafkaJSNonRetriableError(`Invalid topic "${topic}"`);
      }
      if (!partitions || partitions.length === 0) {
        throw new KafkaJSNonRetriableError(`Invalid partitions`);
      }
      const partitionsByBroker = cluster.findLeaderForPartitions(topic, partitions.map((p) => p.partition));
      const partitionsFound = values(partitionsByBroker).flat();
      const topicOffsets = await fetchTopicOffsets(topic);
      const leaderNotFoundErrors = [];
      partitions.forEach(({ partition, offset }) => {
        if (!partitionsFound.includes(partition)) {
          leaderNotFoundErrors.push({
            partition,
            offset,
            error: new KafkaJSBrokerNotFound("Could not find the leader for the partition", {
              retriable: false
            })
          });
          return;
        }
        const { low } = topicOffsets.find((p) => p.partition === partition) || {
          high: undefined,
          low: undefined
        };
        if (parseInt(offset) < parseInt(low) && parseInt(offset) !== -1) {
          logger.warn("The requested offset is before the earliest offset maintained on the partition - no records will be deleted from this partition", {
            topic,
            partition,
            offset
          });
        }
      });
      if (leaderNotFoundErrors.length > 0) {
        throw new KafkaJSDeleteTopicRecordsError({ topic, partitions: leaderNotFoundErrors });
      }
      const seekEntriesByBroker = entries(partitionsByBroker).reduce((obj, [nodeId, nodePartitions]) => {
        obj[nodeId] = {
          topic,
          partitions: partitions.filter((p) => nodePartitions.includes(p.partition))
        };
        return obj;
      }, {});
      const retrier = createRetry(retry);
      return retrier(async (bail) => {
        try {
          const partitionErrors = [];
          const brokerRequests = entries(seekEntriesByBroker).map(([nodeId, { topic: topic2, partitions: partitions2 }]) => async () => {
            const broker = await cluster.findBroker({ nodeId });
            await broker.deleteRecords({ topics: [{ topic: topic2, partitions: partitions2 }] });
            delete seekEntriesByBroker[nodeId];
          });
          await Promise.all(brokerRequests.map((request3) => request3().catch((e) => {
            if (e.name === "KafkaJSDeleteTopicRecordsError") {
              e.partitions.forEach(({ partition, offset, error }) => {
                partitionErrors.push({
                  partition,
                  offset,
                  error
                });
              });
            } else {
              throw e;
            }
          })));
          if (partitionErrors.length > 0) {
            throw new KafkaJSDeleteTopicRecordsError({
              topic,
              partitions: partitionErrors
            });
          }
        } catch (e) {
          if (e.retriable && e.partitions.some(({ error }) => staleMetadata(error) || error.name === "KafkaJSMetadataNotLoaded")) {
            await cluster.refreshMetadata();
          }
          throw e;
        }
      });
    };
    const createAcls = async ({ acl }) => {
      if (!acl || !Array.isArray(acl)) {
        throw new KafkaJSNonRetriableError(`Invalid ACL array ${acl}`);
      }
      if (acl.length === 0) {
        throw new KafkaJSNonRetriableError("Empty ACL array");
      }
      if (acl.some(({ principal }) => typeof principal !== "string")) {
        throw new KafkaJSNonRetriableError("Invalid ACL array, the principals have to be a valid string");
      }
      if (acl.some(({ host }) => typeof host !== "string")) {
        throw new KafkaJSNonRetriableError("Invalid ACL array, the hosts have to be a valid string");
      }
      if (acl.some(({ resourceName }) => typeof resourceName !== "string")) {
        throw new KafkaJSNonRetriableError("Invalid ACL array, the resourceNames have to be a valid string");
      }
      let invalidType;
      const validOperationTypes = Object.values(ACL_OPERATION_TYPES);
      invalidType = acl.find((i) => !validOperationTypes.includes(i.operation));
      if (invalidType) {
        throw new KafkaJSNonRetriableError(`Invalid operation type ${invalidType.operation}: ${JSON.stringify(invalidType)}`);
      }
      const validResourcePatternTypes = Object.values(RESOURCE_PATTERN_TYPES);
      invalidType = acl.find((i) => !validResourcePatternTypes.includes(i.resourcePatternType));
      if (invalidType) {
        throw new KafkaJSNonRetriableError(`Invalid resource pattern type ${invalidType.resourcePatternType}: ${JSON.stringify(invalidType)}`);
      }
      const validPermissionTypes = Object.values(ACL_PERMISSION_TYPES);
      invalidType = acl.find((i) => !validPermissionTypes.includes(i.permissionType));
      if (invalidType) {
        throw new KafkaJSNonRetriableError(`Invalid permission type ${invalidType.permissionType}: ${JSON.stringify(invalidType)}`);
      }
      const validResourceTypes = Object.values(ACL_RESOURCE_TYPES);
      invalidType = acl.find((i) => !validResourceTypes.includes(i.resourceType));
      if (invalidType) {
        throw new KafkaJSNonRetriableError(`Invalid resource type ${invalidType.resourceType}: ${JSON.stringify(invalidType)}`);
      }
      const retrier = createRetry(retry);
      return retrier(async (bail, retryCount, retryTime) => {
        try {
          await cluster.refreshMetadata();
          const broker = await cluster.findControllerBroker();
          await broker.createAcls({ acl });
          return true;
        } catch (e) {
          if (e.type === "NOT_CONTROLLER") {
            logger.warn("Could not create ACL", { error: e.message, retryCount, retryTime });
            throw e;
          }
          bail(e);
        }
      });
    };
    const describeAcls = async ({
      resourceType,
      resourceName,
      resourcePatternType,
      principal,
      host,
      operation,
      permissionType
    }) => {
      if (typeof principal !== "string" && typeof principal !== "undefined") {
        throw new KafkaJSNonRetriableError("Invalid principal, the principal have to be a valid string");
      }
      if (typeof host !== "string" && typeof host !== "undefined") {
        throw new KafkaJSNonRetriableError("Invalid host, the host have to be a valid string");
      }
      if (typeof resourceName !== "string" && typeof resourceName !== "undefined") {
        throw new KafkaJSNonRetriableError("Invalid resourceName, the resourceName have to be a valid string");
      }
      const validOperationTypes = Object.values(ACL_OPERATION_TYPES);
      if (!validOperationTypes.includes(operation)) {
        throw new KafkaJSNonRetriableError(`Invalid operation type ${operation}`);
      }
      const validResourcePatternTypes = Object.values(RESOURCE_PATTERN_TYPES);
      if (!validResourcePatternTypes.includes(resourcePatternType)) {
        throw new KafkaJSNonRetriableError(`Invalid resource pattern filter type ${resourcePatternType}`);
      }
      const validPermissionTypes = Object.values(ACL_PERMISSION_TYPES);
      if (!validPermissionTypes.includes(permissionType)) {
        throw new KafkaJSNonRetriableError(`Invalid permission type ${permissionType}`);
      }
      const validResourceTypes = Object.values(ACL_RESOURCE_TYPES);
      if (!validResourceTypes.includes(resourceType)) {
        throw new KafkaJSNonRetriableError(`Invalid resource type ${resourceType}`);
      }
      const retrier = createRetry(retry);
      return retrier(async (bail, retryCount, retryTime) => {
        try {
          await cluster.refreshMetadata();
          const broker = await cluster.findControllerBroker();
          const { resources } = await broker.describeAcls({
            resourceType,
            resourceName,
            resourcePatternType,
            principal,
            host,
            operation,
            permissionType
          });
          return { resources };
        } catch (e) {
          if (e.type === "NOT_CONTROLLER") {
            logger.warn("Could not describe ACL", { error: e.message, retryCount, retryTime });
            throw e;
          }
          bail(e);
        }
      });
    };
    const deleteAcls = async ({ filters }) => {
      if (!filters || !Array.isArray(filters)) {
        throw new KafkaJSNonRetriableError(`Invalid ACL Filter array ${filters}`);
      }
      if (filters.length === 0) {
        throw new KafkaJSNonRetriableError("Empty ACL Filter array");
      }
      if (filters.some(({ principal }) => typeof principal !== "string" && typeof principal !== "undefined")) {
        throw new KafkaJSNonRetriableError("Invalid ACL Filter array, the principals have to be a valid string");
      }
      if (filters.some(({ host }) => typeof host !== "string" && typeof host !== "undefined")) {
        throw new KafkaJSNonRetriableError("Invalid ACL Filter array, the hosts have to be a valid string");
      }
      if (filters.some(({ resourceName }) => typeof resourceName !== "string" && typeof resourceName !== "undefined")) {
        throw new KafkaJSNonRetriableError("Invalid ACL Filter array, the resourceNames have to be a valid string");
      }
      let invalidType;
      const validOperationTypes = Object.values(ACL_OPERATION_TYPES);
      invalidType = filters.find((i) => !validOperationTypes.includes(i.operation));
      if (invalidType) {
        throw new KafkaJSNonRetriableError(`Invalid operation type ${invalidType.operation}: ${JSON.stringify(invalidType)}`);
      }
      const validResourcePatternTypes = Object.values(RESOURCE_PATTERN_TYPES);
      invalidType = filters.find((i) => !validResourcePatternTypes.includes(i.resourcePatternType));
      if (invalidType) {
        throw new KafkaJSNonRetriableError(`Invalid resource pattern type ${invalidType.resourcePatternType}: ${JSON.stringify(invalidType)}`);
      }
      const validPermissionTypes = Object.values(ACL_PERMISSION_TYPES);
      invalidType = filters.find((i) => !validPermissionTypes.includes(i.permissionType));
      if (invalidType) {
        throw new KafkaJSNonRetriableError(`Invalid permission type ${invalidType.permissionType}: ${JSON.stringify(invalidType)}`);
      }
      const validResourceTypes = Object.values(ACL_RESOURCE_TYPES);
      invalidType = filters.find((i) => !validResourceTypes.includes(i.resourceType));
      if (invalidType) {
        throw new KafkaJSNonRetriableError(`Invalid resource type ${invalidType.resourceType}: ${JSON.stringify(invalidType)}`);
      }
      const retrier = createRetry(retry);
      return retrier(async (bail, retryCount, retryTime) => {
        try {
          await cluster.refreshMetadata();
          const broker = await cluster.findControllerBroker();
          const { filterResponses } = await broker.deleteAcls({ filters });
          return { filterResponses };
        } catch (e) {
          if (e.type === "NOT_CONTROLLER") {
            logger.warn("Could not delete ACL", { error: e.message, retryCount, retryTime });
            throw e;
          }
          bail(e);
        }
      });
    };
    const alterPartitionReassignments = async ({ topics, timeout }) => {
      if (!topics || !Array.isArray(topics)) {
        throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`);
      }
      if (topics.filter(({ topic }) => typeof topic !== "string").length > 0) {
        throw new KafkaJSNonRetriableError("Invalid topics array, the topic names have to be a valid string");
      }
      const topicNames = new Set(topics.map(({ topic }) => topic));
      if (topicNames.size < topics.length) {
        throw new KafkaJSNonRetriableError("Invalid topics array, it cannot have multiple entries for the same topic");
      }
      for (const { topic, partitionAssignment } of topics) {
        if (!partitionAssignment || !Array.isArray(partitionAssignment)) {
          throw new KafkaJSNonRetriableError(`Invalid partitions array: ${partitionAssignment} for topic: ${topic}`);
        }
        for (const { partition, replicas } of partitionAssignment) {
          if (partition === null || partition === undefined || typeof partition !== "number" || partition < 0) {
            throw new KafkaJSNonRetriableError(`Invalid partitions index: ${partition} for topic: ${topic}`);
          }
          if (!replicas || !Array.isArray(replicas)) {
            throw new KafkaJSNonRetriableError(`Invalid replica assignment: ${replicas} for topic: ${topic} on partition: ${partition}`);
          }
          if (replicas.filter((replica) => typeof replica !== "number" || replica < 0).length >= 1) {
            throw new KafkaJSNonRetriableError(`Invalid replica assignment: ${replicas} for topic: ${topic} on partition: ${partition}. Replicas must be a non negative number`);
          }
        }
      }
      const retrier = createRetry(retry);
      return retrier(async (bail, retryCount, retryTime) => {
        try {
          await cluster.refreshMetadata();
          const broker = await cluster.findControllerBroker();
          await broker.alterPartitionReassignments({ topics, timeout });
        } catch (e) {
          if (e.type === "NOT_CONTROLLER") {
            logger.warn("Could not reassign partitions", { error: e.message, retryCount, retryTime });
            throw e;
          }
          bail(e);
        }
      });
    };
    const listPartitionReassignments = async ({ topics = null, timeout }) => {
      if (topics) {
        if (!Array.isArray(topics)) {
          throw new KafkaJSNonRetriableError(`Invalid topics array ${topics}`);
        }
        if (topics.filter(({ topic }) => typeof topic !== "string").length > 0) {
          throw new KafkaJSNonRetriableError("Invalid topics array, the topic names have to be a valid string");
        }
        const topicNames = new Set(topics.map(({ topic }) => topic));
        if (topicNames.size < topics.length) {
          throw new KafkaJSNonRetriableError("Invalid topics array, it cannot have multiple entries for the same topic");
        }
        for (const { topic, partitions } of topics) {
          if (!partitions || !Array.isArray(partitions)) {
            throw new KafkaJSNonRetriableError(`Invalid partition array: ${partitions} for topic: ${topic}`);
          }
          if (partitions.filter((partition) => typeof partition !== "number" || partition < 0).length >= 1) {
            throw new KafkaJSNonRetriableError(`Invalid partition array: ${partitions} for topic: ${topic}. The partition indices have to be a valid number greater than 0.`);
          }
        }
      }
      const retrier = createRetry(retry);
      return retrier(async (bail, retryCount, retryTime) => {
        try {
          await cluster.refreshMetadata();
          const broker = await cluster.findControllerBroker();
          const response = await broker.listPartitionReassignments({ topics, timeout });
          return { topics: response.topics };
        } catch (e) {
          if (e.type === "NOT_CONTROLLER") {
            logger.warn("Could not reassign partitions", { error: e.message, retryCount, retryTime });
            throw e;
          }
          bail(e);
        }
      });
    };
    const on = (eventName, listener) => {
      if (!eventNames.includes(eventName)) {
        throw new KafkaJSNonRetriableError(`Event name should be one of ${eventKeys}`);
      }
      return instrumentationEmitter.addListener(unwrapEvent(eventName), (event) => {
        event.type = wrapEvent(event.type);
        Promise.resolve(listener(event)).catch((e) => {
          logger.error(`Failed to execute listener: ${e.message}`, {
            eventName,
            stack: e.stack
          });
        });
      });
    };
    const getLogger = () => logger;
    return {
      connect,
      disconnect,
      listTopics,
      createTopics,
      deleteTopics,
      createPartitions,
      fetchTopicMetadata,
      describeCluster,
      events,
      fetchOffsets,
      fetchTopicOffsets,
      fetchTopicOffsetsByTimestamp,
      setOffsets,
      resetOffsets,
      describeConfigs,
      alterConfigs,
      on,
      logger: getLogger,
      listGroups,
      describeGroups,
      deleteGroups,
      describeAcls,
      deleteAcls,
      createAcls,
      deleteTopicRecords,
      alterPartitionReassignments,
      listPartitionReassignments
    };
  };
});

// node_modules/kafkajs/src/network/socketFactory.js
var require_socketFactory = __commonJS((exports, module) => {
  var KEEP_ALIVE_DELAY = 60000;
  module.exports = () => {
    const net = import.meta.require("net");
    const tls = import.meta.require("tls");
    return ({ host, port, ssl, onConnect }) => {
      const socket = ssl ? tls.connect(Object.assign({ host, port }, !net.isIP(host) ? { servername: host } : {}, ssl), onConnect) : net.connect({ host, port }, onConnect);
      socket.setKeepAlive(true, KEEP_ALIVE_DELAY);
      return socket;
    };
  };
});

// node_modules/kafkajs/src/utils/once.js
var require_once = __commonJS((exports, module) => {
  module.exports = (fn) => {
    let called = false;
    return (...args) => {
      if (!called) {
        called = true;
        return fn(...args);
      }
    };
  };
});

// node_modules/kafkajs/src/index.js
var require_src = __commonJS((exports, module) => {
  var {
    createLogger,
    LEVELS: { INFO }
  } = require_loggers();
  var InstrumentationEventEmitter = require_emitter();
  var LoggerConsole = require_console();
  var Cluster = require_cluster();
  var createProducer = require_producer();
  var createConsumer = require_consumer();
  var createAdmin = require_admin();
  var ISOLATION_LEVEL = require_isolationLevel();
  var defaultSocketFactory = require_socketFactory();
  var once = require_once();
  var websiteUrl = require_websiteUrl();
  var PRIVATE = {
    CREATE_CLUSTER: Symbol("private:Kafka:createCluster"),
    CLUSTER_RETRY: Symbol("private:Kafka:clusterRetry"),
    LOGGER: Symbol("private:Kafka:logger"),
    OFFSETS: Symbol("private:Kafka:offsets")
  };
  var DEFAULT_METADATA_MAX_AGE = 300000;
  var warnOfDefaultPartitioner = once((logger) => {
    if (process.env.KAFKAJS_NO_PARTITIONER_WARNING == null) {
      logger.warn(`KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option "createPartitioner: Partitioners.LegacyPartitioner". See the migration guide at ${websiteUrl("docs/migration-guide-v2.0.0", "producer-new-default-partitioner")} for details. Silence this warning by setting the environment variable "KAFKAJS_NO_PARTITIONER_WARNING=1"`);
    }
  });
  module.exports = class Client {
    constructor({
      brokers,
      ssl,
      sasl,
      clientId,
      connectionTimeout = 1000,
      authenticationTimeout,
      reauthenticationThreshold,
      requestTimeout,
      enforceRequestTimeout = true,
      retry,
      socketFactory = defaultSocketFactory(),
      logLevel = INFO,
      logCreator = LoggerConsole
    }) {
      this[PRIVATE.OFFSETS] = new Map;
      this[PRIVATE.LOGGER] = createLogger({ level: logLevel, logCreator });
      this[PRIVATE.CLUSTER_RETRY] = retry;
      this[PRIVATE.CREATE_CLUSTER] = ({
        metadataMaxAge,
        allowAutoTopicCreation = true,
        maxInFlightRequests = null,
        instrumentationEmitter = null,
        isolationLevel
      }) => new Cluster({
        logger: this[PRIVATE.LOGGER],
        retry: this[PRIVATE.CLUSTER_RETRY],
        offsets: this[PRIVATE.OFFSETS],
        socketFactory,
        brokers,
        ssl,
        sasl,
        clientId,
        connectionTimeout,
        authenticationTimeout,
        reauthenticationThreshold,
        requestTimeout,
        enforceRequestTimeout,
        metadataMaxAge,
        instrumentationEmitter,
        allowAutoTopicCreation,
        maxInFlightRequests,
        isolationLevel
      });
    }
    producer({
      createPartitioner,
      retry,
      metadataMaxAge = DEFAULT_METADATA_MAX_AGE,
      allowAutoTopicCreation,
      idempotent,
      transactionalId,
      transactionTimeout,
      maxInFlightRequests
    } = {}) {
      const instrumentationEmitter = new InstrumentationEventEmitter;
      const cluster = this[PRIVATE.CREATE_CLUSTER]({
        metadataMaxAge,
        allowAutoTopicCreation,
        maxInFlightRequests,
        instrumentationEmitter
      });
      if (createPartitioner == null) {
        warnOfDefaultPartitioner(this[PRIVATE.LOGGER]);
      }
      return createProducer({
        retry: { ...this[PRIVATE.CLUSTER_RETRY], ...retry },
        logger: this[PRIVATE.LOGGER],
        cluster,
        createPartitioner,
        idempotent,
        transactionalId,
        transactionTimeout,
        instrumentationEmitter
      });
    }
    consumer({
      groupId,
      partitionAssigners,
      metadataMaxAge = DEFAULT_METADATA_MAX_AGE,
      sessionTimeout,
      rebalanceTimeout,
      heartbeatInterval,
      maxBytesPerPartition,
      minBytes,
      maxBytes,
      maxWaitTimeInMs,
      retry = { retries: 5 },
      allowAutoTopicCreation,
      maxInFlightRequests,
      readUncommitted = false,
      rackId = ""
    } = {}) {
      const isolationLevel = readUncommitted ? ISOLATION_LEVEL.READ_UNCOMMITTED : ISOLATION_LEVEL.READ_COMMITTED;
      const instrumentationEmitter = new InstrumentationEventEmitter;
      const cluster = this[PRIVATE.CREATE_CLUSTER]({
        metadataMaxAge,
        allowAutoTopicCreation,
        maxInFlightRequests,
        isolationLevel,
        instrumentationEmitter
      });
      return createConsumer({
        retry: { ...this[PRIVATE.CLUSTER_RETRY], ...retry },
        logger: this[PRIVATE.LOGGER],
        cluster,
        groupId,
        partitionAssigners,
        sessionTimeout,
        rebalanceTimeout,
        heartbeatInterval,
        maxBytesPerPartition,
        minBytes,
        maxBytes,
        maxWaitTimeInMs,
        isolationLevel,
        instrumentationEmitter,
        rackId,
        metadataMaxAge
      });
    }
    admin({ retry } = {}) {
      const instrumentationEmitter = new InstrumentationEventEmitter;
      const cluster = this[PRIVATE.CREATE_CLUSTER]({
        allowAutoTopicCreation: false,
        instrumentationEmitter
      });
      return createAdmin({
        retry: { ...this[PRIVATE.CLUSTER_RETRY], ...retry },
        logger: this[PRIVATE.LOGGER],
        instrumentationEmitter,
        cluster
      });
    }
    logger() {
      return this[PRIVATE.LOGGER];
    }
  };
});

// node_modules/kafkajs/index.js
var require_kafkajs = __commonJS((exports, module) => {
  var Kafka = require_src();
  var PartitionAssigners = require_assigners();
  var AssignerProtocol = require_assignerProtocol();
  var Partitioners = require_partitioners();
  var Compression = require_compression();
  var ConfigResourceTypes = require_configResourceTypes();
  var ConfigSource = require_configSource();
  var AclResourceTypes = require_aclResourceTypes();
  var AclOperationTypes = require_aclOperationTypes();
  var AclPermissionTypes = require_aclPermissionTypes();
  var ResourcePatternTypes = require_resourcePatternTypes();
  var { isRebalancing, isKafkaJSError, ...errors } = require_errors();
  var { LEVELS } = require_loggers();
  module.exports = {
    Kafka,
    PartitionAssigners,
    AssignerProtocol,
    Partitioners,
    logLevel: LEVELS,
    CompressionTypes: Compression.Types,
    CompressionCodecs: Compression.Codecs,
    ConfigResourceTypes,
    AclResourceTypes,
    AclOperationTypes,
    AclPermissionTypes,
    ResourcePatternTypes,
    ConfigSource,
    ...errors
  };
});

// node_modules/dotenv/package.json
var require_package3 = __commonJS((exports, module) => {
  module.exports = {
    name: "dotenv",
    version: "16.4.5",
    description: "Loads environment variables from .env file",
    main: "lib/main.js",
    types: "lib/main.d.ts",
    exports: {
      ".": {
        types: "./lib/main.d.ts",
        require: "./lib/main.js",
        default: "./lib/main.js"
      },
      "./config": "./config.js",
      "./config.js": "./config.js",
      "./lib/env-options": "./lib/env-options.js",
      "./lib/env-options.js": "./lib/env-options.js",
      "./lib/cli-options": "./lib/cli-options.js",
      "./lib/cli-options.js": "./lib/cli-options.js",
      "./package.json": "./package.json"
    },
    scripts: {
      "dts-check": "tsc --project tests/types/tsconfig.json",
      lint: "standard",
      "lint-readme": "standard-markdown",
      pretest: "npm run lint && npm run dts-check",
      test: "tap tests/*.js --100 -Rspec",
      "test:coverage": "tap --coverage-report=lcov",
      prerelease: "npm test",
      release: "standard-version"
    },
    repository: {
      type: "git",
      url: "git://github.com/motdotla/dotenv.git"
    },
    funding: "https://dotenvx.com",
    keywords: [
      "dotenv",
      "env",
      ".env",
      "environment",
      "variables",
      "config",
      "settings"
    ],
    readmeFilename: "README.md",
    license: "BSD-2-Clause",
    devDependencies: {
      "@definitelytyped/dtslint": "^0.0.133",
      "@types/node": "^18.11.3",
      decache: "^4.6.1",
      sinon: "^14.0.1",
      standard: "^17.0.0",
      "standard-markdown": "^7.1.0",
      "standard-version": "^9.5.0",
      tap: "^16.3.0",
      tar: "^6.1.11",
      typescript: "^4.8.4"
    },
    engines: {
      node: ">=12"
    },
    browser: {
      fs: false
    }
  };
});

// node_modules/dotenv/lib/main.js
var require_main = __commonJS((exports, module) => {
  var parse = function(src) {
    const obj = {};
    let lines = src.toString();
    lines = lines.replace(/\r\n?/mg, "\n");
    let match;
    while ((match = LINE.exec(lines)) != null) {
      const key = match[1];
      let value = match[2] || "";
      value = value.trim();
      const maybeQuote = value[0];
      value = value.replace(/^(['"`])([\s\S]*)\1$/mg, "$2");
      if (maybeQuote === '"') {
        value = value.replace(/\\n/g, "\n");
        value = value.replace(/\\r/g, "\r");
      }
      obj[key] = value;
    }
    return obj;
  };
  var _parseVault = function(options) {
    const vaultPath = _vaultPath(options);
    const result = DotenvModule.configDotenv({ path: vaultPath });
    if (!result.parsed) {
      const err = new Error(`MISSING_DATA: Cannot parse ${vaultPath} for an unknown reason`);
      err.code = "MISSING_DATA";
      throw err;
    }
    const keys = _dotenvKey(options).split(",");
    const length = keys.length;
    let decrypted;
    for (let i = 0;i < length; i++) {
      try {
        const key = keys[i].trim();
        const attrs = _instructions(result, key);
        decrypted = DotenvModule.decrypt(attrs.ciphertext, attrs.key);
        break;
      } catch (error) {
        if (i + 1 >= length) {
          throw error;
        }
      }
    }
    return DotenvModule.parse(decrypted);
  };
  var _log = function(message) {
    console.log(`[dotenv@${version}][INFO] ${message}`);
  };
  var _warn = function(message) {
    console.log(`[dotenv@${version}][WARN] ${message}`);
  };
  var _debug = function(message) {
    console.log(`[dotenv@${version}][DEBUG] ${message}`);
  };
  var _dotenvKey = function(options) {
    if (options && options.DOTENV_KEY && options.DOTENV_KEY.length > 0) {
      return options.DOTENV_KEY;
    }
    if (process.env.DOTENV_KEY && process.env.DOTENV_KEY.length > 0) {
      return process.env.DOTENV_KEY;
    }
    return "";
  };
  var _instructions = function(result, dotenvKey) {
    let uri;
    try {
      uri = new URL(dotenvKey);
    } catch (error) {
      if (error.code === "ERR_INVALID_URL") {
        const err = new Error("INVALID_DOTENV_KEY: Wrong format. Must be in valid uri format like dotenv://:key_1234@dotenvx.com/vault/.env.vault?environment=development");
        err.code = "INVALID_DOTENV_KEY";
        throw err;
      }
      throw error;
    }
    const key = uri.password;
    if (!key) {
      const err = new Error("INVALID_DOTENV_KEY: Missing key part");
      err.code = "INVALID_DOTENV_KEY";
      throw err;
    }
    const environment = uri.searchParams.get("environment");
    if (!environment) {
      const err = new Error("INVALID_DOTENV_KEY: Missing environment part");
      err.code = "INVALID_DOTENV_KEY";
      throw err;
    }
    const environmentKey = `DOTENV_VAULT_${environment.toUpperCase()}`;
    const ciphertext = result.parsed[environmentKey];
    if (!ciphertext) {
      const err = new Error(`NOT_FOUND_DOTENV_ENVIRONMENT: Cannot locate environment ${environmentKey} in your .env.vault file.`);
      err.code = "NOT_FOUND_DOTENV_ENVIRONMENT";
      throw err;
    }
    return { ciphertext, key };
  };
  var _vaultPath = function(options) {
    let possibleVaultPath = null;
    if (options && options.path && options.path.length > 0) {
      if (Array.isArray(options.path)) {
        for (const filepath of options.path) {
          if (fs.existsSync(filepath)) {
            possibleVaultPath = filepath.endsWith(".vault") ? filepath : `${filepath}.vault`;
          }
        }
      } else {
        possibleVaultPath = options.path.endsWith(".vault") ? options.path : `${options.path}.vault`;
      }
    } else {
      possibleVaultPath = path.resolve(process.cwd(), ".env.vault");
    }
    if (fs.existsSync(possibleVaultPath)) {
      return possibleVaultPath;
    }
    return null;
  };
  var _resolveHome = function(envPath) {
    return envPath[0] === "~" ? path.join(os.homedir(), envPath.slice(1)) : envPath;
  };
  var _configVault = function(options) {
    _log("Loading env from encrypted .env.vault");
    const parsed = DotenvModule._parseVault(options);
    let processEnv = process.env;
    if (options && options.processEnv != null) {
      processEnv = options.processEnv;
    }
    DotenvModule.populate(processEnv, parsed, options);
    return { parsed };
  };
  var configDotenv = function(options) {
    const dotenvPath = path.resolve(process.cwd(), ".env");
    let encoding = "utf8";
    const debug = Boolean(options && options.debug);
    if (options && options.encoding) {
      encoding = options.encoding;
    } else {
      if (debug) {
        _debug("No encoding is specified. UTF-8 is used by default");
      }
    }
    let optionPaths = [dotenvPath];
    if (options && options.path) {
      if (!Array.isArray(options.path)) {
        optionPaths = [_resolveHome(options.path)];
      } else {
        optionPaths = [];
        for (const filepath of options.path) {
          optionPaths.push(_resolveHome(filepath));
        }
      }
    }
    let lastError;
    const parsedAll = {};
    for (const path2 of optionPaths) {
      try {
        const parsed = DotenvModule.parse(fs.readFileSync(path2, { encoding }));
        DotenvModule.populate(parsedAll, parsed, options);
      } catch (e) {
        if (debug) {
          _debug(`Failed to load ${path2} ${e.message}`);
        }
        lastError = e;
      }
    }
    let processEnv = process.env;
    if (options && options.processEnv != null) {
      processEnv = options.processEnv;
    }
    DotenvModule.populate(processEnv, parsedAll, options);
    if (lastError) {
      return { parsed: parsedAll, error: lastError };
    } else {
      return { parsed: parsedAll };
    }
  };
  var config2 = function(options) {
    if (_dotenvKey(options).length === 0) {
      return DotenvModule.configDotenv(options);
    }
    const vaultPath = _vaultPath(options);
    if (!vaultPath) {
      _warn(`You set DOTENV_KEY but you are missing a .env.vault file at ${vaultPath}. Did you forget to build it?`);
      return DotenvModule.configDotenv(options);
    }
    return DotenvModule._configVault(options);
  };
  var decrypt = function(encrypted, keyStr) {
    const key = Buffer.from(keyStr.slice(-64), "hex");
    let ciphertext = Buffer.from(encrypted, "base64");
    const nonce = ciphertext.subarray(0, 12);
    const authTag = ciphertext.subarray(-16);
    ciphertext = ciphertext.subarray(12, -16);
    try {
      const aesgcm = crypto2.createDecipheriv("aes-256-gcm", key, nonce);
      aesgcm.setAuthTag(authTag);
      return `${aesgcm.update(ciphertext)}${aesgcm.final()}`;
    } catch (error) {
      const isRange = error instanceof RangeError;
      const invalidKeyLength = error.message === "Invalid key length";
      const decryptionFailed = error.message === "Unsupported state or unable to authenticate data";
      if (isRange || invalidKeyLength) {
        const err = new Error("INVALID_DOTENV_KEY: It must be 64 characters long (or more)");
        err.code = "INVALID_DOTENV_KEY";
        throw err;
      } else if (decryptionFailed) {
        const err = new Error("DECRYPTION_FAILED: Please check your DOTENV_KEY");
        err.code = "DECRYPTION_FAILED";
        throw err;
      } else {
        throw error;
      }
    }
  };
  var populate = function(processEnv, parsed, options = {}) {
    const debug = Boolean(options && options.debug);
    const override = Boolean(options && options.override);
    if (typeof parsed !== "object") {
      const err = new Error("OBJECT_REQUIRED: Please check the processEnv argument being passed to populate");
      err.code = "OBJECT_REQUIRED";
      throw err;
    }
    for (const key of Object.keys(parsed)) {
      if (Object.prototype.hasOwnProperty.call(processEnv, key)) {
        if (override === true) {
          processEnv[key] = parsed[key];
        }
        if (debug) {
          if (override === true) {
            _debug(`"${key}" is already defined and WAS overwritten`);
          } else {
            _debug(`"${key}" is already defined and was NOT overwritten`);
          }
        }
      } else {
        processEnv[key] = parsed[key];
      }
    }
  };
  var fs = import.meta.require("fs");
  var path = import.meta.require("path");
  var os = import.meta.require("os");
  var crypto2 = import.meta.require("crypto");
  var packageJson = require_package3();
  var version = packageJson.version;
  var LINE = /(?:^|^)\s*(?:export\s+)?([\w.-]+)(?:\s*=\s*?|:\s+?)(\s*'(?:\\'|[^'])*'|\s*"(?:\\"|[^"])*"|\s*`(?:\\`|[^`])*`|[^#\r\n]+)?\s*(?:#.*)?(?:$|$)/mg;
  var DotenvModule = {
    configDotenv,
    _configVault,
    _parseVault,
    config: config2,
    decrypt,
    parse,
    populate
  };
  exports.configDotenv = DotenvModule.configDotenv;
  exports._configVault = DotenvModule._configVault;
  exports._parseVault = DotenvModule._parseVault;
  exports.config = DotenvModule.config;
  exports.decrypt = DotenvModule.decrypt;
  exports.parse = DotenvModule.parse;
  exports.populate = DotenvModule.populate;
  module.exports = DotenvModule;
});

// node_modules/dotenv/lib/env-options.js
var require_env_options = __commonJS((exports, module) => {
  var options = {};
  if (process.env.DOTENV_CONFIG_ENCODING != null) {
    options.encoding = process.env.DOTENV_CONFIG_ENCODING;
  }
  if (process.env.DOTENV_CONFIG_PATH != null) {
    options.path = process.env.DOTENV_CONFIG_PATH;
  }
  if (process.env.DOTENV_CONFIG_DEBUG != null) {
    options.debug = process.env.DOTENV_CONFIG_DEBUG;
  }
  if (process.env.DOTENV_CONFIG_OVERRIDE != null) {
    options.override = process.env.DOTENV_CONFIG_OVERRIDE;
  }
  if (process.env.DOTENV_CONFIG_DOTENV_KEY != null) {
    options.DOTENV_KEY = process.env.DOTENV_CONFIG_DOTENV_KEY;
  }
  module.exports = options;
});

// node_modules/dotenv/lib/cli-options.js
var require_cli_options = __commonJS((exports, module) => {
  var re = /^dotenv_config_(encoding|path|debug|override|DOTENV_KEY)=(.+)$/;
  module.exports = function optionMatcher(args) {
    return args.reduce(function(acc, cur) {
      const matches = cur.match(re);
      if (matches) {
        acc[matches[1]] = matches[2];
      }
      return acc;
    }, {});
  };
});

// node_modules/dotenv/config.js
var require_config = __commonJS(() => {
  (function() {
    require_main().config(Object.assign({}, require_env_options(), require_cli_options()(process.argv)));
  })();
});

// node_modules/@prisma/client/runtime/library.js
var require_library = __commonJS((exports, module) => {
  var no = function(e) {
    return typeof e == "function" ? e : (r) => r.$extends(e);
  };
  var io = function(e) {
    return e;
  };
  var oo = function(...e) {
    return (r) => r;
  };
  var F = function(e, r) {
    let t = new RegExp(`\\x1b\\[${r}m`, "g"), n = `\x1B[${e}m`, i = `\x1B[${r}m`;
    return function(o) {
      return !co.enabled || o == null ? o : n + (~("" + o).indexOf(i) ? o.replace(t, i + n) : o) + i;
    };
  };
  var su = function(e) {
    let r = { color: po[ou++ % po.length], enabled: $r.enabled(e), namespace: e, log: $r.log, extend: () => {
    } }, t = (...n) => {
      let { enabled: i, namespace: o, color: s, log: a } = r;
      if (n.length !== 0 && Mr.push([o, ...n]), Mr.length > iu && Mr.shift(), $r.enabled(o) || i) {
        let l = n.map((c) => typeof c == "string" ? c : au(c)), u = `+${Date.now() - mo}ms`;
        mo = Date.now(), globalThis.DEBUG_COLORS ? a(It[s](W(o)), ...l, It[s](u)) : a(o, ...l, u);
      }
    };
    return new Proxy(t, { get: (n, i) => r[i], set: (n, i, o) => r[i] = o });
  };
  var au = function(e, r = 2) {
    let t = new Set;
    return JSON.stringify(e, (n, i) => {
      if (typeof i == "object" && i !== null) {
        if (t.has(i))
          return "[Circular *]";
        t.add(i);
      } else if (typeof i == "bigint")
        return i.toString();
      return i;
    }, r);
  };
  var fo = function(e = 7500) {
    let r = Mr.map(([t, ...n]) => `${t} ${n.map((i) => typeof i == "string" ? i : JSON.stringify(i)).join(" ")}`).join(`
`);
    return r.length < e ? r : r.slice(-e);
  };
  var go = function() {
    Mr.length = 0;
  };
  var Fn = function() {
    let e = process.env.PRISMA_QUERY_ENGINE_LIBRARY;
    if (!(e && ho.default.existsSync(e)) && process.arch === "ia32")
      throw new Error('The default query engine type (Node-API, "library") is currently not supported for 32bit Node. Please set `engineType = "binary"` in the "generator" block of your "schema.prisma" file (or use the environment variables "PRISMA_CLIENT_ENGINE_TYPE=binary" and/or "PRISMA_CLI_QUERY_ENGINE_TYPE=binary".)');
  };
  var kt = function(e, r) {
    let t = r === "url";
    return e.includes("windows") ? t ? "query_engine.dll.node" : `query_engine-${e}.dll.node` : e.includes("darwin") ? t ? `${_t}.dylib.node` : `${_t}-${e}.dylib.node` : t ? `${_t}.so.node` : `${_t}-${e}.so.node`;
  };
  var fe = function(e) {
    return Object.assign(e, { optional: () => uu(e), and: (r) => B(e, r), or: (r) => cu(e, r), select: (r) => r === undefined ? yo(e) : yo(r, e) });
  };
  var uu = function(e) {
    return fe({ [ke]: () => ({ match: (r) => {
      let t = {}, n = (i, o) => {
        t[i] = o;
      };
      return r === undefined ? (Ve(e).forEach((i) => n(i, undefined)), { matched: true, selections: t }) : { matched: we(e, r, n), selections: t };
    }, getSelectionKeys: () => Ve(e), matcherType: "optional" }) });
  };
  var B = function(...e) {
    return fe({ [ke]: () => ({ match: (r) => {
      let t = {}, n = (i, o) => {
        t[i] = o;
      };
      return { matched: e.every((i) => we(i, r, n)), selections: t };
    }, getSelectionKeys: () => qr(e, Ve), matcherType: "and" }) });
  };
  var cu = function(...e) {
    return fe({ [ke]: () => ({ match: (r) => {
      let t = {}, n = (i, o) => {
        t[i] = o;
      };
      return qr(e, Ve).forEach((i) => n(i, undefined)), { matched: e.some((i) => we(i, r, n)), selections: t };
    }, getSelectionKeys: () => qr(e, Ve), matcherType: "or" }) });
  };
  var k = function(e) {
    return { [ke]: () => ({ match: (r) => ({ matched: !!e(r) }) }) };
  };
  var yo = function(...e) {
    let r = typeof e[0] == "string" ? e[0] : undefined, t = e.length === 2 ? e[1] : typeof e[0] == "string" ? undefined : e[0];
    return fe({ [ke]: () => ({ match: (n) => {
      let i = { [r ?? Lt]: n };
      return { matched: t === undefined || we(t, n, (o, s) => {
        i[o] = s;
      }), selections: i };
    }, getSelectionKeys: () => [r ?? Lt].concat(t === undefined ? [] : Ve(t)) }) });
  };
  var Ee = function(e) {
    return typeof e == "number";
  };
  var Xe = function(e) {
    return typeof e == "string";
  };
  var qe = function(e) {
    return typeof e == "bigint";
  };
  var mr = function(e) {
    return new Bn(e, qn);
  };
  var jr = function(e) {
    return (0, Io.default)(e, e, { fallback: ee });
  };
  var Ur = function(e, ...r) {
    hu.warn() && console.warn(`${gu.warn} ${e}`, ...r);
  };
  async function No() {
    let e = Ft.default.platform(), r = process.arch;
    if (e === "freebsd") {
      let s = await Mt("freebsd-version");
      if (s && s.trim().length > 0) {
        let l = /^(\d+)\.?/.exec(s);
        if (l)
          return { platform: "freebsd", targetDistro: `freebsd${l[1]}`, arch: r };
      }
    }
    if (e !== "linux")
      return { platform: e, arch: r };
    let t = await wu(), n = await Au(), i = Pu({ arch: r, archFromUname: n, familyDistro: t.familyDistro }), { libssl: o } = await vu(i);
    return { platform: "linux", libssl: o, arch: r, archFromUname: n, ...t };
  }
  var bu = function(e) {
    let r = /^ID="?([^"\n]*)"?$/im, t = /^ID_LIKE="?([^"\n]*)"?$/im, n = r.exec(e), i = n && n[1] && n[1].toLowerCase() || "", o = t.exec(e), s = o && o[1] && o[1].toLowerCase() || "", a = mr({ id: i, idLike: s }).with({ id: "alpine" }, ({ id: l }) => ({ targetDistro: "musl", familyDistro: l, originalDistro: l })).with({ id: "raspbian" }, ({ id: l }) => ({ targetDistro: "arm", familyDistro: "debian", originalDistro: l })).with({ id: "nixos" }, ({ id: l }) => ({ targetDistro: "nixos", originalDistro: l, familyDistro: "nixos" })).with({ id: "debian" }, { id: "ubuntu" }, ({ id: l }) => ({ targetDistro: "debian", familyDistro: "debian", originalDistro: l })).with({ id: "rhel" }, { id: "centos" }, { id: "fedora" }, ({ id: l }) => ({ targetDistro: "rhel", familyDistro: "rhel", originalDistro: l })).when(({ idLike: l }) => l.includes("debian") || l.includes("ubuntu"), ({ id: l }) => ({ targetDistro: "debian", familyDistro: "debian", originalDistro: l })).when(({ idLike: l }) => i === "arch" || l.includes("arch"), ({ id: l }) => ({ targetDistro: "debian", familyDistro: "arch", originalDistro: l })).when(({ idLike: l }) => l.includes("centos") || l.includes("fedora") || l.includes("rhel") || l.includes("suse"), ({ id: l }) => ({ targetDistro: "rhel", familyDistro: "rhel", originalDistro: l })).otherwise(({ id: l }) => ({ targetDistro: undefined, familyDistro: undefined, originalDistro: l }));
    return ie(`Found distro info:
${JSON.stringify(a, null, 2)}`), a;
  };
  async function wu() {
    let e = "/etc/os-release";
    try {
      let r = await Hn.default.readFile(e, { encoding: "utf-8" });
      return bu(r);
    } catch {
      return { targetDistro: undefined, familyDistro: undefined, originalDistro: undefined };
    }
  }
  var xu = function(e) {
    let r = /^OpenSSL\s(\d+\.\d+)\.\d+/.exec(e);
    if (r) {
      let t = `${r[1]}.x`;
      return Oo(t);
    }
  };
  var _o = function(e) {
    let r = /libssl\.so\.(\d)(\.\d)?/.exec(e);
    if (r) {
      let t = `${r[1]}${r[2] ?? ".0"}.x`;
      return Oo(t);
    }
  };
  var Oo = function(e) {
    let r = (() => {
      if (Mo(e))
        return e;
      let t = e.split(".");
      return t[1] = "0", t.join(".");
    })();
    if (Eu.includes(r))
      return r;
  };
  var Pu = function(e) {
    return mr(e).with({ familyDistro: "musl" }, () => (ie('Trying platform-specific paths for "alpine"'), ["/lib"])).with({ familyDistro: "debian" }, ({ archFromUname: r }) => (ie('Trying platform-specific paths for "debian" (and "ubuntu")'), [`/usr/lib/${r}-linux-gnu`, `/lib/${r}-linux-gnu`])).with({ familyDistro: "rhel" }, () => (ie('Trying platform-specific paths for "rhel"'), ["/lib64", "/usr/lib64"])).otherwise(({ familyDistro: r, arch: t, archFromUname: n }) => (ie(`Don't know any platform-specific paths for "${r}" on ${t} (${n})`), []));
  };
  async function vu(e) {
    let r = 'grep -v "libssl.so.0"', t = await ko(e);
    if (t) {
      ie(`Found libssl.so file using platform-specific paths: ${t}`);
      let o = _o(t);
      if (ie(`The parsed libssl version is: ${o}`), o)
        return { libssl: o, strategy: "libssl-specific-path" };
    }
    ie('Falling back to "ldconfig" and other generic paths');
    let n = await Mt(`ldconfig -p | sed "s/.*=>s*//" | sed "s|.*/||" | grep libssl | sort | ${r}`);
    if (n || (n = await ko(["/lib64", "/usr/lib64", "/lib"])), n) {
      ie(`Found libssl.so file using "ldconfig" or other generic paths: ${n}`);
      let o = _o(n);
      if (ie(`The parsed libssl version is: ${o}`), o)
        return { libssl: o, strategy: "ldconfig" };
    }
    let i = await Mt("openssl version -v");
    if (i) {
      ie(`Found openssl binary with version: ${i}`);
      let o = xu(i);
      if (ie(`The parsed openssl version is: ${o}`), o)
        return { libssl: o, strategy: "openssl-binary" };
    }
    return ie("Couldn't find any version of libssl or OpenSSL in the system"), {};
  }
  async function ko(e) {
    for (let r of e) {
      let t = await Tu(r);
      if (t)
        return t;
    }
  }
  async function Tu(e) {
    try {
      return (await Hn.default.readdir(e)).find((t) => t.startsWith("libssl.so.") && !t.startsWith("libssl.so.0"));
    } catch (r) {
      if (r.code === "ENOENT")
        return;
      throw r;
    }
  }
  async function rr() {
    let { binaryTarget: e } = await Fo();
    return e;
  }
  var Cu = function(e) {
    return e.binaryTarget !== undefined;
  };
  async function Wn() {
    let { memoized: e, ...r } = await Fo();
    return r;
  }
  async function Fo() {
    if (Cu(Ot))
      return Promise.resolve({ ...Ot, memoized: true });
    let e = await No(), r = Su(e);
    return Ot = { ...e, binaryTarget: r }, { ...Ot, memoized: false };
  }
  var Su = function(e) {
    let { platform: r, arch: t, archFromUname: n, libssl: i, targetDistro: o, familyDistro: s, originalDistro: a } = e;
    r === "linux" && !["x64", "arm64"].includes(t) && Ur(`Prisma only officially supports Linux on amd64 (x86_64) and arm64 (aarch64) system architectures. If you are using your own custom Prisma engines, you can ignore this warning, as long as you've compiled the engines for your system architecture "${n}".`);
    let l = "1.1.x";
    if (r === "linux" && i === undefined) {
      let c = mr({ familyDistro: s }).with({ familyDistro: "debian" }, () => "Please manually install OpenSSL via `apt-get update -y && apt-get install -y openssl` and try installing Prisma again. If you're running Prisma on Docker, add this command to your Dockerfile, or switch to an image that already has OpenSSL installed.").otherwise(() => "Please manually install OpenSSL and try installing Prisma again.");
      Ur(`Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-${l}".
${c}`);
    }
    let u = "debian";
    if (r === "linux" && o === undefined && Ur(`Prisma doesn't know which engines to download for the Linux distro "${a}". Falling back to Prisma engines built "${u}".
Please report your experience by creating an issue at ${jr("https://github.com/prisma/prisma/issues")} so we can add your distro to the list of known supported distros.`), r === "darwin" && t === "arm64")
      return "darwin-arm64";
    if (r === "darwin")
      return "darwin";
    if (r === "win32")
      return "windows";
    if (r === "freebsd")
      return o;
    if (r === "openbsd")
      return "openbsd";
    if (r === "netbsd")
      return "netbsd";
    if (r === "linux" && o === "nixos")
      return "linux-nixos";
    if (r === "linux" && t === "arm64")
      return `${o === "musl" ? "linux-musl-arm64" : "linux-arm64"}-openssl-${i || l}`;
    if (r === "linux" && t === "arm")
      return `linux-arm-openssl-${i || l}`;
    if (r === "linux" && o === "musl") {
      let c = "linux-musl";
      return !i || Mo(i) ? c : `${c}-openssl-${i}`;
    }
    return r === "linux" && o && i ? `${o}-openssl-${i}` : (r !== "linux" && Ur(`Prisma detected unknown OS "${r}" and may not work as expected. Defaulting to "linux".`), i ? `${u}-openssl-${i}` : o ? `${o}-openssl-${l}` : `${u}-openssl-${l}`);
  };
  async function Ru(e) {
    try {
      return await e();
    } catch {
      return;
    }
  }
  var Mt = function(e) {
    return Ru(async () => {
      let r = await yu(e);
      return ie(`Command "${e}" successfully returned "${r.stdout}"`), r.stdout;
    });
  };
  async function Au() {
    return typeof Ft.default.machine == "function" ? Ft.default.machine() : (await Mt("uname -m"))?.trim();
  }
  var Mo = function(e) {
    return e.startsWith("1.");
  };
  var Vo = function(e) {
    let r = e.ignoreProcessEnv ? {} : process.env, t = (n) => n.match(/(.?\${(?:[a-zA-Z0-9_]+)?})/g)?.reduce(function(o, s) {
      let a = /(.?)\${([a-zA-Z0-9_]+)?}/g.exec(s);
      if (!a)
        return o;
      let l = a[1], u, c;
      if (l === "\\")
        c = a[0], u = c.replace("\\$", "$");
      else {
        let p = a[2];
        c = a[0].substring(l.length), u = Object.hasOwnProperty.call(r, p) ? r[p] : e.parsed[p] || "", u = t(u);
      }
      return o.replace(c, u);
    }, n) ?? n;
    for (let n in e.parsed) {
      let i = Object.hasOwnProperty.call(r, n) ? r[n] : e.parsed[n];
      e.parsed[n] = t(i);
    }
    for (let n in e.parsed)
      r[n] = e.parsed[n];
    return e;
  };
  var Qr = function({ rootEnvPath: e, schemaEnvPath: r }, t = { conflictCheck: "none" }) {
    let n = jo(e);
    t.conflictCheck !== "none" && $u(n, r, t.conflictCheck);
    let i = null;
    return Uo(n?.path, r) || (i = jo(r)), !n && !i && zn("No Environment variables loaded"), i?.dotenvResult.error ? console.error(ce(W("Schema Env Error: ")) + i.dotenvResult.error) : { message: [n?.message, i?.message].filter(Boolean).join(`
`), parsed: { ...n?.dotenvResult?.parsed, ...i?.dotenvResult?.parsed } };
  };
  var $u = function(e, r, t) {
    let n = e?.dotenvResult.parsed, i = !Uo(e?.path, r);
    if (n && r && i && Bt.default.existsSync(r)) {
      let o = Yn.default.parse(Bt.default.readFileSync(r)), s = [];
      for (let a in o)
        n[a] === o[a] && s.push(a);
      if (s.length > 0) {
        let a = gr.default.relative(process.cwd(), e.path), l = gr.default.relative(process.cwd(), r);
        if (t === "error") {
          let u = `There is a conflict between env var${s.length > 1 ? "s" : ""} in ${ee(a)} and ${ee(l)}
Conflicting env vars:
${s.map((c) => `  ${W(c)}`).join(`
`)}

We suggest to move the contents of ${ee(l)} to ${ee(a)} to consolidate your env vars.
`;
          throw new Error(u);
        } else if (t === "warn") {
          let u = `Conflict for env var${s.length > 1 ? "s" : ""} ${s.map((c) => W(c)).join(", ")} in ${ee(a)} and ${ee(l)}
Env vars from ${ee(l)} overwrite the ones from ${ee(a)}
      `;
          console.warn(`${de("warn(prisma)")} ${u}`);
        }
      }
    }
  };
  var jo = function(e) {
    if (qu(e)) {
      zn(`Environment variables loaded from ${e}`);
      let r = Yn.default.config({ path: e, debug: process.env.DOTENV_CONFIG_DEBUG ? true : undefined });
      return { dotenvResult: Vo(r), message: Ie(`Environment variables loaded from ${gr.default.relative(process.cwd(), e)}`), path: e };
    } else
      zn(`Environment variables not found at ${e}`);
    return null;
  };
  var Uo = function(e, r) {
    return e && r && gr.default.resolve(e) === gr.default.resolve(r);
  };
  var qu = function(e) {
    return !!(e && Bt.default.existsSync(e));
  };
  var Gr = function(e) {
    let r = Bu();
    return r || (e?.config.engineType === "library" ? "library" : e?.config.engineType === "binary" ? "binary" : Qo);
  };
  var Bu = function() {
    let e = process.env.PRISMA_CLIENT_ENGINE_TYPE;
    return e === "library" ? "library" : e === "binary" ? "binary" : undefined;
  };
  var Ko = function() {
    return M.default.join(__dirname, "../");
  };
  var ri = function(e) {
    if (process.platform === "win32")
      return;
    let r = ei.default.statSync(e), t = r.mode | 64 | 8 | 1;
    if (r.mode === t) {
      zo(`Execution permissions of ${e} are fine`);
      return;
    }
    let n = t.toString(8).slice(-3);
    zo(`Have to call chmodPlusX on ${e}`), ei.default.chmodSync(e, n);
  };
  var ti = function(e) {
    let r = e.e, t = (a) => `Prisma cannot find the required \`${a}\` system library in your system`, n = r.message.includes("cannot open shared object file"), i = `Please refer to the documentation about Prisma's system requirements: ${jr("https://pris.ly/d/system-requirements")}`, o = `Unable to require(\`${Ie(e.id)}\`).`, s = mr({ message: r.message, code: r.code }).with({ code: "ENOENT" }, () => "File does not exist.").when(({ message: a }) => n && a.includes("libz"), () => `${t("libz")}. Please install it and try again.`).when(({ message: a }) => n && a.includes("libgcc_s"), () => `${t("libgcc_s")}. Please install it and try again.`).when(({ message: a }) => n && a.includes("libssl"), () => {
      let a = e.platformInfo.libssl ? `openssl-${e.platformInfo.libssl}` : "openssl";
      return `${t("libssl")}. Please install ${a} and try again.`;
    }).when(({ message: a }) => a.includes("GLIBC"), () => `Prisma has detected an incompatible version of the \`glibc\` C standard library installed in your system. This probably means your system may be too old to run Prisma. ${i}`).when(({ message: a }) => e.platformInfo.platform === "linux" && a.includes("symbol not found"), () => `The Prisma engines are not compatible with your system ${e.platformInfo.originalDistro} on (${e.platformInfo.archFromUname}) which uses the \`${e.platformInfo.binaryTarget}\` binaryTarget by default. ${i}`).otherwise(() => `The Prisma engines do not seem to be compatible with your system. ${i}`);
    return `${o}
${s}

Details: ${r.message}`;
  };
  var ni = function(e) {
    return Jr.default.sep === Jr.default.posix.sep ? e : e.split(Jr.default.sep).join(Jr.default.posix.sep);
  };
  var si = function(e) {
    return String(new oi(e));
  };
  var Gu = function(e) {
    let r;
    if (e.length > 0) {
      let t = e.find((n) => n.fromEnvVar !== null);
      t ? r = `env("${t.fromEnvVar}")` : r = e.map((n) => n.native ? "native" : n.value);
    } else
      r = undefined;
    return r;
  };
  var Ju = function(e) {
    let r = Object.keys(e).reduce((t, n) => Math.max(t, n.length), 0);
    return Object.entries(e).map(([t, n]) => `${t.padEnd(r)} = ${Hu(n)}`).join(`
`);
  };
  var Hu = function(e) {
    return JSON.parse(JSON.stringify(e, (r, t) => Array.isArray(t) ? `[${t.map((n) => JSON.stringify(n)).join(", ")}]` : JSON.stringify(t)));
  };
  var Wu = function(...e) {
    console.log(...e);
  };
  var ai = function(e, ...r) {
    Xo.warn() && console.warn(`${Hr.warn} ${e}`, ...r);
  };
  var Ku = function(e, ...r) {
    console.info(`${Hr.info} ${e}`, ...r);
  };
  var zu = function(e, ...r) {
    console.error(`${Hr.error} ${e}`, ...r);
  };
  var Yu = function(e, ...r) {
    console.log(`${Hr.query} ${e}`, ...r);
  };
  var jt = function(e, r) {
    if (!e)
      throw new Error(`${r}. This should never happen. If you see this error, please, open an issue at https://pris.ly/prisma-prisma-bug-report`);
  };
  var tr = function(e, r) {
    throw new Error(r);
  };
  var ui = function(e, r) {
    return Object.prototype.hasOwnProperty.call(e, r);
  };
  var hr = function(e, r) {
    let t = {};
    for (let n of Object.keys(e))
      t[n] = r(e[n], n);
    return t;
  };
  var pi = function(e, r) {
    if (e.length === 0)
      return;
    let t = e[0];
    for (let n = 1;n < e.length; n++)
      r(t, e[n]) < 0 && (t = e[n]);
    return t;
  };
  var w = function(e, r) {
    Object.defineProperty(e, "name", { value: r, configurable: true });
  };
  var zr = function(e) {
    let r;
    return { get() {
      return r || (r = { value: e() }), r.value;
    } };
  };
  var os = function(e, r) {
    let t = zr(() => Xu(r));
    Object.defineProperty(e, "dmmf", { get: () => t.get() });
  };
  var Xu = function(e) {
    return { datamodel: { models: mi(e.models), enums: mi(e.enums), types: mi(e.types) } };
  };
  var mi = function(e) {
    return Object.entries(e).map(([r, t]) => ({ name: r, ...t }));
  };
  var fi = function(e, r) {
    Object.defineProperty(e, "name", { value: r, configurable: true });
  };
  var rt = function(e) {
    return { ok: false, error: e, map() {
      return rt(e);
    }, flatMap() {
      return rt(e);
    } };
  };
  var nr = function(e, r) {
    return async (...t) => {
      try {
        return await r(...t);
      } catch (n) {
        let i = e.registerNewError(n);
        return rt({ kind: "GenericJs", id: i });
      }
    };
  };
  var rc = function(e, r) {
    return (...t) => {
      try {
        return r(...t);
      } catch (n) {
        let i = e.registerNewError(n);
        return rt({ kind: "GenericJs", id: i });
      }
    };
  };
  var ss = function(e, r = ",", t = "", n = "") {
    if (e.length === 0)
      throw new TypeError("Expected `join([])` to be called with an array of multiple elements, but got an empty array");
    return new oe([t, ...Array(e.length - 1).fill(r), n], e);
  };
  var yi = function(e) {
    return new oe([e], []);
  };
  var Ei = function(e, ...r) {
    return new oe(e, r);
  };
  var tt = function(e) {
    return { getKeys() {
      return Object.keys(e);
    }, getPropertyValue(r) {
      return e[r];
    } };
  };
  var te = function(e, r) {
    return { getKeys() {
      return [e];
    }, getPropertyValue() {
      return r();
    } };
  };
  var ir = function(e) {
    let r = new xe;
    return { getKeys() {
      return e.getKeys();
    }, getPropertyValue(t) {
      return r.getOrCreate(t, () => e.getPropertyValue(t));
    }, getPropertyDescriptor(t) {
      return e.getPropertyDescriptor?.(t);
    } };
  };
  var Wt = function(e) {
    let r = new Set(e);
    return { getOwnPropertyDescriptor: () => Ht, has: (t, n) => r.has(n), set: (t, n, i) => r.add(n) && Reflect.set(t, n, i), ownKeys: () => [...r] };
  };
  var Pe = function(e, r) {
    let t = tc(r), n = new Set, i = new Proxy(e, { get(o, s) {
      if (n.has(s))
        return o[s];
      let a = t.get(s);
      return a ? a.getPropertyValue(s) : o[s];
    }, has(o, s) {
      if (n.has(s))
        return true;
      let a = t.get(s);
      return a ? a.has?.(s) ?? true : Reflect.has(o, s);
    }, ownKeys(o) {
      let s = us(Reflect.ownKeys(o), t), a = us(Array.from(t.keys()), t);
      return [...new Set([...s, ...a, ...n])];
    }, set(o, s, a) {
      return t.get(s)?.getPropertyDescriptor?.(s)?.writable === false ? false : (n.add(s), Reflect.set(o, s, a));
    }, getOwnPropertyDescriptor(o, s) {
      let a = Reflect.getOwnPropertyDescriptor(o, s);
      if (a && !a.configurable)
        return a;
      let l = t.get(s);
      return l ? l.getPropertyDescriptor ? { ...Ht, ...l?.getPropertyDescriptor(s) } : Ht : a;
    }, defineProperty(o, s, a) {
      return n.add(s), Reflect.defineProperty(o, s, a);
    } });
    return i[ls] = function(o, s, a = cs.inspect) {
      let l = { ...this };
      return delete l[ls], a(l, s);
    }, i;
  };
  var tc = function(e) {
    let r = new Map;
    for (let t of e) {
      let n = t.getKeys();
      for (let i of n)
        r.set(i, t);
    }
    return r;
  };
  var us = function(e, r) {
    return e.filter((t) => r.get(t)?.has?.(t) ?? true);
  };
  var nt = function(e) {
    return { getKeys() {
      return e;
    }, has() {
      return false;
    }, getPropertyValue() {
    } };
  };
  var Er = function(e, r) {
    return { batch: e, transaction: r?.kind === "batch" ? { isolationLevel: r.options.isolationLevel } : undefined };
  };
  var ps = function(e) {
    return e.substring(0, 1).toLowerCase() + e.substring(1);
  };
  var wr = function(e) {
    return e instanceof Date || Object.prototype.toString.call(e) === "[object Date]";
  };
  var Kt = function(e) {
    return e.toString() !== "Invalid Date";
  };
  var z = function(e) {
    var r, t, n, i = e.length - 1, o = "", s = e[0];
    if (i > 0) {
      for (o += s, r = 1;r < i; r++)
        n = e[r] + "", t = b - n.length, t && (o += Ue(t)), o += n;
      s = e[r], n = s + "", t = b - n.length, t && (o += Ue(t));
    } else if (s === 0)
      return "0";
    for (;s % 10 === 0; )
      s /= 10;
    return o + s;
  };
  var se = function(e, r, t) {
    if (e !== ~~e || e < r || e > t)
      throw Error(Ge + e);
  };
  var it = function(e, r, t, n) {
    var i, o, s, a;
    for (o = e[0];o >= 10; o /= 10)
      --r;
    return --r < 0 ? (r += b, i = 0) : (i = Math.ceil((r + 1) / b), r %= b), o = Q(10, b - r), a = e[i] % o | 0, n == null ? r < 3 ? (r == 0 ? a = a / 100 | 0 : r == 1 && (a = a / 10 | 0), s = t < 4 && a == 99999 || t > 3 && a == 49999 || a == 50000 || a == 0) : s = (t < 4 && a + 1 == o || t > 3 && a + 1 == o / 2) && (e[i + 1] / o / 100 | 0) == Q(10, r - 2) - 1 || (a == o / 2 || a == 0) && (e[i + 1] / o / 100 | 0) == 0 : r < 4 ? (r == 0 ? a = a / 1000 | 0 : r == 1 ? a = a / 100 | 0 : r == 2 && (a = a / 10 | 0), s = (n || t < 4) && a == 9999 || !n && t > 3 && a == 4999) : s = ((n || t < 4) && a + 1 == o || !n && t > 3 && a + 1 == o / 2) && (e[i + 1] / o / 1000 | 0) == Q(10, r - 3) - 1, s;
  };
  var zt = function(e, r, t) {
    for (var n, i = [0], o, s = 0, a = e.length;s < a; ) {
      for (o = i.length;o--; )
        i[o] *= r;
      for (i[0] += bi.indexOf(e.charAt(s++)), n = 0;n < i.length; n++)
        i[n] > t - 1 && (i[n + 1] === undefined && (i[n + 1] = 0), i[n + 1] += i[n] / t | 0, i[n] %= t);
    }
    return i.reverse();
  };
  var lc = function(e, r) {
    var t, n, i;
    if (r.isZero())
      return r;
    n = r.d.length, n < 32 ? (t = Math.ceil(n / 3), i = (1 / tn(4, t)).toString()) : (t = 16, i = "2.3283064365386962890625e-10"), e.precision += t, r = Pr(e, 1, r.times(i), new e(1));
    for (var o = t;o--; ) {
      var s = r.times(r);
      r = s.times(s).minus(s).times(8).plus(1);
    }
    return e.precision -= t, r;
  };
  var y = function(e, r, t, n) {
    var i, o, s, a, l, u, c, p, m, f = e.constructor;
    e:
      if (r != null) {
        if (p = e.d, !p)
          return e;
        for (i = 1, a = p[0];a >= 10; a /= 10)
          i++;
        if (o = r - i, o < 0)
          o += b, s = r, c = p[m = 0], l = c / Q(10, i - s - 1) % 10 | 0;
        else if (m = Math.ceil((o + 1) / b), a = p.length, m >= a)
          if (n) {
            for (;a++ <= m; )
              p.push(0);
            c = l = 0, i = 1, o %= b, s = o - b + 1;
          } else
            break e;
        else {
          for (c = a = p[m], i = 1;a >= 10; a /= 10)
            i++;
          o %= b, s = o - b + i, l = s < 0 ? 0 : c / Q(10, i - s - 1) % 10 | 0;
        }
        if (n = n || r < 0 || p[m + 1] !== undefined || (s < 0 ? c : c % Q(10, i - s - 1)), u = t < 4 ? (l || n) && (t == 0 || t == (e.s < 0 ? 3 : 2)) : l > 5 || l == 5 && (t == 4 || n || t == 6 && (o > 0 ? s > 0 ? c / Q(10, i - s) : 0 : p[m - 1]) % 10 & 1 || t == (e.s < 0 ? 8 : 7)), r < 1 || !p[0])
          return p.length = 0, u ? (r -= e.e + 1, p[0] = Q(10, (b - r % b) % b), e.e = -r || 0) : p[0] = e.e = 0, e;
        if (o == 0 ? (p.length = m, a = 1, m--) : (p.length = m + 1, a = Q(10, b - o), p[m] = s > 0 ? (c / Q(10, i - s) % Q(10, s) | 0) * a : 0), u)
          for (;; )
            if (m == 0) {
              for (o = 1, s = p[0];s >= 10; s /= 10)
                o++;
              for (s = p[0] += a, a = 1;s >= 10; s /= 10)
                a++;
              o != a && (e.e++, p[0] == he && (p[0] = 1));
              break;
            } else {
              if (p[m] += a, p[m] != he)
                break;
              p[m--] = 0, a = 1;
            }
        for (o = p.length;p[--o] === 0; )
          p.pop();
      }
    return x && (e.e > f.maxE ? (e.d = null, e.e = NaN) : e.e < f.minE && (e.e = 0, e.d = [0])), e;
  };
  var ve = function(e, r, t) {
    if (!e.isFinite())
      return vs(e);
    var n, i = e.e, o = z(e.d), s = o.length;
    return r ? (t && (n = t - s) > 0 ? o = o.charAt(0) + "." + o.slice(1) + Ue(n) : s > 1 && (o = o.charAt(0) + "." + o.slice(1)), o = o + (e.e < 0 ? "e" : "e+") + e.e) : i < 0 ? (o = "0." + Ue(-i - 1) + o, t && (n = t - s) > 0 && (o += Ue(n))) : i >= s ? (o += Ue(i + 1 - s), t && (n = t - i - 1) > 0 && (o = o + "." + Ue(n))) : ((n = i + 1) < s && (o = o.slice(0, n) + "." + o.slice(n)), t && (n = t - s) > 0 && (i + 1 === s && (o += "."), o += Ue(n))), o;
  };
  var rn = function(e, r) {
    var t = e[0];
    for (r *= b;t >= 10; t /= 10)
      r++;
    return r;
  };
  var Xt = function(e, r, t) {
    if (r > ac)
      throw x = true, t && (e.precision = t), Error(hs);
    return y(new e(Yt), r, 1, true);
  };
  var ge = function(e, r, t) {
    if (r > xi)
      throw Error(hs);
    return y(new e(Zt), r, t, true);
  };
  var ws = function(e) {
    var r = e.length - 1, t = r * b + 1;
    if (r = e[r], r) {
      for (;r % 10 == 0; r /= 10)
        t--;
      for (r = e[0];r >= 10; r /= 10)
        t++;
    }
    return t;
  };
  var Ue = function(e) {
    for (var r = "";e--; )
      r += "0";
    return r;
  };
  var xs = function(e, r, t, n) {
    var i, o = new e(1), s = Math.ceil(n / b + 4);
    for (x = false;; ) {
      if (t % 2 && (o = o.times(r), ds(o.d, s) && (i = true)), t = re(t / 2), t === 0) {
        t = o.d.length - 1, i && o.d[t] === 0 && ++o.d[t];
        break;
      }
      r = r.times(r), ds(r.d, s);
    }
    return x = true, o;
  };
  var ms = function(e) {
    return e.d[e.d.length - 1] & 1;
  };
  var Ps = function(e, r, t) {
    for (var n, i = new e(r[0]), o = 0;++o < r.length; )
      if (n = new e(r[o]), n.s)
        i[t](n) && (i = n);
      else {
        i = n;
        break;
      }
    return i;
  };
  var Pi = function(e, r) {
    var t, n, i, o, s, a, l, u = 0, c = 0, p = 0, m = e.constructor, f = m.rounding, g = m.precision;
    if (!e.d || !e.d[0] || e.e > 17)
      return new m(e.d ? e.d[0] ? e.s < 0 ? 0 : 1 / 0 : 1 : e.s ? e.s < 0 ? 0 : e : NaN);
    for (r == null ? (x = false, l = g) : l = r, a = new m(0.03125);e.e > -2; )
      e = e.times(a), p += 5;
    for (n = Math.log(Q(2, p)) / Math.LN10 * 2 + 5 | 0, l += n, t = o = s = new m(1), m.precision = l;; ) {
      if (o = y(o.times(e), l, 1), t = t.times(++c), a = s.plus(O(o, t, l, 1)), z(a.d).slice(0, l) === z(s.d).slice(0, l)) {
        for (i = p;i--; )
          s = y(s.times(s), l, 1);
        if (r == null)
          if (u < 3 && it(s.d, l - n, f, u))
            m.precision = l += 10, t = o = a = new m(1), c = 0, u++;
          else
            return y(s, m.precision = g, f, x = true);
        else
          return m.precision = g, s;
      }
      s = a;
    }
  };
  var Qe = function(e, r) {
    var t, n, i, o, s, a, l, u, c, p, m, f = 1, g = 10, h = e, A = h.d, T = h.constructor, C = T.rounding, E = T.precision;
    if (h.s < 0 || !A || !A[0] || !h.e && A[0] == 1 && A.length == 1)
      return new T(A && !A[0] ? -1 / 0 : h.s != 1 ? NaN : A ? 0 : h);
    if (r == null ? (x = false, c = E) : c = r, T.precision = c += g, t = z(A), n = t.charAt(0), Math.abs(o = h.e) < 1500000000000000) {
      for (;n < 7 && n != 1 || n == 1 && t.charAt(1) > 3; )
        h = h.times(e), t = z(h.d), n = t.charAt(0), f++;
      o = h.e, n > 1 ? (h = new T("0." + t), o++) : h = new T(n + "." + t.slice(1));
    } else
      return u = Xt(T, c + 2, E).times(o + ""), h = Qe(new T(n + "." + t.slice(1)), c - g).plus(u), T.precision = E, r == null ? y(h, E, C, x = true) : h;
    for (p = h, l = s = h = O(h.minus(1), h.plus(1), c, 1), m = y(h.times(h), c, 1), i = 3;; ) {
      if (s = y(s.times(m), c, 1), u = l.plus(O(s, new T(i), c, 1)), z(u.d).slice(0, c) === z(l.d).slice(0, c))
        if (l = l.times(2), o !== 0 && (l = l.plus(Xt(T, c + 2, E).times(o + ""))), l = O(l, new T(f), c, 1), r == null)
          if (it(l.d, c - g, C, a))
            T.precision = c += g, u = s = h = O(p.minus(1), p.plus(1), c, 1), m = y(h.times(h), c, 1), i = a = 1;
          else
            return y(l, T.precision = E, C, x = true);
        else
          return T.precision = E, l;
      l = u, i += 2;
    }
  };
  var vs = function(e) {
    return String(e.s * e.s / 0);
  };
  var vi = function(e, r) {
    var t, n, i;
    for ((t = r.indexOf(".")) > -1 && (r = r.replace(".", "")), (n = r.search(/e/i)) > 0 ? (t < 0 && (t = n), t += +r.slice(n + 1), r = r.substring(0, n)) : t < 0 && (t = r.length), n = 0;r.charCodeAt(n) === 48; n++)
      ;
    for (i = r.length;r.charCodeAt(i - 1) === 48; --i)
      ;
    if (r = r.slice(n, i), r) {
      if (i -= n, e.e = t = t - n - 1, e.d = [], n = (t + 1) % b, t < 0 && (n += b), n < i) {
        for (n && e.d.push(+r.slice(0, n)), i -= b;n < i; )
          e.d.push(+r.slice(n, n += b));
        r = r.slice(n), n = b - r.length;
      } else
        n -= i;
      for (;n--; )
        r += "0";
      e.d.push(+r), x && (e.e > e.constructor.maxE ? (e.d = null, e.e = NaN) : e.e < e.constructor.minE && (e.e = 0, e.d = [0]));
    } else
      e.e = 0, e.d = [0];
    return e;
  };
  var uc = function(e, r) {
    var t, n, i, o, s, a, l, u, c;
    if (r.indexOf("_") > -1) {
      if (r = r.replace(/(\d)_(?=\d)/g, "$1"), bs.test(r))
        return vi(e, r);
    } else if (r === "Infinity" || r === "NaN")
      return +r || (e.s = NaN), e.e = NaN, e.d = null, e;
    if (ic.test(r))
      t = 16, r = r.toLowerCase();
    else if (nc.test(r))
      t = 2;
    else if (oc.test(r))
      t = 8;
    else
      throw Error(Ge + r);
    for (o = r.search(/p/i), o > 0 ? (l = +r.slice(o + 1), r = r.substring(2, o)) : r = r.slice(2), o = r.indexOf("."), s = o >= 0, n = e.constructor, s && (r = r.replace(".", ""), a = r.length, o = a - o, i = xs(n, new n(t), o, o * 2)), u = zt(r, t, he), c = u.length - 1, o = c;u[o] === 0; --o)
      u.pop();
    return o < 0 ? new n(e.s * 0) : (e.e = rn(u, c), e.d = u, x = false, s && (e = O(e, i, a * 4)), l && (e = e.times(Math.abs(l) < 54 ? Q(2, l) : or.pow(2, l))), x = true, e);
  };
  var cc = function(e, r) {
    var t, n = r.d.length;
    if (n < 3)
      return r.isZero() ? r : Pr(e, 2, r, r);
    t = 1.4 * Math.sqrt(n), t = t > 16 ? 16 : t | 0, r = r.times(1 / tn(5, t)), r = Pr(e, 2, r, r);
    for (var i, o = new e(5), s = new e(16), a = new e(20);t--; )
      i = r.times(r), r = r.times(o.plus(i.times(s.times(i).minus(a))));
    return r;
  };
  var Pr = function(e, r, t, n, i) {
    var o, s, a, l, u = 1, c = e.precision, p = Math.ceil(c / b);
    for (x = false, l = t.times(t), a = new e(n);; ) {
      if (s = O(a.times(l), new e(r++ * r++), c, 1), a = i ? n.plus(s) : n.minus(s), n = O(s.times(l), new e(r++ * r++), c, 1), s = a.plus(n), s.d[p] !== undefined) {
        for (o = p;s.d[o] === a.d[o] && o--; )
          ;
        if (o == -1)
          break;
      }
      o = a, a = n, n = s, s = o, u++;
    }
    return x = true, s.d.length = p + 1, s;
  };
  var tn = function(e, r) {
    for (var t = e;--r; )
      t *= e;
    return t;
  };
  var Ts = function(e, r) {
    var t, n = r.s < 0, i = ge(e, e.precision, 1), o = i.times(0.5);
    if (r = r.abs(), r.lte(o))
      return Oe = n ? 4 : 1, r;
    if (t = r.divToInt(i), t.isZero())
      Oe = n ? 3 : 2;
    else {
      if (r = r.minus(t.times(i)), r.lte(o))
        return Oe = ms(t) ? n ? 2 : 3 : n ? 4 : 1, r;
      Oe = ms(t) ? n ? 1 : 4 : n ? 3 : 2;
    }
    return r.minus(i).abs();
  };
  var Ti = function(e, r, t, n) {
    var i, o, s, a, l, u, c, p, m, f = e.constructor, g = t !== undefined;
    if (g ? (se(t, 1, Je), n === undefined ? n = f.rounding : se(n, 0, 8)) : (t = f.precision, n = f.rounding), !e.isFinite())
      c = vs(e);
    else {
      for (c = ve(e), s = c.indexOf("."), g ? (i = 2, r == 16 ? t = t * 4 - 3 : r == 8 && (t = t * 3 - 2)) : i = r, s >= 0 && (c = c.replace(".", ""), m = new f(1), m.e = c.length - s, m.d = zt(ve(m), 10, i), m.e = m.d.length), p = zt(c, 10, i), o = l = p.length;p[--l] == 0; )
        p.pop();
      if (!p[0])
        c = g ? "0p+0" : "0";
      else {
        if (s < 0 ? o-- : (e = new f(e), e.d = p, e.e = o, e = O(e, m, t, n, 0, i), p = e.d, o = e.e, u = gs), s = p[t], a = i / 2, u = u || p[t + 1] !== undefined, u = n < 4 ? (s !== undefined || u) && (n === 0 || n === (e.s < 0 ? 3 : 2)) : s > a || s === a && (n === 4 || u || n === 6 && p[t - 1] & 1 || n === (e.s < 0 ? 8 : 7)), p.length = t, u)
          for (;++p[--t] > i - 1; )
            p[t] = 0, t || (++o, p.unshift(1));
        for (l = p.length;!p[l - 1]; --l)
          ;
        for (s = 0, c = "";s < l; s++)
          c += bi.charAt(p[s]);
        if (g) {
          if (l > 1)
            if (r == 16 || r == 8) {
              for (s = r == 16 ? 4 : 3, --l;l % s; l++)
                c += "0";
              for (p = zt(c, i, r), l = p.length;!p[l - 1]; --l)
                ;
              for (s = 1, c = "1.";s < l; s++)
                c += bi.charAt(p[s]);
            } else
              c = c.charAt(0) + "." + c.slice(1);
          c = c + (o < 0 ? "p" : "p+") + o;
        } else if (o < 0) {
          for (;++o; )
            c = "0" + c;
          c = "0." + c;
        } else if (++o > l)
          for (o -= l;o--; )
            c += "0";
        else
          o < l && (c = c.slice(0, o) + "." + c.slice(o));
      }
      c = (r == 16 ? "0x" : r == 2 ? "0b" : r == 8 ? "0o" : "") + c;
    }
    return e.s < 0 ? "-" + c : c;
  };
  var ds = function(e, r) {
    if (e.length > r)
      return e.length = r, true;
  };
  var pc = function(e) {
    return new this(e).abs();
  };
  var mc = function(e) {
    return new this(e).acos();
  };
  var dc = function(e) {
    return new this(e).acosh();
  };
  var fc = function(e, r) {
    return new this(e).plus(r);
  };
  var gc = function(e) {
    return new this(e).asin();
  };
  var hc = function(e) {
    return new this(e).asinh();
  };
  var yc = function(e) {
    return new this(e).atan();
  };
  var Ec = function(e) {
    return new this(e).atanh();
  };
  var bc = function(e, r) {
    e = new this(e), r = new this(r);
    var t, n = this.precision, i = this.rounding, o = n + 4;
    return !e.s || !r.s ? t = new this(NaN) : !e.d && !r.d ? (t = ge(this, o, 1).times(r.s > 0 ? 0.25 : 0.75), t.s = e.s) : !r.d || e.isZero() ? (t = r.s < 0 ? ge(this, n, i) : new this(0), t.s = e.s) : !e.d || r.isZero() ? (t = ge(this, o, 1).times(0.5), t.s = e.s) : r.s < 0 ? (this.precision = o, this.rounding = 1, t = this.atan(O(e, r, o, 1)), r = ge(this, o, 1), this.precision = n, this.rounding = i, t = e.s < 0 ? t.minus(r) : t.plus(r)) : t = this.atan(O(e, r, o, 1)), t;
  };
  var wc = function(e) {
    return new this(e).cbrt();
  };
  var xc = function(e) {
    return y(e = new this(e), e.e + 1, 2);
  };
  var Pc = function(e, r, t) {
    return new this(e).clamp(r, t);
  };
  var vc = function(e) {
    if (!e || typeof e != "object")
      throw Error(en + "Object expected");
    var r, t, n, i = e.defaults === true, o = ["precision", 1, Je, "rounding", 0, 8, "toExpNeg", -xr, 0, "toExpPos", 0, xr, "maxE", 0, xr, "minE", -xr, 0, "modulo", 0, 9];
    for (r = 0;r < o.length; r += 3)
      if (t = o[r], i && (this[t] = wi[t]), (n = e[t]) !== undefined)
        if (re(n) === n && n >= o[r + 1] && n <= o[r + 2])
          this[t] = n;
        else
          throw Error(Ge + t + ": " + n);
    if (t = "crypto", i && (this[t] = wi[t]), (n = e[t]) !== undefined)
      if (n === true || n === false || n === 0 || n === 1)
        if (n)
          if (typeof crypto < "u" && crypto && (crypto.getRandomValues || crypto.randomBytes))
            this[t] = true;
          else
            throw Error(ys);
        else
          this[t] = false;
      else
        throw Error(Ge + t + ": " + n);
    return this;
  };
  var Tc = function(e) {
    return new this(e).cos();
  };
  var Cc = function(e) {
    return new this(e).cosh();
  };
  var Cs = function(e) {
    var r, t, n;
    function i(o) {
      var s, a, l, u = this;
      if (!(u instanceof i))
        return new i(o);
      if (u.constructor = i, fs(o)) {
        u.s = o.s, x ? !o.d || o.e > i.maxE ? (u.e = NaN, u.d = null) : o.e < i.minE ? (u.e = 0, u.d = [0]) : (u.e = o.e, u.d = o.d.slice()) : (u.e = o.e, u.d = o.d ? o.d.slice() : o.d);
        return;
      }
      if (l = typeof o, l === "number") {
        if (o === 0) {
          u.s = 1 / o < 0 ? -1 : 1, u.e = 0, u.d = [0];
          return;
        }
        if (o < 0 ? (o = -o, u.s = -1) : u.s = 1, o === ~~o && o < 1e7) {
          for (s = 0, a = o;a >= 10; a /= 10)
            s++;
          x ? s > i.maxE ? (u.e = NaN, u.d = null) : s < i.minE ? (u.e = 0, u.d = [0]) : (u.e = s, u.d = [o]) : (u.e = s, u.d = [o]);
          return;
        } else if (o * 0 !== 0) {
          o || (u.s = NaN), u.e = NaN, u.d = null;
          return;
        }
        return vi(u, o.toString());
      } else if (l !== "string")
        throw Error(Ge + o);
      return (a = o.charCodeAt(0)) === 45 ? (o = o.slice(1), u.s = -1) : (a === 43 && (o = o.slice(1)), u.s = 1), bs.test(o) ? vi(u, o) : uc(u, o);
    }
    if (i.prototype = d, i.ROUND_UP = 0, i.ROUND_DOWN = 1, i.ROUND_CEIL = 2, i.ROUND_FLOOR = 3, i.ROUND_HALF_UP = 4, i.ROUND_HALF_DOWN = 5, i.ROUND_HALF_EVEN = 6, i.ROUND_HALF_CEIL = 7, i.ROUND_HALF_FLOOR = 8, i.EUCLID = 9, i.config = i.set = vc, i.clone = Cs, i.isDecimal = fs, i.abs = pc, i.acos = mc, i.acosh = dc, i.add = fc, i.asin = gc, i.asinh = hc, i.atan = yc, i.atanh = Ec, i.atan2 = bc, i.cbrt = wc, i.ceil = xc, i.clamp = Pc, i.cos = Tc, i.cosh = Cc, i.div = Sc, i.exp = Rc, i.floor = Ac, i.hypot = Ic, i.ln = _c, i.log = kc, i.log10 = Lc, i.log2 = Dc, i.max = Nc, i.min = Oc, i.mod = Fc, i.mul = Mc, i.pow = $c, i.random = qc, i.round = Bc, i.sign = Vc, i.sin = jc, i.sinh = Uc, i.sqrt = Qc, i.sub = Gc, i.sum = Jc, i.tan = Hc, i.tanh = Wc, i.trunc = Kc, e === undefined && (e = {}), e && e.defaults !== true)
      for (n = ["precision", "rounding", "toExpNeg", "toExpPos", "maxE", "minE", "modulo", "crypto"], r = 0;r < n.length; )
        e.hasOwnProperty(t = n[r++]) || (e[t] = this[t]);
    return i.config(e), i;
  };
  var Sc = function(e, r) {
    return new this(e).div(r);
  };
  var Rc = function(e) {
    return new this(e).exp();
  };
  var Ac = function(e) {
    return y(e = new this(e), e.e + 1, 3);
  };
  var Ic = function() {
    var e, r, t = new this(0);
    for (x = false, e = 0;e < arguments.length; )
      if (r = new this(arguments[e++]), r.d)
        t.d && (t = t.plus(r.times(r)));
      else {
        if (r.s)
          return x = true, new this(1 / 0);
        t = r;
      }
    return x = true, t.sqrt();
  };
  var fs = function(e) {
    return e instanceof or || e && e.toStringTag === Es || false;
  };
  var _c = function(e) {
    return new this(e).ln();
  };
  var kc = function(e, r) {
    return new this(e).log(r);
  };
  var Dc = function(e) {
    return new this(e).log(2);
  };
  var Lc = function(e) {
    return new this(e).log(10);
  };
  var Nc = function() {
    return Ps(this, arguments, "lt");
  };
  var Oc = function() {
    return Ps(this, arguments, "gt");
  };
  var Fc = function(e, r) {
    return new this(e).mod(r);
  };
  var Mc = function(e, r) {
    return new this(e).mul(r);
  };
  var $c = function(e, r) {
    return new this(e).pow(r);
  };
  var qc = function(e) {
    var r, t, n, i, o = 0, s = new this(1), a = [];
    if (e === undefined ? e = this.precision : se(e, 1, Je), n = Math.ceil(e / b), this.crypto)
      if (crypto.getRandomValues)
        for (r = crypto.getRandomValues(new Uint32Array(n));o < n; )
          i = r[o], i >= 4290000000 ? r[o] = crypto.getRandomValues(new Uint32Array(1))[0] : a[o++] = i % 1e7;
      else if (crypto.randomBytes) {
        for (r = crypto.randomBytes(n *= 4);o < n; )
          i = r[o] + (r[o + 1] << 8) + (r[o + 2] << 16) + ((r[o + 3] & 127) << 24), i >= 2140000000 ? crypto.randomBytes(4).copy(r, o) : (a.push(i % 1e7), o += 4);
        o = n / 4;
      } else
        throw Error(ys);
    else
      for (;o < n; )
        a[o++] = Math.random() * 1e7 | 0;
    for (n = a[--o], e %= b, n && e && (i = Q(10, b - e), a[o] = (n / i | 0) * i);a[o] === 0; o--)
      a.pop();
    if (o < 0)
      t = 0, a = [0];
    else {
      for (t = -1;a[0] === 0; t -= b)
        a.shift();
      for (n = 1, i = a[0];i >= 10; i /= 10)
        n++;
      n < b && (t -= b - n);
    }
    return s.e = t, s.d = a, s;
  };
  var Bc = function(e) {
    return y(e = new this(e), e.e + 1, this.rounding);
  };
  var Vc = function(e) {
    return e = new this(e), e.d ? e.d[0] ? e.s : 0 * e.s : e.s || NaN;
  };
  var jc = function(e) {
    return new this(e).sin();
  };
  var Uc = function(e) {
    return new this(e).sinh();
  };
  var Qc = function(e) {
    return new this(e).sqrt();
  };
  var Gc = function(e, r) {
    return new this(e).sub(r);
  };
  var Jc = function() {
    var e = 0, r = arguments, t = new this(r[e]);
    for (x = false;t.s && ++e < r.length; )
      t = t.plus(r[e]);
    return x = true, y(t, this.precision, this.rounding);
  };
  var Hc = function(e) {
    return new this(e).tan();
  };
  var Wc = function(e) {
    return new this(e).tanh();
  };
  var Kc = function(e) {
    return y(e = new this(e), e.e + 1, 1);
  };
  var vr = function(e) {
    return or.isDecimal(e) ? true : e !== null && typeof e == "object" && typeof e.s == "number" && typeof e.e == "number" && typeof e.toFixed == "function" && Array.isArray(e.d);
  };
  var Tr = function(e) {
    return e instanceof ot;
  };
  var ln = function(e) {
    return new Ci(As(e));
  };
  var As = function(e) {
    let r = new J;
    for (let [t, n] of Object.entries(e)) {
      let i = new an(t, Is(n));
      r.addField(i);
    }
    return r;
  };
  var Is = function(e) {
    if (typeof e == "string")
      return new H(JSON.stringify(e));
    if (typeof e == "number" || typeof e == "boolean")
      return new H(String(e));
    if (typeof e == "bigint")
      return new H(`${e}n`);
    if (e === null)
      return new H("null");
    if (e === undefined)
      return new H("undefined");
    if (vr(e))
      return new H(`new Prisma.Decimal("${e.toFixed()}")`);
    if (e instanceof Uint8Array)
      return Buffer.isBuffer(e) ? new H(`Buffer.alloc(${e.byteLength})`) : new H(`new Uint8Array(${e.byteLength})`);
    if (e instanceof Date) {
      let r = Kt(e) ? e.toISOString() : "Invalid Date";
      return new H(`new Date("${r}")`);
    }
    return e instanceof Ne ? new H(`Prisma.${e._getName()}`) : Tr(e) ? new H(`prisma.${ps(e.modelName)}.\$fields.${e.name}`) : Array.isArray(e) ? Yc(e) : typeof e == "object" ? As(e) : new H(Object.prototype.toString.call(e));
  };
  var Yc = function(e) {
    let r = new Sr;
    for (let t of e)
      r.addItem(Is(t));
    return r;
  };
  var _s = function(e) {
    if (e === undefined)
      return "";
    let r = ln(e);
    return new br(0, { colors: sn }).write(r).toString();
  };
  var sr = function({ error: e, user_facing_error: r }, t, n) {
    return r.error_code ? new V(Xc(r, n), { code: r.error_code, clientVersion: t, meta: r.meta, batchRequestIdx: r.batch_request_idx }) : new j(e, { clientVersion: t, batchRequestIdx: r.batch_request_idx });
  };
  var Xc = function(e, r) {
    let t = e.message;
    return (r === "postgresql" || r === "postgres" || r === "mysql") && e.error_code === Zc && (t += `
Prisma Accelerate has built-in connection pooling to prevent such errors: https://pris.ly/client/error-accelerate`), t;
  };
  var ks = function(e) {
    var r = e.split(`
`);
    return r.reduce(function(t, n) {
      var i = tp(n) || ip(n) || ap(n) || pp(n) || up(n);
      return i && t.push(i), t;
    }, []);
  };
  var tp = function(e) {
    var r = ep.exec(e);
    if (!r)
      return null;
    var t = r[2] && r[2].indexOf("native") === 0, n = r[2] && r[2].indexOf("eval") === 0, i = rp.exec(r[2]);
    return n && i != null && (r[2] = i[1], r[3] = i[2], r[4] = i[3]), { file: t ? null : r[2], methodName: r[1] || st, arguments: t ? [r[2]] : [], lineNumber: r[3] ? +r[3] : null, column: r[4] ? +r[4] : null };
  };
  var ip = function(e) {
    var r = np.exec(e);
    return r ? { file: r[2], methodName: r[1] || st, arguments: [], lineNumber: +r[3], column: r[4] ? +r[4] : null } : null;
  };
  var ap = function(e) {
    var r = op.exec(e);
    if (!r)
      return null;
    var t = r[3] && r[3].indexOf(" > eval") > -1, n = sp.exec(r[3]);
    return t && n != null && (r[3] = n[1], r[4] = n[2], r[5] = null), { file: r[3], methodName: r[1] || st, arguments: r[2] ? r[2].split(",") : [], lineNumber: r[4] ? +r[4] : null, column: r[5] ? +r[5] : null };
  };
  var up = function(e) {
    var r = lp.exec(e);
    return r ? { file: r[3], methodName: r[1] || st, arguments: [], lineNumber: +r[4], column: r[5] ? +r[5] : null } : null;
  };
  var pp = function(e) {
    var r = cp.exec(e);
    return r ? { file: r[2], methodName: r[1] || st, arguments: [], lineNumber: +r[3], column: r[4] ? +r[4] : null } : null;
  };
  var We = function(e) {
    return e === "minimal" ? typeof $EnabledCallSite == "function" && e !== "minimal" ? new $EnabledCallSite : new Si : new Ri;
  };
  var Rr = function(e = {}) {
    let r = dp(e);
    return Object.entries(r).reduce((n, [i, o]) => (Ds[i] !== undefined ? n.select[i] = { select: o } : n[i] = o, n), { select: {} });
  };
  var dp = function(e = {}) {
    return typeof e._count == "boolean" ? { ...e, _count: { _all: e._count } } : e;
  };
  var un = function(e = {}) {
    return (r) => (typeof e._count == "boolean" && (r._count = r._count._all), r);
  };
  var Ls = function(e, r) {
    let t = un(e);
    return r({ action: "aggregate", unpacker: t, argsMapper: Rr })(e);
  };
  var fp = function(e = {}) {
    let { select: r, ...t } = e;
    return typeof r == "object" ? Rr({ ...t, _count: r }) : Rr({ ...t, _count: { _all: true } });
  };
  var gp = function(e = {}) {
    return typeof e.select == "object" ? (r) => un(e)(r)._count : (r) => un(e)(r)._count._all;
  };
  var Ns = function(e, r) {
    return r({ action: "count", unpacker: gp(e), argsMapper: fp })(e);
  };
  var hp = function(e = {}) {
    let r = Rr(e);
    if (Array.isArray(r.by))
      for (let t of r.by)
        typeof t == "string" && (r.select[t] = true);
    else
      typeof r.by == "string" && (r.select[r.by] = true);
    return r;
  };
  var yp = function(e = {}) {
    return (r) => (typeof e?._count == "boolean" && r.forEach((t) => {
      t._count = t._count._all;
    }), r);
  };
  var Os = function(e, r) {
    return r({ action: "groupBy", unpacker: yp(e), argsMapper: hp })(e);
  };
  var Fs = function(e, r, t) {
    if (r === "aggregate")
      return (n) => Ls(n, t);
    if (r === "count")
      return (n) => Ns(n, t);
    if (r === "groupBy")
      return (n) => Os(n, t);
  };
  var Ms = function(e, r) {
    let t = r.fields.filter((i) => !i.relationName), n = ci(t, (i) => i.name);
    return new Proxy({}, { get(i, o) {
      if (o in i || typeof o == "symbol")
        return i[o];
      let s = n[o];
      if (s)
        return new ot(e, o, s.type, s.isList, s.kind === "enum");
    }, ...Wt(Object.keys(n)) });
  };
  var Ep = function(e, r) {
    return e === undefined || r === undefined ? [] : [...r, "select", e];
  };
  var bp = function(e, r, t) {
    return r === undefined ? e ?? {} : qs(r, t, e || true);
  };
  var Ii = function(e, r, t, n, i, o) {
    let a = e._runtimeDataModel.models[r].fields.reduce((l, u) => ({ ...l, [u.name]: u }), {});
    return (l) => {
      let u = We(e._errorFormat), c = Ep(n, i), p = bp(l, o, c), m = t({ dataPath: c, callsite: u })(p), f = wp(e, r);
      return new Proxy(m, { get(g, h) {
        if (!f.includes(h))
          return g[h];
        let T = [a[h].type, t, h], C = [c, p];
        return Ii(e, ...T, ...C);
      }, ...Wt([...f, ...Object.getOwnPropertyNames(m)]) });
    };
  };
  var wp = function(e, r) {
    return e._runtimeDataModel.models[r].fields.filter((t) => t.kind === "object").map((t) => t.name);
  };
  var ye = function(e, r, t, n, i) {
    this.type = e, this.content = r, this.alias = t, this.length = (n || "").length | 0, this.greedy = !!i;
  };
  var vp = function(e) {
    return Bs[e] || xp;
  };
  var Vs = function(e) {
    return Tp(e, P.languages.javascript);
  };
  var Tp = function(e, r) {
    return P.tokenize(e, r).map((n) => ye.stringify(n)).join("");
  };
  var Us = function(e) {
    return (0, js.default)(e);
  };
  var Rp = function({ message: e, originalMethod: r, isPanic: t, callArguments: n }) {
    return { functionName: `prisma.${r}()`, message: e, isPanic: t ?? false, callArguments: n };
  };
  var Ap = function({ callsite: e, message: r, originalMethod: t, isPanic: n, callArguments: i }, o) {
    let s = Rp({ message: r, originalMethod: t, isPanic: n, callArguments: i });
    if (!e || typeof window < "u" || false)
      return s;
    let a = e.getLocation();
    if (!a || !a.lineNumber || !a.columnNumber)
      return s;
    let l = Math.max(1, a.lineNumber - 3), u = pn.read(a.fileName)?.slice(l, a.lineNumber), c = u?.lineAt(a.lineNumber);
    if (u && c) {
      let p = _p(c), m = Ip(c);
      if (!m)
        return s;
      s.functionName = `${m.code})`, s.location = a, n || (u = u.mapLineAt(a.lineNumber, (g) => g.slice(0, m.openingBraceIndex))), u = o.highlightSource(u);
      let f = String(u.lastLineNumber).length;
      if (s.contextLines = u.mapLines((g, h) => o.gray(String(h).padStart(f)) + " " + g).mapLines((g) => o.dim(g)).prependSymbolAt(a.lineNumber, o.bold(o.red("\u2192"))), i) {
        let g = p + f + 1;
        g += 2, s.callArguments = (0, Gs.default)(i, g).slice(g);
      }
    }
    return s;
  };
  var Ip = function(e) {
    let r = Object.keys(De.ModelAction).join("|"), n = new RegExp(String.raw`\.(${r})\(`).exec(e);
    if (n) {
      let i = n.index + n[0].length, o = e.lastIndexOf(" ", n.index) + 1;
      return { code: e.slice(o, i), openingBraceIndex: i };
    }
    return null;
  };
  var _p = function(e) {
    let r = 0;
    for (let t = 0;t < e.length; t++) {
      if (e.charAt(t) !== " ")
        return r;
      r++;
    }
    return r;
  };
  var kp = function({ functionName: e, location: r, message: t, isPanic: n, contextLines: i, callArguments: o }, s) {
    let a = [""], l = r ? " in" : ":";
    if (n ? (a.push(s.red(`Oops, an unknown error occurred! This is ${s.bold("on us")}, you did nothing wrong.`)), a.push(s.red(`It occurred in the ${s.bold(`\`${e}\``)} invocation${l}`))) : a.push(s.red(`Invalid ${s.bold(`\`${e}\``)} invocation${l}`)), r && a.push(s.underline(Dp(r))), i) {
      a.push("");
      let u = [i.toString()];
      o && (u.push(o), u.push(s.dim(")"))), a.push(u.join("")), o && a.push("");
    } else
      a.push(""), o && a.push(o), a.push("");
    return a.push(t), a.join(`
`);
  };
  var Dp = function(e) {
    let r = [e.fileName];
    return e.lineNumber && r.push(String(e.lineNumber)), e.columnNumber && r.push(String(e.columnNumber)), r.join(":");
  };
  var Ar = function(e) {
    let r = e.showColors ? Cp : Sp, t;
    return t = Ap(e, r), kp(t, r);
  };
  var Js = function(e, r, t, n) {
    return e === De.ModelAction.findFirstOrThrow || e === De.ModelAction.findUniqueOrThrow ? Lp(r, t, n) : n;
  };
  var Lp = function(e, r, t) {
    return async (n) => {
      if ("rejectOnNotFound" in n.args) {
        let o = Ar({ originalMethod: n.clientMethod, callsite: n.callsite, message: "'rejectOnNotFound' option is not supported" });
        throw new K(o, { clientVersion: r });
      }
      return await t(n).catch((o) => {
        throw o instanceof V && o.code === "P2025" ? new Le(`No ${e} found`, r) : o;
      });
    };
  };
  var Se = function(e) {
    return e.replace(/^./, (r) => r.toLowerCase());
  };
  var _i = function(e, r) {
    let t = e._extensions.getAllModelExtensions(r) ?? {}, n = [Fp(e, r), $p(e, r), tt(t), te("name", () => r), te("$name", () => r), te("$parent", () => e._appliedParent)];
    return Pe({}, n);
  };
  var Fp = function(e, r) {
    let t = Se(r), n = Object.keys(De.ModelAction).concat("count");
    return { getKeys() {
      return n;
    }, getPropertyValue(i) {
      let o = i, s = (l) => e._request(l);
      s = Js(o, r, e._clientVersion, s);
      let a = (l) => (u) => {
        let c = We(e._errorFormat);
        return e._createPrismaPromise((p) => {
          let m = { args: u, dataPath: [], action: o, model: r, clientMethod: `${t}.${i}`, jsModelName: t, transaction: p, callsite: c };
          return s({ ...m, ...l });
        });
      };
      return Np.includes(o) ? Ii(e, r, a) : Mp(i) ? Fs(e, i, a) : a({});
    } };
  };
  var Mp = function(e) {
    return Op.includes(e);
  };
  var $p = function(e, r) {
    return ir(te("fields", () => {
      let t = e._runtimeDataModel.models[r];
      return Ms(r, t);
    }));
  };
  var Hs = function(e) {
    return e.replace(/^./, (r) => r.toUpperCase());
  };
  var at = function(e) {
    let r = [qp(e), te(ki, () => e), te("$parent", () => e._appliedParent)], t = e._extensions.getAllClientExtensions();
    return t && r.push(tt(t)), Pe(e, r);
  };
  var qp = function(e) {
    let r = Object.keys(e._runtimeDataModel.models), t = r.map(Se), n = [...new Set(r.concat(t))];
    return ir({ getKeys() {
      return n;
    }, getPropertyValue(i) {
      let o = Hs(i);
      if (e._runtimeDataModel.models[o] !== undefined)
        return _i(e, o);
      if (e._runtimeDataModel.models[i] !== undefined)
        return _i(e, i);
    }, getPropertyDescriptor(i) {
      if (!t.includes(i))
        return { enumerable: false };
    } });
  };
  var Ws = function(e) {
    return e[ki] ? e[ki] : e;
  };
  var Ks = function(e) {
    if (typeof e == "function")
      return e(this);
    if (e.client?.__AccelerateEngine) {
      let t = e.client.__AccelerateEngine;
      this._originalClient._engine = new t(this._originalClient._accelerateEngineConfig);
    }
    let r = Object.create(this._originalClient, { _extensions: { value: this._extensions.append(e) }, _appliedParent: { value: this, configurable: true }, $use: { value: undefined }, $on: { value: undefined } });
    return at(r);
  };
  var zs = function({ result: e, modelName: r, select: t, extensions: n }) {
    let i = n.getAllComputedFields(r);
    if (!i)
      return e;
    let o = [], s = [];
    for (let a of Object.values(i)) {
      if (t) {
        if (!t[a.name])
          continue;
        let l = a.needs.filter((u) => !t[u]);
        l.length > 0 && s.push(nt(l));
      }
      Bp(e, a.needs) && o.push(Vp(a, Pe(e, o)));
    }
    return o.length > 0 || s.length > 0 ? Pe(e, [...o, ...s]) : e;
  };
  var Bp = function(e, r) {
    return r.every((t) => ui(e, t));
  };
  var Vp = function(e, r) {
    return ir(te(e.name, () => e.compute(r)));
  };
  var mn = function({ visitor: e, result: r, args: t, runtimeDataModel: n, modelName: i }) {
    if (Array.isArray(r)) {
      for (let s = 0;s < r.length; s++)
        r[s] = mn({ result: r[s], args: t, modelName: i, runtimeDataModel: n, visitor: e });
      return r;
    }
    let o = e(r, i, t) ?? r;
    return t.include && Ys({ includeOrSelect: t.include, result: o, parentModelName: i, runtimeDataModel: n, visitor: e }), t.select && Ys({ includeOrSelect: t.select, result: o, parentModelName: i, runtimeDataModel: n, visitor: e }), o;
  };
  var Ys = function({ includeOrSelect: e, result: r, parentModelName: t, runtimeDataModel: n, visitor: i }) {
    for (let [o, s] of Object.entries(e)) {
      if (!s || r[o] == null)
        continue;
      let l = n.models[t].fields.find((c) => c.name === o);
      if (!l || l.kind !== "object" || !l.relationName)
        continue;
      let u = typeof s == "object" ? s : {};
      r[o] = mn({ visitor: i, result: r[o], args: u, modelName: l.type, runtimeDataModel: n });
    }
  };
  var Zs = function({ result: e, modelName: r, args: t, extensions: n, runtimeDataModel: i }) {
    return n.isEmpty() || e == null || typeof e != "object" || !i.models[r] ? e : mn({ result: e, args: t ?? {}, modelName: r, runtimeDataModel: i, visitor: (s, a, l) => zs({ result: s, modelName: Se(a), select: l.select, extensions: n }) });
  };
  var Xs = function(e) {
    if (e instanceof oe)
      return jp(e);
    if (Array.isArray(e)) {
      let t = [e[0]];
      for (let n = 1;n < e.length; n++)
        t[n] = lt(e[n]);
      return t;
    }
    let r = {};
    for (let t in e)
      r[t] = lt(e[t]);
    return r;
  };
  var jp = function(e) {
    return new oe(e.strings, e.values);
  };
  var lt = function(e) {
    if (typeof e != "object" || e == null || e instanceof Ne || Tr(e))
      return e;
    if (vr(e))
      return new Te(e.toFixed());
    if (wr(e))
      return new Date(+e);
    if (ArrayBuffer.isView(e))
      return e.slice(0);
    if (Array.isArray(e)) {
      let r = e.length, t;
      for (t = Array(r);r--; )
        t[r] = lt(e[r]);
      return t;
    }
    if (typeof e == "object") {
      let r = {};
      for (let t in e)
        t === "__proto__" ? Object.defineProperty(r, t, { value: lt(e[t]), configurable: true, enumerable: true, writable: true }) : r[t] = lt(e[t]);
      return r;
    }
    tr(e, "Unknown value");
  };
  var ra = function(e, r, t, n = 0) {
    return e._createPrismaPromise((i) => {
      let o = r.customDataProxyFetch;
      return "transaction" in r && i !== undefined && (r.transaction?.kind === "batch" && r.transaction.lock.then(), r.transaction = i), n === t.length ? e._executeRequest(r) : t[n]({ model: r.model, operation: r.model ? r.action : r.clientMethod, args: Xs(r.args ?? {}), __internalParams: r, query: (s, a = r) => {
        let l = a.customDataProxyFetch;
        return a.customDataProxyFetch = oa(o, l), a.args = s, ra(e, a, t, n + 1);
      } });
    });
  };
  var ta = function(e, r) {
    let { jsModelName: t, action: n, clientMethod: i } = r, o = t ? n : i;
    if (e._extensions.isEmpty())
      return e._executeRequest(r);
    let s = e._extensions.getAllQueryCallbacks(t ?? "$none", o);
    return ra(e, r, s);
  };
  var na = function(e) {
    return (r) => {
      let t = { requests: r }, n = r[0].extensions.getAllBatchQueryCallbacks();
      return n.length ? ia(t, n, 0, e) : e(t);
    };
  };
  var ia = function(e, r, t, n) {
    if (t === r.length)
      return n(e);
    let i = e.customDataProxyFetch, o = e.requests[0].transaction;
    return r[t]({ args: { queries: e.requests.map((s) => ({ model: s.modelName, operation: s.action, args: s.args })), transaction: o ? { isolationLevel: o.kind === "batch" ? o.isolationLevel : undefined } : undefined }, __internalParams: e, query(s, a = e) {
      let l = a.customDataProxyFetch;
      return a.customDataProxyFetch = oa(i, l), ia(a, r, t + 1, n);
    } });
  };
  var oa = function(e = ea, r = ea) {
    return (t) => e(r(t));
  };
  var aa = function(e, r, t) {
    let n = Se(t);
    return !r.result || !(r.result.$allModels || r.result[n]) ? e : Up({ ...e, ...sa(r.name, e, r.result.$allModels), ...sa(r.name, e, r.result[n]) });
  };
  var Up = function(e) {
    let r = new xe, t = (n, i) => r.getOrCreate(n, () => i.has(n) ? [n] : (i.add(n), e[n] ? e[n].needs.flatMap((o) => t(o, i)) : [n]));
    return hr(e, (n) => ({ ...n, needs: t(n.name, new Set) }));
  };
  var sa = function(e, r, t) {
    return t ? hr(t, ({ needs: n, compute: i }, o) => ({ name: o, needs: n ? Object.keys(n).filter((s) => n[s]) : [], compute: Qp(r, o, i) })) : {};
  };
  var Qp = function(e, r, t) {
    let n = e?.[r]?.compute;
    return n ? (i) => t({ ...i, [r]: n(i) }) : t;
  };
  var la = function(e, r) {
    if (!r)
      return e;
    let t = { ...e };
    for (let n of Object.values(r))
      if (e[n.name])
        for (let i of n.needs)
          t[i] = true;
    return t;
  };
  var pa = function({ postinstall: e, ciName: r, clientVersion: t }) {
    if (ua("checkPlatformCaching:postinstall", e), ua("checkPlatformCaching:ciName", r), e === true && r && r in ca) {
      let n = `Prisma has detected that this project was built on ${r}, which caches dependencies. This leads to an outdated Prisma Client because Prisma's auto-generation isn't triggered. To fix this, make sure to run the \`prisma generate\` command during the build process.

Learn how: https://pris.ly/d/${ca[r]}-build`;
      throw console.error(n), new S(n, t);
    }
  };
  var ma = function(e, r) {
    return e ? e.datasources ? e.datasources : e.datasourceUrl ? { [r[0]]: { url: e.datasourceUrl } } : {} : {};
  };
  var gn = function() {
    return typeof Netlify == "object" ? "netlify" : typeof EdgeRuntime == "string" ? "edge-light" : globalThis.navigator?.userAgent === Gp ? "workerd" : globalThis.Deno ? "deno" : globalThis.__lagon__ ? "lagon" : globalThis.process?.release?.name === Jp ? "node" : globalThis.Bun ? "bun" : globalThis.fastly ? "fastly" : "unknown";
  };
  var hn = function(e) {
    let { runtimeBinaryTarget: r } = e;
    return `Add "${r}" to \`binaryTargets\` in the "schema.prisma" file and run \`prisma generate\` after saving it:

${Hp(e)}`;
  };
  var Hp = function(e) {
    let { generator: r, generatorBinaryTargets: t, runtimeBinaryTarget: n } = e, i = { fromEnvVar: null, value: n }, o = [...t, i];
    return si({ ...r, binaryTargets: o });
  };
  var Ke = function(e) {
    let { runtimeBinaryTarget: r } = e;
    return `Prisma Client could not locate the Query Engine for runtime "${r}".`;
  };
  var ze = function(e) {
    let { searchedLocations: r } = e;
    return `The following locations have been searched:
${[...new Set(r)].map((i) => `  ${i}`).join(`
`)}`;
  };
  var da = function(e) {
    let { runtimeBinaryTarget: r } = e;
    return `${Ke(e)}

This happened because \`binaryTargets\` have been pinned, but the actual deployment also required "${r}".
${hn(e)}

${ze(e)}`;
  };
  var yn = function(e) {
    return `We would appreciate if you could take the time to share some information with us.
Please help us by answering a few questions: https://pris.ly/${e}`;
  };
  var En = function(e) {
    let { errorStack: r } = e;
    return r?.match(/\/\.next|\/next@|\/next\//) ? `

We detected that you are using Next.js, learn how to fix this: https://pris.ly/d/engine-not-found-nextjs.` : "";
  };
  var fa = function(e) {
    let { queryEngineName: r } = e;
    return `${Ke(e)}${En(e)}

This is likely caused by a bundler that has not copied "${r}" next to the resulting bundle.
Ensure that "${r}" has been copied next to the bundle or in "${e.expectedLocation}".

${yn("engine-not-found-bundler-investigation")}

${ze(e)}`;
  };
  var ga = function(e) {
    let { runtimeBinaryTarget: r, generatorBinaryTargets: t } = e, n = t.find((i) => i.native);
    return `${Ke(e)}

This happened because Prisma Client was generated for "${n?.value ?? "unknown"}", but the actual deployment required "${r}".
${hn(e)}

${ze(e)}`;
  };
  var ha = function(e) {
    let { queryEngineName: r } = e;
    return `${Ke(e)}${En(e)}

This is likely caused by tooling that has not copied "${r}" to the deployment folder.
Ensure that you ran \`prisma generate\` and that "${r}" has been copied to "${e.expectedLocation}".

${yn("engine-not-found-tooling-investigation")}

${ze(e)}`;
  };
  async function Ea(e, r) {
    let t = { binary: process.env.PRISMA_QUERY_ENGINE_BINARY, library: process.env.PRISMA_QUERY_ENGINE_LIBRARY }[e] ?? r.prismaPath;
    if (t !== undefined)
      return t;
    let { enginePath: n, searchedLocations: i } = await zp(e, r);
    if (Wp("enginePath", n), n !== undefined && e === "binary" && ri(n), n !== undefined)
      return r.prismaPath = n;
    let o = await rr(), s = r.generator?.binaryTargets ?? [], a = s.some((m) => m.native), l = !s.some((m) => m.value === o), u = __filename.match(Kp()) === null, c = { searchedLocations: i, generatorBinaryTargets: s, generator: r.generator, runtimeBinaryTarget: o, queryEngineName: ba(e, o), expectedLocation: ut.default.relative(process.cwd(), r.dirname), errorStack: new Error().stack }, p;
    throw a && l ? p = ga(c) : l ? p = da(c) : u ? p = fa(c) : p = ha(c), new S(p, r.clientVersion);
  }
  async function zp(engineType, config) {
    let binaryTarget = await rr(), searchedLocations = [], dirname = eval("__dirname"), searchLocations = [config.dirname, ut.default.resolve(dirname, ".."), config.generator?.output?.value ?? dirname, ut.default.resolve(dirname, "../../../.prisma/client"), "/tmp/prisma-engines", config.cwd];
    __filename.includes("resolveEnginePath") && searchLocations.push(Ko());
    for (let e of searchLocations) {
      let r = ba(engineType, binaryTarget), t = ut.default.join(e, r);
      if (searchedLocations.push(e), ya.default.existsSync(t))
        return { enginePath: t, searchedLocations };
    }
    return { enginePath: undefined, searchedLocations };
  }
  function ba(e, r) {
    return e === "library" ? kt(r, "fs") : `query-engine-${r}${r === "windows" ? ".exe" : ""}`;
  }
  function wa(e) {
    return e ? e.replace(/".*"/g, '"X"').replace(/[\s:\[]([+-]?([0-9]*[.])?[0-9]+)/g, (r) => `${r[0]}5`) : "";
  }
  function xa(e) {
    return e.split(`
`).map((r) => r.replace(/^\d{4}-[01]\d-[0-3]\dT[0-2]\d:[0-5]\d:[0-5]\d\.\d+([+-][0-2]\d:[0-5]\d|Z)\s*/, "").replace(/\+\d+\s*ms$/, "")).join(`
`);
  }
  function va({ title: e, user: r = "prisma", repo: t = "prisma", template: n = "bug_report.yml", body: i }) {
    return (0, Pa.default)({ user: r, repo: t, template: n, title: e, body: i });
  }
  function Ta({ version: e, binaryTarget: r, title: t, description: n, engineVersion: i, database: o, query: s }) {
    let a = fo(6000 - (s?.length ?? 0)), l = xa((0, Di.default)(a)), u = n ? `# Description
\`\`\`
${n}
\`\`\`` : "", c = (0, Di.default)(`Hi Prisma Team! My Prisma Client just crashed. This is the report:
## Versions

| Name            | Version            |
|-----------------|--------------------|
| Node            | ${process.version?.padEnd(19)}| 
| OS              | ${r?.padEnd(19)}|
| Prisma Client   | ${e?.padEnd(19)}|
| Query Engine    | ${i?.padEnd(19)}|
| Database        | ${o?.padEnd(19)}|

${u}

## Logs
\`\`\`
${l}
\`\`\`

## Client Snippet
\`\`\`ts
// PLEASE FILL YOUR CODE SNIPPET HERE
\`\`\`

## Schema
\`\`\`prisma
// PLEASE ADD YOUR SCHEMA HERE IF POSSIBLE
\`\`\`

## Prisma Engine Query
\`\`\`
${s ? wa(s) : ""}
\`\`\`
`), p = va({ title: t, body: c });
    return `${t}

This is a non-recoverable error which probably happens when the Prisma Query Engine has a panic.

${ee(p)}

If you want the Prisma team to look into it, please open the link above \uD83D\uDE4F
To increase the chance of success, please post your schema and a snippet of
how you used Prisma Client in the issue. 
`;
  }
  function Ir({ inlineDatasources: e, overrideDatasources: r, env: t, clientVersion: n }) {
    let i, o = Object.keys(e)[0], s = e[o]?.url, a = r[o]?.url;
    if (o === undefined ? i = undefined : a ? i = a : s?.value ? i = s.value : s?.fromEnvVar && (i = t[s.fromEnvVar]), s?.fromEnvVar !== undefined && i === undefined)
      throw new S(`error: Environment variable not found: ${s.fromEnvVar}.`, n);
    if (i === undefined)
      throw new S("error: Missing URL environment variable, value, or override.", n);
    return i;
  }
  function R(e, r) {
    return { ...e, isRetryable: r };
  }
  async function Xp(e) {
    let r;
    try {
      r = await e.text();
    } catch {
      return { type: "EmptyError" };
    }
    try {
      let t = JSON.parse(r);
      if (typeof t == "string")
        switch (t) {
          case "InternalDataProxyError":
            return { type: "DataProxyError", body: t };
          default:
            return { type: "UnknownTextError", body: t };
        }
      if (typeof t == "object" && t !== null) {
        if ("is_panic" in t && "message" in t && "error_code" in t)
          return { type: "QueryEngineError", body: t };
        if ("EngineNotStarted" in t || "InteractiveTransactionMisrouted" in t || "InvalidRequestError" in t) {
          let n = Object.values(t)[0].reason;
          return typeof n == "string" && !["SchemaMissing", "EngineVersionNotSupported"].includes(n) ? { type: "UnknownJsonError", body: t } : { type: "DataProxyError", body: t };
        }
      }
      return { type: "UnknownJsonError", body: t };
    } catch {
      return r === "" ? { type: "EmptyError" } : { type: "UnknownTextError", body: r };
    }
  }
  async function wt(e, r) {
    if (e.ok)
      return;
    let t = { clientVersion: r, response: e }, n = await Xp(e);
    if (n.type === "QueryEngineError")
      throw new V(n.body.message, { code: n.body.error_code, clientVersion: r });
    if (n.type === "DataProxyError") {
      if (n.body === "InternalDataProxyError")
        throw new kr(t, "Internal Data Proxy error");
      if ("EngineNotStarted" in n.body) {
        if (n.body.EngineNotStarted.reason === "SchemaMissing")
          return new ur(t);
        if (n.body.EngineNotStarted.reason === "EngineVersionNotSupported")
          throw new dt(t);
        if ("EngineStartupError" in n.body.EngineNotStarted.reason) {
          let { msg: i, logs: o } = n.body.EngineNotStarted.reason.EngineStartupError;
          throw new mt(t, i, o);
        }
        if ("KnownEngineStartupError" in n.body.EngineNotStarted.reason) {
          let { msg: i, error_code: o } = n.body.EngineNotStarted.reason.KnownEngineStartupError;
          throw new S(i, r, o);
        }
        if ("HealthcheckTimeout" in n.body.EngineNotStarted.reason) {
          let { logs: i } = n.body.EngineNotStarted.reason.HealthcheckTimeout;
          throw new pt(t, i);
        }
      }
      if ("InteractiveTransactionMisrouted" in n.body) {
        let i = { IDParseError: "Could not parse interactive transaction ID", NoQueryEngineFoundError: "Could not find Query Engine for the specified host and transaction ID", TransactionStartError: "Could not start interactive transaction" };
        throw new gt(t, i[n.body.InteractiveTransactionMisrouted.reason]);
      }
      if ("InvalidRequestError" in n.body)
        throw new ht(t, n.body.InvalidRequestError.reason);
    }
    if (e.status === 401 || e.status === 403)
      throw new Et(t, Dr(Mi, n));
    if (e.status === 404)
      return new yt(t, Dr(Oi, n));
    if (e.status === 429)
      throw new bt(t, Dr($i, n));
    if (e.status === 504)
      throw new ft(t, Dr(Ni, n));
    if (e.status >= 500)
      throw new kr(t, Dr(Fi, n));
    if (e.status >= 400)
      throw new ct(t, Dr(Li, n));
  }
  function Dr(e, r) {
    return r.type === "EmptyError" ? e : `${e}: ${JSON.stringify(r)}`;
  }
  function Ca(e) {
    let r = Math.pow(2, e) * 50, t = Math.ceil(Math.random() * r) - Math.ceil(r / 2), n = r + t;
    return new Promise((i) => setTimeout(() => i(n), n));
  }
  function Sa(e) {
    let r = new TextEncoder().encode(e), t = "", n = r.byteLength, i = n % 3, o = n - i, s, a, l, u, c;
    for (let p = 0;p < o; p = p + 3)
      c = r[p] << 16 | r[p + 1] << 8 | r[p + 2], s = (c & 16515072) >> 18, a = (c & 258048) >> 12, l = (c & 4032) >> 6, u = c & 63, t += Fe[s] + Fe[a] + Fe[l] + Fe[u];
    return i == 1 ? (c = r[o], s = (c & 252) >> 2, a = (c & 3) << 4, t += Fe[s] + Fe[a] + "==") : i == 2 && (c = r[o] << 8 | r[o + 1], s = (c & 64512) >> 10, a = (c & 1008) >> 4, l = (c & 15) << 2, t += Fe[s] + Fe[a] + Fe[l] + "="), t;
  }
  function Ra(e) {
    if (!!e.generator?.previewFeatures.some((t) => t.toLowerCase().includes("metrics")))
      throw new S("The `metrics` preview feature is not yet available with Accelerate.\nPlease remove `metrics` from the `previewFeatures` in your schema.\n\nMore information about Accelerate: https://pris.ly/d/accelerate", e.clientVersion);
  }
  function em(e) {
    return e[0] * 1000 + e[1] / 1e6;
  }
  function Aa(e) {
    return new Date(em(e));
  }
  async function cr(e, r, t = (n) => n) {
    let n = r.clientVersion;
    try {
      return typeof fetch == "function" ? await t(fetch)(e, r) : await t(qi)(e, r);
    } catch (i) {
      let o = i.message ?? "Unknown error";
      throw new xt(o, { clientVersion: n });
    }
  }
  function tm(e) {
    return { ...e.headers, "Content-Type": "application/json" };
  }
  function nm(e) {
    return { method: e.method, headers: tm(e) };
  }
  function im(e, r) {
    return { text: () => Promise.resolve(Buffer.concat(e).toString()), json: () => Promise.resolve().then(() => JSON.parse(Buffer.concat(e).toString())), ok: r.statusCode >= 200 && r.statusCode <= 299, status: r.statusCode, url: r.url, headers: new Bi(r.headers) };
  }
  async function qi(e, r = {}) {
    let t = om("https"), n = nm(r), i = [], { origin: o } = new URL(e);
    return new Promise((s, a) => {
      let l = t.request(e, n, (u) => {
        let { statusCode: c, headers: { location: p } } = u;
        c >= 301 && c <= 399 && p && (p.startsWith("http") === false ? s(qi(`${o}${p}`, r)) : s(qi(p, r))), u.on("data", (m) => i.push(m)), u.on("end", () => s(im(i, u))), u.on("error", a);
      });
      l.on("error", a), l.end(r.body ?? "");
    });
  }
  async function am(e, r) {
    let t = Ia["@prisma/engines-version"], n = r.clientVersion ?? "unknown";
    if (process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)
      return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;
    if (e.includes("accelerate") && n !== "0.0.0" && n !== "in-memory")
      return n;
    let [i, o] = n?.split("-") ?? [];
    if (o === undefined && sm.test(i))
      return i;
    if (o !== undefined || n === "0.0.0" || n === "in-memory") {
      if (e.startsWith("localhost") || e.startsWith("127.0.0.1"))
        return "0.0.0";
      let [s] = t.split("-") ?? [], [a, l, u] = s.split("."), c = lm(`<=${a}.${l}.${u}`), p = await cr(c, { clientVersion: n });
      if (!p.ok)
        throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text() || "<empty body>"}`);
      let m = await p.text();
      _a("length of body fetched from unpkg.com", m.length);
      let f;
      try {
        f = JSON.parse(m);
      } catch (g) {
        throw console.error("JSON.parse error: body fetched from unpkg.com: ", m), g;
      }
      return f.version;
    }
    throw new lr("Only `major.minor.patch` versions are supported by Accelerate.", { clientVersion: n });
  }
  async function ka(e, r) {
    let t = await am(e, r);
    return _a("version", t), t;
  }
  function lm(e) {
    return encodeURI(`https://unpkg.com/prisma@${e}/package.json`);
  }
  function La(e) {
    if (e?.kind === "itx")
      return e.options.id;
  }
  function um() {
    let e = globalThis;
    return e[Ui] === undefined && (e[Ui] = {}), e[Ui];
  }
  function cm(e) {
    let r = um();
    if (r[e] !== undefined)
      return r[e];
    let t = Na.default.toNamespacedPath(e), n = { exports: {} }, i = 0;
    return process.platform !== "win32" && (i = Qi.default.constants.dlopen.RTLD_LAZY | Qi.default.constants.dlopen.RTLD_DEEPBIND), process.dlopen(n, t, i), r[e] = n.exports, n.exports;
  }
  function mm(e) {
    return e.item_type === "query" && "query" in e;
  }
  function dm(e) {
    return "level" in e ? e.level === "error" && e.message === "PANIC" : false;
  }
  function fm(e) {
    return typeof e == "object" && e !== null && e.error_code !== undefined;
  }
  function Ji(e, r) {
    return Ta({ binaryTarget: e.binaryTarget, title: r, version: e.config.clientVersion, engineVersion: e.versionInfo?.commit, database: e.config.activeProvider, query: e.lastQuery });
  }
  function qa({ copyEngine: e = true }, r) {
    let t;
    try {
      t = Ir({ inlineDatasources: r.inlineDatasources, overrideDatasources: r.overrideDatasources, env: { ...r.env, ...process.env }, clientVersion: r.clientVersion });
    } catch {
    }
    e && t?.startsWith("prisma://") && Kr("recommend--no-engine", "In production, we recommend using `prisma generate --no-engine` (See: `prisma generate --help`)");
    let n = Gr(r.generator), i = !!(t?.startsWith("prisma://") || !e), o = !!r.adapter, s = n === "library", a = n === "binary";
    if (i && o || o && false) {
      let l;
      throw e ? t?.startsWith("prisma://") ? l = ["Prisma Client was configured to use the `adapter` option but the URL was a `prisma://` URL.", "Please either use the `prisma://` URL or remove the `adapter` from the Prisma Client constructor."] : l = ["Prisma Client was configured to use both the `adapter` and Accelerate, please chose one."] : l = ["Prisma Client was configured to use the `adapter` option but `prisma generate` was run with `--no-engine`.", "Please run `prisma generate` without `--no-engine` to be able to use Prisma Client with the adapter."], new K(l.join(`
`), { clientVersion: r.clientVersion });
    }
    if (i)
      return new Pt(r);
    if (s)
      return new vt(r);
    throw new K("Invalid client engine type, please use `library` or `binary`", { clientVersion: r.clientVersion });
  }
  function wn({ generator: e }) {
    return e?.previewFeatures ?? [];
  }
  function Ua(e, r) {
    let t = Qa(e), n = gm(t), i = ym(n);
    i ? xn(i, r) : r.addErrorMessage(() => "Unknown error");
  }
  function Qa(e) {
    return e.errors.flatMap((r) => r.kind === "Union" ? Qa(r) : [r]);
  }
  function gm(e) {
    let r = new Map, t = [];
    for (let n of e) {
      if (n.kind !== "InvalidArgumentType") {
        t.push(n);
        continue;
      }
      let i = `${n.selectionPath.join(".")}:${n.argumentPath.join(".")}`, o = r.get(i);
      o ? r.set(i, { ...n, argument: { ...n.argument, typeNames: hm(o.argument.typeNames, n.argument.typeNames) } }) : r.set(i, n);
    }
    return t.push(...r.values()), t;
  }
  function hm(e, r) {
    return [...new Set(e.concat(r))];
  }
  function ym(e) {
    return pi(e, (r, t) => {
      let n = Va(r), i = Va(t);
      return n !== i ? n - i : ja(r) - ja(t);
    });
  }
  function Va(e) {
    let r = 0;
    return Array.isArray(e.selectionPath) && (r += e.selectionPath.length), Array.isArray(e.argumentPath) && (r += e.argumentPath.length), r;
  }
  function ja(e) {
    switch (e.kind) {
      case "InvalidArgumentValue":
      case "ValueTooLarge":
        return 20;
      case "InvalidArgumentType":
        return 10;
      case "RequiredArgumentMissing":
        return -10;
      default:
        return 0;
    }
  }
  function xn(e, r) {
    switch (e.kind) {
      case "IncludeAndSelect":
        Em(e, r);
        break;
      case "IncludeOnScalar":
        bm(e, r);
        break;
      case "EmptySelection":
        wm(e, r);
        break;
      case "UnknownSelectionField":
        xm(e, r);
        break;
      case "UnknownArgument":
        Pm(e, r);
        break;
      case "UnknownInputField":
        vm(e, r);
        break;
      case "RequiredArgumentMissing":
        Tm(e, r);
        break;
      case "InvalidArgumentType":
        Cm(e, r);
        break;
      case "InvalidArgumentValue":
        Sm(e, r);
        break;
      case "ValueTooLarge":
        Rm(e, r);
        break;
      case "SomeFieldsMissing":
        Am(e, r);
        break;
      case "TooManyFieldsGiven":
        Im(e, r);
        break;
      case "Union":
        Ua(e, r);
        break;
      default:
        throw new Error("not implemented: " + e.kind);
    }
  }
  function Em(e, r) {
    let t = r.arguments.getDeepSubSelectionValue(e.selectionPath);
    t && t instanceof J && (t.getField("include")?.markAsError(), t.getField("select")?.markAsError()), r.addErrorMessage((n) => `Please ${n.bold("either")} use ${n.green("`include`")} or ${n.green("`select`")}, but ${n.red("not both")} at the same time.`);
  }
  function bm(e, r) {
    let [t, n] = vn(e.selectionPath), i = e.outputType, o = r.arguments.getDeepSelectionParent(t)?.value;
    if (o && (o.getField(n)?.markAsError(), i))
      for (let s of i.fields)
        s.isRelation && o.addSuggestion(new Me(s.name, "true"));
    r.addErrorMessage((s) => {
      let a = `Invalid scalar field ${s.red(`\`${n}\``)} for ${s.bold("include")} statement`;
      return i ? a += ` on model ${s.bold(i.name)}. ${Tt(s)}` : a += ".", a += `
Note that ${s.bold("include")} statements only accept relation fields.`, a;
    });
  }
  function wm(e, r) {
    let t = e.outputType, n = r.arguments.getDeepSelectionParent(e.selectionPath)?.value, i = n?.isEmpty() ?? false;
    n && (n.removeAllFields(), Wa(n, t)), r.addErrorMessage((o) => i ? `The ${o.red("`select`")} statement for type ${o.bold(t.name)} must not be empty. ${Tt(o)}` : `The ${o.red("`select`")} statement for type ${o.bold(t.name)} needs ${o.bold("at least one truthy value")}.`);
  }
  function xm(e, r) {
    let [t, n] = vn(e.selectionPath), i = r.arguments.getDeepSelectionParent(t);
    i && (i.value.getField(n)?.markAsError(), Wa(i.value, e.outputType)), r.addErrorMessage((o) => {
      let s = [`Unknown field ${o.red(`\`${n}\``)}`];
      return i && s.push(`for ${o.bold(i.kind)} statement`), s.push(`on model ${o.bold(`\`${e.outputType.name}\``)}.`), s.push(Tt(o)), s.join(" ");
    });
  }
  function Pm(e, r) {
    let t = e.argumentPath[0], n = r.arguments.getDeepSubSelectionValue(e.selectionPath);
    n instanceof J && (n.getField(t)?.markAsError(), _m(n, e.arguments)), r.addErrorMessage((i) => Ja(i, t, e.arguments.map((o) => o.name)));
  }
  function vm(e, r) {
    let [t, n] = vn(e.argumentPath), i = r.arguments.getDeepSubSelectionValue(e.selectionPath);
    if (i instanceof J) {
      i.getDeepField(e.argumentPath)?.markAsError();
      let o = i.getDeepFieldValue(t);
      o instanceof J && Ka(o, e.inputType);
    }
    r.addErrorMessage((o) => Ja(o, n, e.inputType.fields.map((s) => s.name)));
  }
  function Ja(e, r, t) {
    let n = [`Unknown argument \`${e.red(r)}\`.`], i = Dm(r, t);
    return i && n.push(`Did you mean \`${e.green(i)}\`?`), t.length > 0 && n.push(Tt(e)), n.join(" ");
  }
  function Tm(e, r) {
    let t;
    r.addErrorMessage((l) => t?.value instanceof H && t.value.text === "null" ? `Argument \`${l.green(o)}\` must not be ${l.red("null")}.` : `Argument \`${l.green(o)}\` is missing.`);
    let n = r.arguments.getDeepSubSelectionValue(e.selectionPath);
    if (!(n instanceof J))
      return;
    let [i, o] = vn(e.argumentPath), s = new Pn, a = n.getDeepFieldValue(i);
    if (a instanceof J)
      if (t = a.getField(o), t && a.removeField(o), e.inputTypes.length === 1 && e.inputTypes[0].kind === "object") {
        for (let l of e.inputTypes[0].fields)
          s.addField(l.name, l.typeNames.join(" | "));
        a.addSuggestion(new Me(o, s).makeRequired());
      } else {
        let l = e.inputTypes.map(Ha).join(" | ");
        a.addSuggestion(new Me(o, l).makeRequired());
      }
  }
  function Ha(e) {
    return e.kind === "list" ? `${Ha(e.elementType)}[]` : e.name;
  }
  function Cm(e, r) {
    let t = e.argument.name, n = r.arguments.getDeepSubSelectionValue(e.selectionPath);
    n instanceof J && n.getDeepFieldValue(e.argumentPath)?.markAsError(), r.addErrorMessage((i) => {
      let o = Tn("or", e.argument.typeNames.map((s) => i.green(s)));
      return `Argument \`${i.bold(t)}\`: Invalid value provided. Expected ${o}, provided ${i.red(e.inferredType)}.`;
    });
  }
  function Sm(e, r) {
    let t = e.argument.name, n = r.arguments.getDeepSubSelectionValue(e.selectionPath);
    n instanceof J && n.getDeepFieldValue(e.argumentPath)?.markAsError(), r.addErrorMessage((i) => {
      let o = [`Invalid value for argument \`${i.bold(t)}\``];
      if (e.underlyingError && o.push(`: ${e.underlyingError}`), o.push("."), e.argument.typeNames.length > 0) {
        let s = Tn("or", e.argument.typeNames.map((a) => i.green(a)));
        o.push(` Expected ${s}.`);
      }
      return o.join("");
    });
  }
  function Rm(e, r) {
    let t = e.argument.name, n = r.arguments.getDeepSubSelectionValue(e.selectionPath), i;
    if (n instanceof J) {
      let s = n.getDeepField(e.argumentPath)?.value;
      s?.markAsError(), s instanceof H && (i = s.text);
    }
    r.addErrorMessage((o) => {
      let s = ["Unable to fit value"];
      return i && s.push(o.red(i)), s.push(`into a 64-bit signed integer for field \`${o.bold(t)}\``), s.join(" ");
    });
  }
  function Am(e, r) {
    let t = e.argumentPath[e.argumentPath.length - 1], n = r.arguments.getDeepSubSelectionValue(e.selectionPath);
    if (n instanceof J) {
      let i = n.getDeepFieldValue(e.argumentPath);
      i instanceof J && Ka(i, e.inputType);
    }
    r.addErrorMessage((i) => {
      let o = [`Argument \`${i.bold(t)}\` of type ${i.bold(e.inputType.name)} needs`];
      return e.constraints.minFieldCount === 1 ? e.constraints.requiredFields ? o.push(`${i.green("at least one of")} ${Tn("or", e.constraints.requiredFields.map((s) => `\`${i.bold(s)}\``))} arguments.`) : o.push(`${i.green("at least one")} argument.`) : o.push(`${i.green(`at least ${e.constraints.minFieldCount}`)} arguments.`), o.push(Tt(i)), o.join(" ");
    });
  }
  function Im(e, r) {
    let t = e.argumentPath[e.argumentPath.length - 1], n = r.arguments.getDeepSubSelectionValue(e.selectionPath), i = [];
    if (n instanceof J) {
      let o = n.getDeepFieldValue(e.argumentPath);
      o instanceof J && (o.markAsError(), i = Object.keys(o.getFields()));
    }
    r.addErrorMessage((o) => {
      let s = [`Argument \`${o.bold(t)}\` of type ${o.bold(e.inputType.name)} needs`];
      return e.constraints.minFieldCount === 1 && e.constraints.maxFieldCount == 1 ? s.push(`${o.green("exactly one")} argument,`) : e.constraints.maxFieldCount == 1 ? s.push(`${o.green("at most one")} argument,`) : s.push(`${o.green(`at most ${e.constraints.maxFieldCount}`)} arguments,`), s.push(`but you provided ${Tn("and", i.map((a) => o.red(a)))}. Please choose`), e.constraints.maxFieldCount === 1 ? s.push("one.") : s.push(`${e.constraints.maxFieldCount}.`), s.join(" ");
    });
  }
  function Wa(e, r) {
    for (let t of r.fields)
      e.hasField(t.name) || e.addSuggestion(new Me(t.name, "true"));
  }
  function _m(e, r) {
    for (let t of r)
      e.hasField(t.name) || e.addSuggestion(new Me(t.name, t.typeNames.join(" | ")));
  }
  function Ka(e, r) {
    if (r.kind === "object")
      for (let t of r.fields)
        e.hasField(t.name) || e.addSuggestion(new Me(t.name, t.typeNames.join(" | ")));
  }
  function vn(e) {
    let r = [...e], t = r.pop();
    if (!t)
      throw new Error("unexpected empty path");
    return [r, t];
  }
  function Tt({ green: e, enabled: r }) {
    return "Available options are " + (r ? `listed in ${e("green")}` : "marked with ?") + ".";
  }
  function Tn(e, r) {
    if (r.length === 1)
      return r[0];
    let t = [...r], n = t.pop();
    return `${t.join(", ")} ${e} ${n}`;
  }
  function Dm(e, r) {
    let t = 1 / 0, n;
    for (let i of r) {
      let o = (0, Ga.default)(e, i);
      o > km || o < t && (t = o, n = i);
    }
    return n;
  }
  function Cn({ args: e, errors: r, errorFormat: t, callsite: n, originalMethod: i, clientVersion: o }) {
    let s = ln(e);
    for (let p of r)
      xn(p, s);
    let a = t === "pretty" ? Ss : sn, l = s.renderAllMessages(a), u = new br(0, { colors: a }).write(s).toString(), c = Ar({ message: l, callsite: n, originalMethod: i, showColors: t === "pretty", callArguments: u });
    throw new K(c, { clientVersion: o });
  }
  function za({ modelName: e, action: r, args: t, runtimeDataModel: n, extensions: i, callsite: o, clientMethod: s, errorFormat: a, clientVersion: l }) {
    let u = new Wi({ runtimeDataModel: n, modelName: e, action: r, rootArgs: t, callsite: o, extensions: i, selectionPath: [], argumentPath: [], originalMethod: s, errorFormat: a, clientVersion: l });
    return { modelName: e, action: Lm[r], query: Ki(t, u) };
  }
  function Ki({ select: e, include: r, ...t } = {}, n) {
    return { arguments: Za(t, n), selection: Nm(e, r, n) };
  }
  function Nm(e, r, t) {
    return e && r && t.throwValidationError({ kind: "IncludeAndSelect", selectionPath: t.getSelectionPath() }), e ? Mm(e, t) : Om(t, r);
  }
  function Om(e, r) {
    let t = {};
    return e.model && !e.isRawAction() && (t.$composites = true, t.$scalars = true), r && Fm(t, r, e), t;
  }
  function Fm(e, r, t) {
    for (let [n, i] of Object.entries(r)) {
      let o = t.findField(n);
      o && o?.kind !== "object" && t.throwValidationError({ kind: "IncludeOnScalar", selectionPath: t.getSelectionPath().concat(n), outputType: t.getOutputTypeDescription() }), i === true ? e[n] = true : typeof i == "object" && (e[n] = Ki(i, t.nestSelection(n)));
    }
  }
  function Mm(e, r) {
    let t = {}, n = r.getComputedFields(), i = la(e, n);
    for (let [o, s] of Object.entries(i)) {
      let a = r.findField(o);
      n?.[o] && !a || (s === true ? t[o] = true : typeof s == "object" && (t[o] = Ki(s, r.nestSelection(o))));
    }
    return t;
  }
  function Ya(e, r) {
    if (e === null)
      return null;
    if (typeof e == "string" || typeof e == "number" || typeof e == "boolean")
      return e;
    if (typeof e == "bigint")
      return { $type: "BigInt", value: String(e) };
    if (wr(e)) {
      if (Kt(e))
        return { $type: "DateTime", value: e.toISOString() };
      r.throwValidationError({ kind: "InvalidArgumentValue", selectionPath: r.getSelectionPath(), argumentPath: r.getArgumentPath(), argument: { name: r.getArgumentName(), typeNames: ["Date"] }, underlyingError: "Provided Date object is invalid" });
    }
    if (Tr(e))
      return { $type: "FieldRef", value: { _ref: e.name, _container: e.modelName } };
    if (Array.isArray(e))
      return $m(e, r);
    if (ArrayBuffer.isView(e))
      return { $type: "Bytes", value: Buffer.from(e).toString("base64") };
    if (qm(e))
      return e.values;
    if (vr(e))
      return { $type: "Decimal", value: e.toFixed() };
    if (e instanceof Ne) {
      if (e !== Jt.instances[e._getName()])
        throw new Error("Invalid ObjectEnumValue");
      return { $type: "Enum", value: e._getName() };
    }
    if (Bm(e))
      return e.toJSON();
    if (typeof e == "object")
      return Za(e, r);
    r.throwValidationError({ kind: "InvalidArgumentValue", selectionPath: r.getSelectionPath(), argumentPath: r.getArgumentPath(), argument: { name: r.getArgumentName(), typeNames: [] }, underlyingError: `We could not serialize ${Object.prototype.toString.call(e)} value. Serialize the object to JSON or implement a ".toJSON()" method on it` });
  }
  function Za(e, r) {
    if (e.$type)
      return { $type: "Raw", value: e };
    let t = {};
    for (let n in e) {
      let i = e[n];
      i !== undefined && (t[n] = Ya(i, r.nestArgument(n)));
    }
    return t;
  }
  function $m(e, r) {
    let t = [];
    for (let n = 0;n < e.length; n++) {
      let i = r.nestArgument(String(n)), o = e[n];
      o === undefined && r.throwValidationError({ kind: "InvalidArgumentValue", selectionPath: i.getSelectionPath(), argumentPath: i.getArgumentPath(), argument: { name: `${r.getArgumentName()}[${n}]`, typeNames: [] }, underlyingError: "Can not use `undefined` value within array. Use `null` or filter out `undefined` values" }), t.push(Ya(o, i));
    }
    return t;
  }
  function qm(e) {
    return typeof e == "object" && e !== null && e.__prismaRawParameters__ === true;
  }
  function Bm(e) {
    return typeof e == "object" && e !== null && typeof e.toJSON == "function";
  }
  function Ct(e) {
    try {
      return rl(e, "fast");
    } catch {
      return rl(e, "slow");
    }
  }
  function rl(e, r) {
    return JSON.stringify(e.map((t) => Vm(t, r)));
  }
  function Vm(e, r) {
    return typeof e == "bigint" ? { prisma__type: "bigint", prisma__value: e.toString() } : wr(e) ? { prisma__type: "date", prisma__value: e.toJSON() } : Te.isDecimal(e) ? { prisma__type: "decimal", prisma__value: e.toJSON() } : Buffer.isBuffer(e) ? { prisma__type: "bytes", prisma__value: e.toString("base64") } : jm(e) || ArrayBuffer.isView(e) ? { prisma__type: "bytes", prisma__value: Buffer.from(e).toString("base64") } : typeof e == "object" && r === "slow" ? nl(e) : e;
  }
  function jm(e) {
    return e instanceof ArrayBuffer || e instanceof SharedArrayBuffer ? true : typeof e == "object" && e !== null ? e[Symbol.toStringTag] === "ArrayBuffer" || e[Symbol.toStringTag] === "SharedArrayBuffer" : false;
  }
  function nl(e) {
    if (typeof e != "object" || e === null)
      return e;
    if (typeof e.toJSON == "function")
      return e.toJSON();
    if (Array.isArray(e))
      return e.map(tl);
    let r = {};
    for (let t of Object.keys(e))
      r[t] = tl(e[t]);
    return r;
  }
  function tl(e) {
    return typeof e == "bigint" ? e.toString() : nl(e);
  }
  function zi(e, r, t, n) {
    if (!(e !== "postgresql" && e !== "cockroachdb") && t.length > 0 && Um.exec(r))
      throw new Error(`Running ALTER using ${n} is not supported
Using the example below you can still execute your query with Prisma, but please note that it is vulnerable to SQL injection attacks and requires you to take care of input sanitization.

Example:
  await prisma.$executeRawUnsafe(\`ALTER USER prisma WITH PASSWORD '\${password}'\`)

More Information: https://pris.ly/d/execute-raw
`);
  }
  function Zi(e) {
    return function(t) {
      let n, i = (o = e) => {
        try {
          return o === undefined || o?.kind === "itx" ? n ?? (n = al(t(o))) : al(t(o));
        } catch (s) {
          return Promise.reject(s);
        }
      };
      return { then(o, s) {
        return i().then(o, s);
      }, catch(o) {
        return i().catch(o);
      }, finally(o) {
        return i().finally(o);
      }, requestTransaction(o) {
        let s = i(o);
        return s.requestTransaction ? s.requestTransaction(o) : s;
      }, [Symbol.toStringTag]: "PrismaPromise" };
    };
  }
  function al(e) {
    return typeof e.then == "function" ? e : Promise.resolve(e);
  }
  function ul(e) {
    return e.includes("tracing") ? new Xi : ll;
  }
  function cl(e, r = () => {
  }) {
    let t, n = new Promise((i) => t = i);
    return { then(i) {
      return --e === 0 && t(r()), i?.(n);
    } };
  }
  function ml(e) {
    return typeof e == "string" ? e : e.reduce((r, t) => {
      let n = typeof t == "string" ? t : t.level;
      return n === "query" ? r : r && (t === "info" || r === "info") ? "info" : n;
    }, undefined);
  }
  function Rn(e) {
    return typeof e.batchRequestIdx == "number";
  }
  function An(e) {
    return e === null ? e : Array.isArray(e) ? e.map(An) : typeof e == "object" ? Gm(e) ? Jm(e) : hr(e, An) : e;
  }
  function Gm(e) {
    return e !== null && typeof e == "object" && typeof e.$type == "string";
  }
  function Jm({ $type: e, value: r }) {
    switch (e) {
      case "BigInt":
        return BigInt(r);
      case "Bytes":
        return Buffer.from(r, "base64");
      case "DateTime":
        return new Date(r);
      case "Decimal":
        return new Te(r);
      case "Json":
        return JSON.parse(r);
      default:
        tr(r, "Unknown tagged value");
    }
  }
  function dl(e) {
    if (e.action !== "findUnique" && e.action !== "findUniqueOrThrow")
      return;
    let r = [];
    return e.modelName && r.push(e.modelName), e.query.arguments && r.push(eo(e.query.arguments)), r.push(eo(e.query.selection)), r.join("");
  }
  function eo(e) {
    return `(${Object.keys(e).sort().map((t) => {
      let n = e[t];
      return typeof n == "object" && n !== null ? `(${t} ${eo(n)})` : t;
    }).join(" ")})`;
  }
  function ro(e) {
    return Hm[e];
  }
  function Km(e) {
    if (e) {
      if (e.kind === "batch")
        return { kind: "batch", options: { isolationLevel: e.isolationLevel } };
      if (e.kind === "itx")
        return { kind: "itx", options: gl(e) };
      tr(e, "Unknown transaction kind");
    }
  }
  function gl(e) {
    return { id: e.id, payload: e.payload };
  }
  function zm(e, r) {
    return Rn(e) && r?.kind === "batch" && e.batchRequestIdx !== r.index;
  }
  function Ym(e) {
    return e.code === "P2009" || e.code === "P2012";
  }
  function hl(e) {
    if (e.kind === "Union")
      return { kind: "Union", errors: e.errors.map(hl) };
    if (Array.isArray(e.selectionPath)) {
      let [, ...r] = e.selectionPath;
      return { ...e, selectionPath: r };
    }
    return e;
  }
  function bl(e) {
    return e.map((r) => {
      let t = {};
      for (let n of Object.keys(r))
        t[n] = wl(r[n]);
      return t;
    });
  }
  function wl({ prisma__type: e, prisma__value: r }) {
    switch (e) {
      case "bigint":
        return BigInt(r);
      case "bytes":
        return Buffer.from(r, "base64");
      case "decimal":
        return new Te(r);
      case "datetime":
      case "date":
        return new Date(r);
      case "time":
        return new Date(`1970-01-01T${r}Z`);
      case "array":
        return r.map(wl);
      default:
        return r;
    }
  }
  function Cl(e, r) {
    for (let [t, n] of Object.entries(e)) {
      if (!xl.includes(t)) {
        let i = Lr(t, xl);
        throw new q(`Unknown property ${t} provided to PrismaClient constructor.${i}`);
      }
      Xm[t](n, r);
    }
    if (e.datasourceUrl && e.datasources)
      throw new q('Can not use "datasourceUrl" and "datasources" options at the same time. Pick one of them');
  }
  function Lr(e, r) {
    if (r.length === 0 || typeof e != "string")
      return "";
    let t = ed(e, r);
    return t ? ` Did you mean "${t}"?` : "";
  }
  function ed(e, r) {
    if (r.length === 0)
      return null;
    let t = r.map((i) => ({ value: i, distance: (0, Tl.default)(e, i) }));
    t.sort((i, o) => i.distance < o.distance ? -1 : 1);
    let n = t[0];
    return n.distance < 3 ? n.value : null;
  }
  function Sl(e) {
    return e.length === 0 ? Promise.resolve([]) : new Promise((r, t) => {
      let n = new Array(e.length), i = null, o = false, s = 0, a = () => {
        o || (s++, s === e.length && (o = true, i ? t(i) : r(n)));
      }, l = (u) => {
        o || (o = true, t(u));
      };
      for (let u = 0;u < e.length; u++)
        e[u].then((c) => {
          n[u] = c, a();
        }, (c) => {
          if (!Rn(c)) {
            l(c);
            return;
          }
          c.batchRequestIdx === u ? l(c) : (i || (i = c), a());
        });
    });
  }
  function Dl(e) {

    class r {
      constructor(n) {
        this._originalClient = this;
        this._middlewares = new Sn;
        this._createPrismaPromise = Zi();
        this.$extends = Ks;
        e = n?.__internal?.configOverride?.(e) ?? e, pa(e), n && Cl(n, e);
        let i = n?.adapter ? hi(n.adapter) : undefined, o = new _l.EventEmitter().on("error", () => {
        });
        this._extensions = fn.empty(), this._previewFeatures = wn(e), this._clientVersion = e.clientVersion ?? El, this._activeProvider = e.activeProvider, this._tracingHelper = ul(this._previewFeatures);
        let s = { rootEnvPath: e.relativeEnvPaths.rootEnvPath && St.default.resolve(e.dirname, e.relativeEnvPaths.rootEnvPath), schemaEnvPath: e.relativeEnvPaths.schemaEnvPath && St.default.resolve(e.dirname, e.relativeEnvPaths.schemaEnvPath) }, a = !i && Qr(s, { conflictCheck: "none" }) || e.injectableEdgeEnv?.();
        try {
          let l = n ?? {}, u = l.__internal ?? {}, c = u.debug === true;
          c && N.enable("prisma:client");
          let p = St.default.resolve(e.dirname, e.relativePath);
          kl.default.existsSync(p) || (p = e.dirname), Ye("dirname", e.dirname), Ye("relativePath", e.relativePath), Ye("cwd", p);
          let m = u.engine || {};
          if (l.errorFormat ? this._errorFormat = l.errorFormat : process.env.NO_COLOR ? this._errorFormat = "colorless" : this._errorFormat = "colorless", this._runtimeDataModel = e.runtimeDataModel, this._engineConfig = { cwd: p, dirname: e.dirname, enableDebugLogs: c, allowTriggerPanic: m.allowTriggerPanic, datamodelPath: St.default.join(e.dirname, e.filename ?? "schema.prisma"), prismaPath: m.binaryPath ?? undefined, engineEndpoint: m.endpoint, generator: e.generator, showColors: this._errorFormat === "pretty", logLevel: l.log && ml(l.log), logQueries: l.log && !!(typeof l.log == "string" ? l.log === "query" : l.log.find((f) => typeof f == "string" ? f === "query" : f.level === "query")), env: a?.parsed ?? {}, flags: [], engineWasm: e.engineWasm, clientVersion: e.clientVersion, engineVersion: e.engineVersion, previewFeatures: this._previewFeatures, activeProvider: e.activeProvider, inlineSchema: e.inlineSchema, overrideDatasources: ma(l, e.datasourceNames), inlineDatasources: e.inlineDatasources, inlineSchemaHash: e.inlineSchemaHash, tracingHelper: this._tracingHelper, transactionOptions: { maxWait: l.transactionOptions?.maxWait ?? 2000, timeout: l.transactionOptions?.timeout ?? 5000, isolationLevel: l.transactionOptions?.isolationLevel }, logEmitter: o, isBundled: e.isBundled, adapter: i }, this._accelerateEngineConfig = { ...this._engineConfig, accelerateUtils: { resolveDatasourceUrl: Ir, getBatchRequestPayload: Er, prismaGraphQLToJSError: sr, PrismaClientUnknownRequestError: j, PrismaClientInitializationError: S, PrismaClientKnownRequestError: V, debug: N("prisma:client:accelerateEngine"), engineVersion: Al.version, clientVersion: e.clientVersion } }, Ye("clientVersion", e.clientVersion), this._engine = qa(e, this._engineConfig), this._requestHandler = new _n(this, o), l.log)
            for (let f of l.log) {
              let g = typeof f == "string" ? f : f.emit === "stdout" ? f.level : null;
              g && this.$on(g, (h) => {
                Wr.log(`${Wr.tags[g] ?? ""}`, h.message || h.query);
              });
            }
          this._metrics = new yr(this._engine);
        } catch (l) {
          throw l.clientVersion = this._clientVersion, l;
        }
        return this._appliedParent = at(this);
      }
      get [Symbol.toStringTag]() {
        return "PrismaClient";
      }
      $use(n) {
        this._middlewares.use(n);
      }
      $on(n, i) {
        n === "beforeExit" ? this._engine.onBeforeExit(i) : n && this._engineConfig.logEmitter.on(n, i);
      }
      $connect() {
        try {
          return this._engine.start();
        } catch (n) {
          throw n.clientVersion = this._clientVersion, n;
        }
      }
      async $disconnect() {
        try {
          await this._engine.stop();
        } catch (n) {
          throw n.clientVersion = this._clientVersion, n;
        } finally {
          go();
        }
      }
      $executeRawInternal(n, i, o, s) {
        let a = this._activeProvider;
        return this._request({ action: "executeRaw", args: o, transaction: n, clientMethod: i, argsMapper: Yi({ clientMethod: i, activeProvider: a }), callsite: We(this._errorFormat), dataPath: [], middlewareArgsMapper: s });
      }
      $executeRaw(n, ...i) {
        return this._createPrismaPromise((o) => {
          if (n.raw !== undefined || n.sql !== undefined) {
            let [s, a] = Rl(n, i);
            return zi(this._activeProvider, s.text, s.values, Array.isArray(n) ? "prisma.$executeRaw`<SQL>`" : "prisma.$executeRaw(sql`<SQL>`)"), this.$executeRawInternal(o, "$executeRaw", s, a);
          }
          throw new K("`$executeRaw` is a tag function, please use it like the following:\n```\nconst result = await prisma.$executeRaw`UPDATE User SET cool = ${true} WHERE email = ${'user@email.com'};`\n```\n\nOr read our docs at https://www.prisma.io/docs/concepts/components/prisma-client/raw-database-access#executeraw\n", { clientVersion: this._clientVersion });
        });
      }
      $executeRawUnsafe(n, ...i) {
        return this._createPrismaPromise((o) => (zi(this._activeProvider, n, i, "prisma.$executeRawUnsafe(<SQL>, [...values])"), this.$executeRawInternal(o, "$executeRawUnsafe", [n, ...i])));
      }
      $runCommandRaw(n) {
        if (e.activeProvider !== "mongodb")
          throw new K(`The ${e.activeProvider} provider does not support \$runCommandRaw. Use the mongodb provider.`, { clientVersion: this._clientVersion });
        return this._createPrismaPromise((i) => this._request({ args: n, clientMethod: "$runCommandRaw", dataPath: [], action: "runCommandRaw", argsMapper: Xa, callsite: We(this._errorFormat), transaction: i }));
      }
      async $queryRawInternal(n, i, o, s) {
        let a = this._activeProvider;
        return this._request({ action: "queryRaw", args: o, transaction: n, clientMethod: i, argsMapper: Yi({ clientMethod: i, activeProvider: a }), callsite: We(this._errorFormat), dataPath: [], middlewareArgsMapper: s }).then(bl);
      }
      $queryRaw(n, ...i) {
        return this._createPrismaPromise((o) => {
          if (n.raw !== undefined || n.sql !== undefined)
            return this.$queryRawInternal(o, "$queryRaw", ...Rl(n, i));
          throw new K("`$queryRaw` is a tag function, please use it like the following:\n```\nconst result = await prisma.$queryRaw`SELECT * FROM User WHERE id = ${1} OR email = ${'user@email.com'};`\n```\n\nOr read our docs at https://www.prisma.io/docs/concepts/components/prisma-client/raw-database-access#queryraw\n", { clientVersion: this._clientVersion });
        });
      }
      $queryRawUnsafe(n, ...i) {
        return this._createPrismaPromise((o) => this.$queryRawInternal(o, "$queryRawUnsafe", [n, ...i]));
      }
      _transactionWithArray({ promises: n, options: i }) {
        let o = nd.nextId(), s = cl(n.length), a = n.map((l, u) => {
          if (l?.[Symbol.toStringTag] !== "PrismaPromise")
            throw new Error("All elements of the array need to be Prisma Client promises. Hint: Please make sure you are not awaiting the Prisma client calls you intended to pass in the $transaction function.");
          let c = i?.isolationLevel ?? this._engineConfig.transactionOptions.isolationLevel, p = { kind: "batch", id: o, index: u, isolationLevel: c, lock: s };
          return l.requestTransaction?.(p) ?? l;
        });
        return Sl(a);
      }
      async _transactionWithCallback({ callback: n, options: i }) {
        let o = { traceparent: this._tracingHelper.getTraceParent() }, s = { maxWait: i?.maxWait ?? this._engineConfig.transactionOptions.maxWait, timeout: i?.timeout ?? this._engineConfig.transactionOptions.timeout, isolationLevel: i?.isolationLevel ?? this._engineConfig.transactionOptions.isolationLevel }, a = await this._engine.transaction("start", o, s), l;
        try {
          let u = { kind: "itx", ...a };
          l = await n(this._createItxClient(u)), await this._engine.transaction("commit", o, a);
        } catch (u) {
          throw await this._engine.transaction("rollback", o, a).catch(() => {
          }), u;
        }
        return l;
      }
      _createItxClient(n) {
        return at(Pe(Ws(this), [te("_appliedParent", () => this._appliedParent._createItxClient(n)), te("_createPrismaPromise", () => Zi(n)), te(td, () => n.id), nt(pl)]));
      }
      $transaction(n, i) {
        let o;
        typeof n == "function" ? o = () => this._transactionWithCallback({ callback: n, options: i }) : o = () => this._transactionWithArray({ promises: n, options: i });
        let s = { name: "transaction", attributes: { method: "$transaction" } };
        return this._tracingHelper.runInChildSpan(s, o);
      }
      _request(n) {
        n.otelParentCtx = this._tracingHelper.getActiveContext();
        let i = n.middlewareArgsMapper ?? rd, o = { args: i.requestArgsToMiddlewareArgs(n.args), dataPath: n.dataPath, runInTransaction: !!n.transaction, action: n.action, model: n.model }, s = { middleware: { name: "middleware", middleware: true, attributes: { method: "$use" }, active: false }, operation: { name: "operation", attributes: { method: o.action, model: o.model, name: o.model ? `${o.model}.${o.action}` : o.action } } }, a = -1, l = async (u) => {
          let c = this._middlewares.get(++a);
          if (c)
            return this._tracingHelper.runInChildSpan(s.middleware, (A) => c(u, (T) => (A?.end(), l(T))));
          let { runInTransaction: p, args: m, ...f } = u, g = { ...n, ...f };
          m && (g.args = i.middlewareArgsToRequestArgs(m)), n.transaction !== undefined && p === false && delete g.transaction;
          let h = await ta(this, g);
          return g.model ? Zs({ result: h, modelName: g.model, args: g.args, extensions: this._extensions, runtimeDataModel: this._runtimeDataModel }) : h;
        };
        return this._tracingHelper.runInChildSpan(s.operation, () => new Il.AsyncResource("prisma-client-request").runInAsyncScope(() => l(o)));
      }
      async _executeRequest({ args: n, clientMethod: i, dataPath: o, callsite: s, action: a, model: l, argsMapper: u, transaction: c, unpacker: p, otelParentCtx: m, customDataProxyFetch: f }) {
        try {
          n = u ? u(n) : n;
          let g = { name: "serialize" }, h = this._tracingHelper.runInChildSpan(g, () => za({ modelName: l, runtimeDataModel: this._runtimeDataModel, action: a, args: n, clientMethod: i, callsite: s, extensions: this._extensions, errorFormat: this._errorFormat, clientVersion: this._clientVersion }));
          return N.enabled("prisma:client") && (Ye("Prisma Client call:"), Ye(`prisma.${i}(${_s(n)})`), Ye("Generated request:"), Ye(JSON.stringify(h, null, 2) + `
`)), c?.kind === "batch" && await c.lock, this._requestHandler.request({ protocolQuery: h, modelName: l, action: a, clientMethod: i, dataPath: o, callsite: s, args: n, extensions: this._extensions, transaction: c, unpacker: p, otelParentCtx: m, otelChildCtx: this._tracingHelper.getActiveContext(), customDataProxyFetch: f });
        } catch (g) {
          throw g.clientVersion = this._clientVersion, g;
        }
      }
      get $metrics() {
        if (!this._hasPreviewFlag("metrics"))
          throw new K("`metrics` preview feature must be enabled in order to access metrics API", { clientVersion: this._clientVersion });
        return this._metrics;
      }
      _hasPreviewFlag(n) {
        return !!this._engineConfig.previewFeatures?.includes(n);
      }
    }
    return r;
  }
  function Rl(e, r) {
    return id(e) ? [new oe(e, r), ol] : [e, sl];
  }
  function id(e) {
    return Array.isArray(e) && Array.isArray(e.raw);
  }
  function Ll(e) {
    return new Proxy(e, { get(r, t) {
      if (t in r)
        return r[t];
      if (!od.has(t))
        throw new TypeError(`Invalid enum value: ${String(t)}`);
    } });
  }
  function Nl(e) {
    Qr(e, { conflictCheck: "warn" });
  }
  var __dirname = "/Users/ambassador4ik/WebstormProjects/The Brainiest/AuthService/Backend/node_modules/@prisma/client/runtime", __filename = "/Users/ambassador4ik/WebstormProjects/The Brainiest/AuthService/Backend/node_modules/@prisma/client/runtime/library.js";
  var Ol = Object.create;
  var At = Object.defineProperty;
  var Fl = Object.getOwnPropertyDescriptor;
  var Ml = Object.getOwnPropertyNames;
  var $l = Object.getPrototypeOf;
  var ql = Object.prototype.hasOwnProperty;
  var X = (e, r) => () => (r || e((r = { exports: {} }).exports, r), r.exports);
  var Or = (e, r) => {
    for (var t in r)
      At(e, t, { get: r[t], enumerable: true });
  };
  var to = (e, r, t, n) => {
    if (r && typeof r == "object" || typeof r == "function")
      for (let i of Ml(r))
        !ql.call(e, i) && i !== t && At(e, i, { get: () => r[i], enumerable: !(n = Fl(r, i)) || n.enumerable });
    return e;
  };
  var _ = (e, r, t) => (t = e != null ? Ol($l(e)) : {}, to(r || !e || !e.__esModule ? At(t, "default", { value: e, enumerable: true }) : t, e));
  var Bl = (e) => to(At({}, "__esModule", { value: true }), e);
  var bo = X((Rd, Vn) => {
    var v = Vn.exports;
    Vn.exports.default = v;
    var D = "\x1B[", Br = "\x1B]", dr = "\x07", Nt = ";", Eo = process.env.TERM_PROGRAM === "Apple_Terminal";
    v.cursorTo = (e, r) => {
      if (typeof e != "number")
        throw new TypeError("The `x` argument is required");
      return typeof r != "number" ? D + (e + 1) + "G" : D + (r + 1) + ";" + (e + 1) + "H";
    };
    v.cursorMove = (e, r) => {
      if (typeof e != "number")
        throw new TypeError("The `x` argument is required");
      let t = "";
      return e < 0 ? t += D + -e + "D" : e > 0 && (t += D + e + "C"), r < 0 ? t += D + -r + "A" : r > 0 && (t += D + r + "B"), t;
    };
    v.cursorUp = (e = 1) => D + e + "A";
    v.cursorDown = (e = 1) => D + e + "B";
    v.cursorForward = (e = 1) => D + e + "C";
    v.cursorBackward = (e = 1) => D + e + "D";
    v.cursorLeft = D + "G";
    v.cursorSavePosition = Eo ? "\x1B7" : D + "s";
    v.cursorRestorePosition = Eo ? "\x1B8" : D + "u";
    v.cursorGetPosition = D + "6n";
    v.cursorNextLine = D + "E";
    v.cursorPrevLine = D + "F";
    v.cursorHide = D + "?25l";
    v.cursorShow = D + "?25h";
    v.eraseLines = (e) => {
      let r = "";
      for (let t = 0;t < e; t++)
        r += v.eraseLine + (t < e - 1 ? v.cursorUp() : "");
      return e && (r += v.cursorLeft), r;
    };
    v.eraseEndLine = D + "K";
    v.eraseStartLine = D + "1K";
    v.eraseLine = D + "2K";
    v.eraseDown = D + "J";
    v.eraseUp = D + "1J";
    v.eraseScreen = D + "2J";
    v.scrollUp = D + "S";
    v.scrollDown = D + "T";
    v.clearScreen = "\x1Bc";
    v.clearTerminal = process.platform === "win32" ? `${v.eraseScreen}${D}0f` : `${v.eraseScreen}${D}3J${D}H`;
    v.beep = dr;
    v.link = (e, r) => [Br, "8", Nt, Nt, r, dr, e, Br, "8", Nt, Nt, dr].join("");
    v.image = (e, r = {}) => {
      let t = `${Br}1337;File=inline=1`;
      return r.width && (t += `;width=${r.width}`), r.height && (t += `;height=${r.height}`), r.preserveAspectRatio === false && (t += ";preserveAspectRatio=0"), t + ":" + e.toString("base64") + dr;
    };
    v.iTerm = { setCwd: (e = process.cwd()) => `${Br}50;CurrentDir=${e}${dr}`, annotation: (e, r = {}) => {
      let t = `${Br}1337;`, n = typeof r.x < "u", i = typeof r.y < "u";
      if ((n || i) && !(n && i && typeof r.length < "u"))
        throw new Error("`x`, `y` and `length` must be defined when `x` or `y` is defined");
      return e = e.replace(/\|/g, ""), t += r.isHidden ? "AddHiddenAnnotation=" : "AddAnnotation=", r.length > 0 ? t += (n ? [e, r.length, r.x, r.y] : [r.length, e]).join("|") : t += e, t + dr;
    } };
  });
  var jn = X((Ad, wo) => {
    wo.exports = (e, r = process.argv) => {
      let t = e.startsWith("-") ? "" : e.length === 1 ? "-" : "--", n = r.indexOf(t + e), i = r.indexOf("--");
      return n !== -1 && (i === -1 || n < i);
    };
  });
  var vo = X((Id, Po) => {
    var pu = import.meta.require("os"), xo = import.meta.require("tty"), pe = jn(), { env: G } = process, je;
    pe("no-color") || pe("no-colors") || pe("color=false") || pe("color=never") ? je = 0 : (pe("color") || pe("colors") || pe("color=true") || pe("color=always")) && (je = 1);
    "FORCE_COLOR" in G && (G.FORCE_COLOR === "true" ? je = 1 : G.FORCE_COLOR === "false" ? je = 0 : je = G.FORCE_COLOR.length === 0 ? 1 : Math.min(parseInt(G.FORCE_COLOR, 10), 3));
    function Un(e) {
      return e === 0 ? false : { level: e, hasBasic: true, has256: e >= 2, has16m: e >= 3 };
    }
    function Qn(e, r) {
      if (je === 0)
        return 0;
      if (pe("color=16m") || pe("color=full") || pe("color=truecolor"))
        return 3;
      if (pe("color=256"))
        return 2;
      if (e && !r && je === undefined)
        return 0;
      let t = je || 0;
      if (G.TERM === "dumb")
        return t;
      if (process.platform === "win32") {
        let n = pu.release().split(".");
        return Number(n[0]) >= 10 && Number(n[2]) >= 10586 ? Number(n[2]) >= 14931 ? 3 : 2 : 1;
      }
      if ("CI" in G)
        return ["TRAVIS", "CIRCLECI", "APPVEYOR", "GITLAB_CI", "GITHUB_ACTIONS", "BUILDKITE"].some((n) => (n in G)) || G.CI_NAME === "codeship" ? 1 : t;
      if ("TEAMCITY_VERSION" in G)
        return /^(9\.(0*[1-9]\d*)\.|\d{2,}\.)/.test(G.TEAMCITY_VERSION) ? 1 : 0;
      if (G.COLORTERM === "truecolor")
        return 3;
      if ("TERM_PROGRAM" in G) {
        let n = parseInt((G.TERM_PROGRAM_VERSION || "").split(".")[0], 10);
        switch (G.TERM_PROGRAM) {
          case "iTerm.app":
            return n >= 3 ? 3 : 2;
          case "Apple_Terminal":
            return 2;
        }
      }
      return /-256(color)?$/i.test(G.TERM) ? 2 : /^screen|^xterm|^vt100|^vt220|^rxvt|color|ansi|cygwin|linux/i.test(G.TERM) || ("COLORTERM" in G) ? 1 : t;
    }
    function mu(e) {
      let r = Qn(e, e && e.isTTY);
      return Un(r);
    }
    Po.exports = { supportsColor: mu, stdout: Un(Qn(true, xo.isatty(1))), stderr: Un(Qn(true, xo.isatty(2))) };
  });
  var So = X((_d, Co) => {
    var du = vo(), fr = jn();
    function To(e) {
      if (/^\d{3,4}$/.test(e)) {
        let t = /(\d{1,2})(\d{2})/.exec(e);
        return { major: 0, minor: parseInt(t[1], 10), patch: parseInt(t[2], 10) };
      }
      let r = (e || "").split(".").map((t) => parseInt(t, 10));
      return { major: r[0], minor: r[1], patch: r[2] };
    }
    function Gn(e) {
      let { env: r } = process;
      if ("FORCE_HYPERLINK" in r)
        return !(r.FORCE_HYPERLINK.length > 0 && parseInt(r.FORCE_HYPERLINK, 10) === 0);
      if (fr("no-hyperlink") || fr("no-hyperlinks") || fr("hyperlink=false") || fr("hyperlink=never"))
        return false;
      if (fr("hyperlink=true") || fr("hyperlink=always") || "NETLIFY" in r)
        return true;
      if (!du.supportsColor(e) || e && !e.isTTY || process.platform === "win32" || "CI" in r || "TEAMCITY_VERSION" in r)
        return false;
      if ("TERM_PROGRAM" in r) {
        let t = To(r.TERM_PROGRAM_VERSION);
        switch (r.TERM_PROGRAM) {
          case "iTerm.app":
            return t.major === 3 ? t.minor >= 1 : t.major > 3;
          case "WezTerm":
            return t.major >= 20200620;
          case "vscode":
            return t.major > 1 || t.major === 1 && t.minor >= 72;
        }
      }
      if ("VTE_VERSION" in r) {
        if (r.VTE_VERSION === "0.50.0")
          return false;
        let t = To(r.VTE_VERSION);
        return t.major > 0 || t.minor >= 50;
      }
      return false;
    }
    Co.exports = { supportsHyperlink: Gn, stdout: Gn(process.stdout), stderr: Gn(process.stderr) };
  });
  var Ao = X((kd, Vr) => {
    var fu = bo(), Jn = So(), Ro = (e, r, { target: t = "stdout", ...n } = {}) => Jn[t] ? fu.link(e, r) : n.fallback === false ? e : typeof n.fallback == "function" ? n.fallback(e, r) : `${e} (\u200B${r}\u200B)`;
    Vr.exports = (e, r, t = {}) => Ro(e, r, t);
    Vr.exports.stderr = (e, r, t = {}) => Ro(e, r, { target: "stderr", ...t });
    Vr.exports.isSupported = Jn.stdout;
    Vr.exports.stderr.isSupported = Jn.stderr;
  });
  var $o = X((Wd, Iu) => {
    Iu.exports = { name: "dotenv", version: "16.0.3", description: "Loads environment variables from .env file", main: "lib/main.js", types: "lib/main.d.ts", exports: { ".": { require: "./lib/main.js", types: "./lib/main.d.ts", default: "./lib/main.js" }, "./config": "./config.js", "./config.js": "./config.js", "./lib/env-options": "./lib/env-options.js", "./lib/env-options.js": "./lib/env-options.js", "./lib/cli-options": "./lib/cli-options.js", "./lib/cli-options.js": "./lib/cli-options.js", "./package.json": "./package.json" }, scripts: { "dts-check": "tsc --project tests/types/tsconfig.json", lint: "standard", "lint-readme": "standard-markdown", pretest: "npm run lint && npm run dts-check", test: "tap tests/*.js --100 -Rspec", prerelease: "npm test", release: "standard-version" }, repository: { type: "git", url: "git://github.com/motdotla/dotenv.git" }, keywords: ["dotenv", "env", ".env", "environment", "variables", "config", "settings"], readmeFilename: "README.md", license: "BSD-2-Clause", devDependencies: { "@types/node": "^17.0.9", decache: "^4.6.1", dtslint: "^3.7.0", sinon: "^12.0.1", standard: "^16.0.4", "standard-markdown": "^7.1.0", "standard-version": "^9.3.2", tap: "^15.1.6", tar: "^6.1.11", typescript: "^4.5.4" }, engines: { node: ">=12" } };
  });
  var Bo = X((Kd, qt) => {
    var _u = import.meta.require("fs"), qo = import.meta.require("path"), ku = import.meta.require("os"), Du = $o(), Lu = Du.version, Nu = /(?:^|^)\s*(?:export\s+)?([\w.-]+)(?:\s*=\s*?|:\s+?)(\s*'(?:\\'|[^'])*'|\s*"(?:\\"|[^"])*"|\s*`(?:\\`|[^`])*`|[^#\r\n]+)?\s*(?:#.*)?(?:$|$)/mg;
    function Ou(e) {
      let r = {}, t = e.toString();
      t = t.replace(/\r\n?/mg, `
`);
      let n;
      for (;(n = Nu.exec(t)) != null; ) {
        let i = n[1], o = n[2] || "";
        o = o.trim();
        let s = o[0];
        o = o.replace(/^(['"`])([\s\S]*)\1$/mg, "$2"), s === '"' && (o = o.replace(/\\n/g, `
`), o = o.replace(/\\r/g, "\r")), r[i] = o;
      }
      return r;
    }
    function Kn(e) {
      console.log(`[dotenv@${Lu}][DEBUG] ${e}`);
    }
    function Fu(e) {
      return e[0] === "~" ? qo.join(ku.homedir(), e.slice(1)) : e;
    }
    function Mu(e) {
      let r = qo.resolve(process.cwd(), ".env"), t = "utf8", n = !!(e && e.debug), i = !!(e && e.override);
      e && (e.path != null && (r = Fu(e.path)), e.encoding != null && (t = e.encoding));
      try {
        let o = $t.parse(_u.readFileSync(r, { encoding: t }));
        return Object.keys(o).forEach(function(s) {
          Object.prototype.hasOwnProperty.call(process.env, s) ? (i === true && (process.env[s] = o[s]), n && Kn(i === true ? `"${s}" is already defined in \`process.env\` and WAS overwritten` : `"${s}" is already defined in \`process.env\` and was NOT overwritten`)) : process.env[s] = o[s];
        }), { parsed: o };
      } catch (o) {
        return n && Kn(`Failed to load ${r} ${o.message}`), { error: o };
      }
    }
    var $t = { config: Mu, parse: Ou };
    qt.exports.config = $t.config;
    qt.exports.parse = $t.parse;
    qt.exports = $t;
  });
  var Jo = X((tf, Go) => {
    Go.exports = (e) => {
      let r = e.match(/^[ \t]*(?=\S)/gm);
      return r ? r.reduce((t, n) => Math.min(t, n.length), 1 / 0) : 0;
    };
  });
  var Wo = X((nf, Ho) => {
    var Vu = Jo();
    Ho.exports = (e) => {
      let r = Vu(e);
      if (r === 0)
        return e;
      let t = new RegExp(`^[ \\t]{${r}}`, "gm");
      return e.replace(t, "");
    };
  });
  var Zn = X((of, ju) => {
    ju.exports = { name: "@prisma/engines-version", version: "5.10.0-34.5a9203d0590c951969e85a7d07215503f4672eb9", main: "index.js", types: "index.d.ts", license: "Apache-2.0", author: "Tim Suchanek <suchanek@prisma.io>", prisma: { enginesVersion: "5a9203d0590c951969e85a7d07215503f4672eb9" }, repository: { type: "git", url: "https://github.com/prisma/engines-wrapper.git", directory: "packages/engines-version" }, devDependencies: { "@types/node": "18.19.15", typescript: "4.9.5" }, files: ["index.js", "index.d.ts"], scripts: { build: "tsc -d" } };
  });
  var Xn = X((Vt) => {
    Object.defineProperty(Vt, "__esModule", { value: true });
    Vt.enginesVersion = undefined;
    Vt.enginesVersion = Zn().prisma.enginesVersion;
  });
  var ii = X((If, Yo) => {
    Yo.exports = (e, r = 1, t) => {
      if (t = { indent: " ", includeEmptyLines: false, ...t }, typeof e != "string")
        throw new TypeError(`Expected \`input\` to be a \`string\`, got \`${typeof e}\``);
      if (typeof r != "number")
        throw new TypeError(`Expected \`count\` to be a \`number\`, got \`${typeof r}\``);
      if (typeof t.indent != "string")
        throw new TypeError(`Expected \`options.indent\` to be a \`string\`, got \`${typeof t.indent}\``);
      if (r === 0)
        return e;
      let n = t.includeEmptyLines ? /^/gm : /^(?!\s*$)/gm;
      return e.replace(n, t.indent.repeat(r));
    };
  });
  var rs = X((Df, es) => {
    es.exports = ({ onlyFirst: e = false } = {}) => {
      let r = ["[\\u001B\\u009B][[\\]()#;?]*(?:(?:(?:(?:;[-a-zA-Z\\d\\/#&.:=?%@~_]+)*|[a-zA-Z\\d]+(?:;[-a-zA-Z\\d\\/#&.:=?%@~_]*)*)?\\u0007)", "(?:(?:\\d{1,4}(?:;\\d{0,4})*)?[\\dA-PR-TZcf-ntqry=><~]))"].join("|");
      return new RegExp(r, e ? undefined : "g");
    };
  });
  var li = X((Lf, ts) => {
    var Zu = rs();
    ts.exports = (e) => typeof e == "string" ? e.replace(Zu(), "") : e;
  });
  var ns = X((Ff, Ut) => {
    Ut.exports = (e = {}) => {
      let r;
      if (e.repoUrl)
        r = e.repoUrl;
      else if (e.user && e.repo)
        r = `https://github.com/${e.user}/${e.repo}`;
      else
        throw new Error("You need to specify either the `repoUrl` option or both the `user` and `repo` options");
      let t = new URL(`${r}/issues/new`), n = ["body", "title", "labels", "template", "milestone", "assignee", "projects"];
      for (let i of n) {
        let o = e[i];
        if (o !== undefined) {
          if (i === "labels" || i === "projects") {
            if (!Array.isArray(o))
              throw new TypeError(`The \`${i}\` option should be an array`);
            o = o.join(",");
          }
          t.searchParams.set(i, o);
        }
      }
      return t.toString();
    };
    Ut.exports.default = Ut.exports;
  });
  var Hi = X((C0, Ba) => {
    Ba.exports = function() {
      function e(r, t, n, i, o) {
        return r < t || n < t ? r > n ? n + 1 : r + 1 : i === o ? t : t + 1;
      }
      return function(r, t) {
        if (r === t)
          return 0;
        if (r.length > t.length) {
          var n = r;
          r = t, t = n;
        }
        for (var i = r.length, o = t.length;i > 0 && r.charCodeAt(i - 1) === t.charCodeAt(o - 1); )
          i--, o--;
        for (var s = 0;s < i && r.charCodeAt(s) === t.charCodeAt(s); )
          s++;
        if (i -= s, o -= s, i === 0 || o < 3)
          return o;
        var a = 0, l, u, c, p, m, f, g, h, A, T, C, E, I = [];
        for (l = 0;l < i; l++)
          I.push(l + 1), I.push(r.charCodeAt(s + l));
        for (var me = I.length - 1;a < o - 3; )
          for (A = t.charCodeAt(s + (u = a)), T = t.charCodeAt(s + (c = a + 1)), C = t.charCodeAt(s + (p = a + 2)), E = t.charCodeAt(s + (m = a + 3)), f = a += 4, l = 0;l < me; l += 2)
            g = I[l], h = I[l + 1], u = e(g, u, c, A, h), c = e(u, c, p, T, h), p = e(c, p, m, C, h), f = e(p, m, f, E, h), I[l] = f, m = p, p = c, c = u, u = g;
        for (;a < o; )
          for (A = t.charCodeAt(s + (u = a)), f = ++a, l = 0;l < me; l += 2)
            g = I[l], I[l] = f = e(g, u, f, A, I[l + 1]), u = g;
        return f;
      };
    }();
  });
  var sd = {};
  Or(sd, { Debug: () => On, Decimal: () => Te, Extensions: () => Dn, MetricsClient: () => yr, NotFoundError: () => Le, PrismaClientInitializationError: () => S, PrismaClientKnownRequestError: () => V, PrismaClientRustPanicError: () => ue, PrismaClientUnknownRequestError: () => j, PrismaClientValidationError: () => K, Public: () => Ln, Sql: () => oe, defineDmmfProperty: () => os, detectRuntime: () => gn, empty: () => as, getPrismaClient: () => Dl, join: () => ss, makeStrictEnum: () => Ll, objectEnumValues: () => Jt, raw: () => yi, sqltag: () => Ei, warnEnvConflicts: () => Nl, warnOnce: () => Kr });
  module.exports = Bl(sd);
  var Dn = {};
  Or(Dn, { defineExtension: () => no, getExtensionContext: () => io });
  var Ln = {};
  Or(Ln, { validator: () => oo });
  var It = {};
  Or(It, { $: () => co, bgBlack: () => zl, bgBlue: () => eu, bgCyan: () => tu, bgGreen: () => Zl, bgMagenta: () => ru, bgRed: () => Yl, bgWhite: () => nu, bgYellow: () => Xl, black: () => Jl, blue: () => Ze, bold: () => W, cyan: () => _e, dim: () => Ie, gray: () => Fr, green: () => $e, grey: () => Kl, hidden: () => Ql, inverse: () => Ul, italic: () => jl, magenta: () => Hl, red: () => ce, reset: () => Vl, strikethrough: () => Gl, underline: () => ee, white: () => Wl, yellow: () => de });
  var Nn;
  var so;
  var ao;
  var lo;
  var uo = true;
  typeof process < "u" && ({ FORCE_COLOR: Nn, NODE_DISABLE_COLORS: so, NO_COLOR: ao, TERM: lo } = process.env || {}, uo = process.stdout && process.stdout.isTTY);
  var co = { enabled: !so && ao == null && lo !== "dumb" && (Nn != null && Nn !== "0" || uo) };
  var Vl = F(0, 0);
  var W = F(1, 22);
  var Ie = F(2, 22);
  var jl = F(3, 23);
  var ee = F(4, 24);
  var Ul = F(7, 27);
  var Ql = F(8, 28);
  var Gl = F(9, 29);
  var Jl = F(30, 39);
  var ce = F(31, 39);
  var $e = F(32, 39);
  var de = F(33, 39);
  var Ze = F(34, 39);
  var Hl = F(35, 39);
  var _e = F(36, 39);
  var Wl = F(37, 39);
  var Fr = F(90, 39);
  var Kl = F(90, 39);
  var zl = F(40, 49);
  var Yl = F(41, 49);
  var Zl = F(42, 49);
  var Xl = F(43, 49);
  var eu = F(44, 49);
  var ru = F(45, 49);
  var tu = F(46, 49);
  var nu = F(47, 49);
  var iu = 100;
  var po = ["green", "yellow", "blue", "magenta", "cyan", "red"];
  var Mr = [];
  var mo = Date.now();
  var ou = 0;
  globalThis.DEBUG ?? (globalThis.DEBUG = process.env.DEBUG ?? "");
  globalThis.DEBUG_COLORS ?? (globalThis.DEBUG_COLORS = process.env.DEBUG_COLORS ? process.env.DEBUG_COLORS === "true" : true);
  var $r = { enable(e) {
    typeof e == "string" && (globalThis.DEBUG = e);
  }, disable() {
    let e = globalThis.DEBUG;
    return globalThis.DEBUG = "", e;
  }, enabled(e) {
    let r = globalThis.DEBUG.split(",").map((i) => i.replace(/[.+?^${}()|[\]\\]/g, "\\$&")), t = r.some((i) => i === "" || i[0] === "-" ? false : e.match(RegExp(i.split("*").join(".*") + "$"))), n = r.some((i) => i === "" || i[0] !== "-" ? false : e.match(RegExp(i.slice(1).split("*").join(".*") + "$")));
    return t && !n;
  }, log: (...e) => {
    let [r, t, ...n] = e, i;
    typeof import.meta.require == "function" && typeof process < "u" && typeof process.stderr < "u" && typeof process.stderr.write == "function" ? i = (...o) => {
      let s = import.meta.require("util");
      process.stderr.write(s.format(...o) + `
`);
    } : i = console.warn ?? console.log, i(`${r} ${t}`, ...n);
  }, formatters: {} };
  var On = new Proxy(su, { get: (e, r) => $r[r], set: (e, r, t) => $r[r] = t });
  var N = On;
  var ho = _(import.meta.require("fs"));
  var Mn = ["darwin", "darwin-arm64", "debian-openssl-1.0.x", "debian-openssl-1.1.x", "debian-openssl-3.0.x", "rhel-openssl-1.0.x", "rhel-openssl-1.1.x", "rhel-openssl-3.0.x", "linux-arm64-openssl-1.1.x", "linux-arm64-openssl-1.0.x", "linux-arm64-openssl-3.0.x", "linux-arm-openssl-1.1.x", "linux-arm-openssl-1.0.x", "linux-arm-openssl-3.0.x", "linux-musl", "linux-musl-openssl-3.0.x", "linux-musl-arm64-openssl-1.1.x", "linux-musl-arm64-openssl-3.0.x", "linux-nixos", "linux-static-x64", "linux-static-arm64", "windows", "freebsd11", "freebsd12", "freebsd13", "freebsd14", "freebsd15", "openbsd", "netbsd", "arm"];
  var _t = "libquery_engine";
  var Do = _(import.meta.require("child_process"));
  var Hn = _(import.meta.require("fs/promises"));
  var Ft = _(import.meta.require("os"));
  var ke = Symbol.for("@ts-pattern/matcher");
  var lu = Symbol.for("@ts-pattern/isVariadic");
  var Lt = "@ts-pattern/anonymous-select-key";
  var $n = (e) => !!(e && typeof e == "object");
  var Dt = (e) => e && !!e[ke];
  var we = (e, r, t) => {
    if (Dt(e)) {
      let n = e[ke](), { matched: i, selections: o } = n.match(r);
      return i && o && Object.keys(o).forEach((s) => t(s, o[s])), i;
    }
    if ($n(e)) {
      if (!$n(r))
        return false;
      if (Array.isArray(e)) {
        if (!Array.isArray(r))
          return false;
        let n = [], i = [], o = [];
        for (let s of e.keys()) {
          let a = e[s];
          Dt(a) && a[lu] ? o.push(a) : o.length ? i.push(a) : n.push(a);
        }
        if (o.length) {
          if (o.length > 1)
            throw new Error("Pattern error: Using `...P.array(...)` several times in a single pattern is not allowed.");
          if (r.length < n.length + i.length)
            return false;
          let s = r.slice(0, n.length), a = i.length === 0 ? [] : r.slice(-i.length), l = r.slice(n.length, i.length === 0 ? 1 / 0 : -i.length);
          return n.every((u, c) => we(u, s[c], t)) && i.every((u, c) => we(u, a[c], t)) && (o.length === 0 || we(o[0], l, t));
        }
        return e.length === r.length && e.every((s, a) => we(s, r[a], t));
      }
      return Object.keys(e).every((n) => {
        let i = e[n];
        return ((n in r) || Dt(o = i) && o[ke]().matcherType === "optional") && we(i, r[n], t);
        var o;
      });
    }
    return Object.is(r, e);
  };
  var Ve = (e) => {
    var r, t, n;
    return $n(e) ? Dt(e) ? (r = (t = (n = e[ke]()).getSelectionKeys) == null ? undefined : t.call(n)) != null ? r : [] : Array.isArray(e) ? qr(e, Ve) : qr(Object.values(e), Ve) : [];
  };
  var qr = (e, r) => e.reduce((t, n) => t.concat(r(n)), []);
  var bd = fe(k(function(e) {
    return true;
  }));
  var er = (e) => Object.assign(fe(e), { startsWith: (r) => {
    return er(B(e, (t = r, k((n) => Xe(n) && n.startsWith(t)))));
    var t;
  }, endsWith: (r) => {
    return er(B(e, (t = r, k((n) => Xe(n) && n.endsWith(t)))));
    var t;
  }, minLength: (r) => er(B(e, ((t) => k((n) => Xe(n) && n.length >= t))(r))), maxLength: (r) => er(B(e, ((t) => k((n) => Xe(n) && n.length <= t))(r))), includes: (r) => {
    return er(B(e, (t = r, k((n) => Xe(n) && n.includes(t)))));
    var t;
  }, regex: (r) => {
    return er(B(e, (t = r, k((n) => Xe(n) && !!n.match(t)))));
    var t;
  } });
  var wd = er(k(Xe));
  var be = (e) => Object.assign(fe(e), { between: (r, t) => be(B(e, ((n, i) => k((o) => Ee(o) && n <= o && i >= o))(r, t))), lt: (r) => be(B(e, ((t) => k((n) => Ee(n) && n < t))(r))), gt: (r) => be(B(e, ((t) => k((n) => Ee(n) && n > t))(r))), lte: (r) => be(B(e, ((t) => k((n) => Ee(n) && n <= t))(r))), gte: (r) => be(B(e, ((t) => k((n) => Ee(n) && n >= t))(r))), int: () => be(B(e, k((r) => Ee(r) && Number.isInteger(r)))), finite: () => be(B(e, k((r) => Ee(r) && Number.isFinite(r)))), positive: () => be(B(e, k((r) => Ee(r) && r > 0))), negative: () => be(B(e, k((r) => Ee(r) && r < 0))) });
  var xd = be(k(Ee));
  var Be = (e) => Object.assign(fe(e), { between: (r, t) => Be(B(e, ((n, i) => k((o) => qe(o) && n <= o && i >= o))(r, t))), lt: (r) => Be(B(e, ((t) => k((n) => qe(n) && n < t))(r))), gt: (r) => Be(B(e, ((t) => k((n) => qe(n) && n > t))(r))), lte: (r) => Be(B(e, ((t) => k((n) => qe(n) && n <= t))(r))), gte: (r) => Be(B(e, ((t) => k((n) => qe(n) && n >= t))(r))), positive: () => Be(B(e, k((r) => qe(r) && r > 0))), negative: () => Be(B(e, k((r) => qe(r) && r < 0))) });
  var Pd = Be(k(qe));
  var vd = fe(k(function(e) {
    return typeof e == "boolean";
  }));
  var Td = fe(k(function(e) {
    return typeof e == "symbol";
  }));
  var Cd = fe(k(function(e) {
    return e == null;
  }));
  var qn = { matched: false, value: undefined };
  var Bn = class e {
    constructor(r, t) {
      this.input = undefined, this.state = undefined, this.input = r, this.state = t;
    }
    with(...r) {
      if (this.state.matched)
        return this;
      let t = r[r.length - 1], n = [r[0]], i;
      r.length === 3 && typeof r[1] == "function" ? i = r[1] : r.length > 2 && n.push(...r.slice(1, r.length - 1));
      let o = false, s = {}, a = (u, c) => {
        o = true, s[u] = c;
      }, l = !n.some((u) => we(u, this.input, a)) || i && !i(this.input) ? qn : { matched: true, value: t(o ? Lt in s ? s[Lt] : s : this.input, this.input) };
      return new e(this.input, l);
    }
    when(r, t) {
      if (this.state.matched)
        return this;
      let n = !!r(this.input);
      return new e(this.input, n ? { matched: true, value: t(this.input, this.input) } : qn);
    }
    otherwise(r) {
      return this.state.matched ? this.state.value : r(this.input);
    }
    exhaustive() {
      if (this.state.matched)
        return this.state.value;
      let r;
      try {
        r = JSON.stringify(this.input);
      } catch {
        r = this.input;
      }
      throw new Error(`Pattern matching error: no pattern matches value ${r}`);
    }
    run() {
      return this.exhaustive();
    }
    returnType() {
      return this;
    }
  };
  var Lo = import.meta.require("util");
  var Io = _(Ao());
  var gu = { warn: de("prisma:warn") };
  var hu = { warn: () => !process.env.PRISMA_DISABLE_WARNINGS };
  var yu = (0, Lo.promisify)(Do.default.exec);
  var ie = N("prisma:get-platform");
  var Eu = ["1.0.x", "1.1.x", "3.0.x"];
  var Ot = {};
  var Yn = _(Bo());
  var Bt = _(import.meta.require("fs"));
  var gr = _(import.meta.require("path"));
  var zn = N("prisma:tryLoadEnv");
  var Qo = "library";
  var Uu = _(Xn());
  var M = _(import.meta.require("path"));
  var Qu = _(Xn());
  var gf = N("prisma:engines");
  var hf = "libquery-engine";
  M.default.join(__dirname, "../query-engine-darwin");
  M.default.join(__dirname, "../query-engine-darwin-arm64");
  M.default.join(__dirname, "../query-engine-debian-openssl-1.0.x");
  M.default.join(__dirname, "../query-engine-debian-openssl-1.1.x");
  M.default.join(__dirname, "../query-engine-debian-openssl-3.0.x");
  M.default.join(__dirname, "../query-engine-linux-static-x64");
  M.default.join(__dirname, "../query-engine-linux-static-arm64");
  M.default.join(__dirname, "../query-engine-rhel-openssl-1.0.x");
  M.default.join(__dirname, "../query-engine-rhel-openssl-1.1.x");
  M.default.join(__dirname, "../query-engine-rhel-openssl-3.0.x");
  M.default.join(__dirname, "../libquery_engine-darwin.dylib.node");
  M.default.join(__dirname, "../libquery_engine-darwin-arm64.dylib.node");
  M.default.join(__dirname, "../libquery_engine-debian-openssl-1.0.x.so.node");
  M.default.join(__dirname, "../libquery_engine-debian-openssl-1.1.x.so.node");
  M.default.join(__dirname, "../libquery_engine-debian-openssl-3.0.x.so.node");
  M.default.join(__dirname, "../libquery_engine-linux-arm64-openssl-1.0.x.so.node");
  M.default.join(__dirname, "../libquery_engine-linux-arm64-openssl-1.1.x.so.node");
  M.default.join(__dirname, "../libquery_engine-linux-arm64-openssl-3.0.x.so.node");
  M.default.join(__dirname, "../libquery_engine-linux-musl.so.node");
  M.default.join(__dirname, "../libquery_engine-linux-musl-openssl-3.0.x.so.node");
  M.default.join(__dirname, "../libquery_engine-rhel-openssl-1.0.x.so.node");
  M.default.join(__dirname, "../libquery_engine-rhel-openssl-1.1.x.so.node");
  M.default.join(__dirname, "../libquery_engine-rhel-openssl-3.0.x.so.node");
  M.default.join(__dirname, "../query_engine-windows.dll.node");
  var ei = _(import.meta.require("fs"));
  var zo = N("chmodPlusX");
  var De;
  ((r) => {
    let e;
    ((E) => (E.findUnique = "findUnique", E.findUniqueOrThrow = "findUniqueOrThrow", E.findFirst = "findFirst", E.findFirstOrThrow = "findFirstOrThrow", E.findMany = "findMany", E.create = "create", E.createMany = "createMany", E.update = "update", E.updateMany = "updateMany", E.upsert = "upsert", E.delete = "delete", E.deleteMany = "deleteMany", E.groupBy = "groupBy", E.count = "count", E.aggregate = "aggregate", E.findRaw = "findRaw", E.aggregateRaw = "aggregateRaw"))(e = r.ModelAction || (r.ModelAction = {}));
  })(De || (De = {}));
  var Jr = _(import.meta.require("path"));
  var Zo = _(ii());
  var oi = class {
    constructor(r) {
      this.config = r;
    }
    toString() {
      let { config: r } = this, t = r.provider.fromEnvVar ? `env("${r.provider.fromEnvVar}")` : r.provider.value, n = JSON.parse(JSON.stringify({ provider: t, binaryTargets: Gu(r.binaryTargets) }));
      return `generator ${r.name} {
${(0, Zo.default)(Ju(n), 2)}
}`;
    }
  };
  var Wr = {};
  Or(Wr, { error: () => zu, info: () => Ku, log: () => Wu, query: () => Yu, should: () => Xo, tags: () => Hr, warn: () => ai });
  var Hr = { error: ce("prisma:error"), warn: de("prisma:warn"), info: _e("prisma:info"), query: Ze("prisma:query") };
  var Xo = { warn: () => !process.env.PRISMA_DISABLE_WARNINGS };
  var ci = (e, r) => e.reduce((t, n) => (t[r(n)] = n, t), {});
  var is = new Set;
  var Kr = (e, r, ...t) => {
    is.has(e) || (is.add(e), ai(r, ...t));
  };
  var V = class extends Error {
    constructor(r, { code: t, clientVersion: n, meta: i, batchRequestIdx: o }) {
      super(r), this.name = "PrismaClientKnownRequestError", this.code = t, this.clientVersion = n, this.meta = i, Object.defineProperty(this, "batchRequestIdx", { value: o, enumerable: false, writable: true });
    }
    get [Symbol.toStringTag]() {
      return "PrismaClientKnownRequestError";
    }
  };
  w(V, "PrismaClientKnownRequestError");
  var Le = class extends V {
    constructor(r, t) {
      super(r, { code: "P2025", clientVersion: t }), this.name = "NotFoundError";
    }
  };
  w(Le, "NotFoundError");
  var S = class e extends Error {
    constructor(r, t, n) {
      super(r), this.name = "PrismaClientInitializationError", this.clientVersion = t, this.errorCode = n, Error.captureStackTrace(e);
    }
    get [Symbol.toStringTag]() {
      return "PrismaClientInitializationError";
    }
  };
  w(S, "PrismaClientInitializationError");
  var ue = class extends Error {
    constructor(r, t) {
      super(r), this.name = "PrismaClientRustPanicError", this.clientVersion = t;
    }
    get [Symbol.toStringTag]() {
      return "PrismaClientRustPanicError";
    }
  };
  w(ue, "PrismaClientRustPanicError");
  var j = class extends Error {
    constructor(r, { clientVersion: t, batchRequestIdx: n }) {
      super(r), this.name = "PrismaClientUnknownRequestError", this.clientVersion = t, Object.defineProperty(this, "batchRequestIdx", { value: n, writable: true, enumerable: false });
    }
    get [Symbol.toStringTag]() {
      return "PrismaClientUnknownRequestError";
    }
  };
  w(j, "PrismaClientUnknownRequestError");
  var K = class extends Error {
    constructor(t, { clientVersion: n }) {
      super(t);
      this.name = "PrismaClientValidationError";
      this.clientVersion = n;
    }
    get [Symbol.toStringTag]() {
      return "PrismaClientValidationError";
    }
  };
  w(K, "PrismaClientValidationError");
  var yr = class {
    constructor(r) {
      this._engine = r;
    }
    prometheus(r) {
      return this._engine.metrics({ format: "prometheus", ...r });
    }
    json(r) {
      return this._engine.metrics({ format: "json", ...r });
    }
  };
  var Gt = Symbol();
  var di = new WeakMap;
  var Ne = class {
    constructor(r) {
      r === Gt ? di.set(this, `Prisma.${this._getName()}`) : di.set(this, `new Prisma.${this._getNamespace()}.${this._getName()}()`);
    }
    _getName() {
      return this.constructor.name;
    }
    toString() {
      return di.get(this);
    }
  };
  var Yr = class extends Ne {
    _getNamespace() {
      return "NullTypes";
    }
  };
  var Zr = class extends Yr {
  };
  fi(Zr, "DbNull");
  var Xr = class extends Yr {
  };
  fi(Xr, "JsonNull");
  var et = class extends Yr {
  };
  fi(et, "AnyNull");
  var Jt = { classes: { DbNull: Zr, JsonNull: Xr, AnyNull: et }, instances: { DbNull: new Zr(Gt), JsonNull: new Xr(Gt), AnyNull: new et(Gt) } };
  var gi = class {
    constructor() {
      this.registeredErrors = [];
    }
    consumeError(r) {
      return this.registeredErrors[r];
    }
    registerNewError(r) {
      let t = 0;
      for (;this.registeredErrors[t] !== undefined; )
        t++;
      return this.registeredErrors[t] = { error: r }, t;
    }
  };
  var hi = (e) => {
    let r = new gi, t = nr(r, e.startTransaction.bind(e)), n = { errorRegistry: r, queryRaw: nr(r, e.queryRaw.bind(e)), executeRaw: nr(r, e.executeRaw.bind(e)), provider: e.provider, startTransaction: async (...i) => (await t(...i)).map((s) => ec(r, s)) };
    return e.getConnectionInfo && (n.getConnectionInfo = rc(r, e.getConnectionInfo.bind(e))), n;
  };
  var ec = (e, r) => ({ provider: r.provider, options: r.options, queryRaw: nr(e, r.queryRaw.bind(r)), executeRaw: nr(e, r.executeRaw.bind(r)), commit: nr(e, r.commit.bind(r)), rollback: nr(e, r.rollback.bind(r)) });
  var Al = _(Zn());
  var Il = import.meta.require("async_hooks");
  var _l = import.meta.require("events");
  var kl = _(import.meta.require("fs"));
  var St = _(import.meta.require("path"));
  var oe = class e {
    constructor(r, t) {
      if (r.length - 1 !== t.length)
        throw r.length === 0 ? new TypeError("Expected at least 1 string") : new TypeError(`Expected ${r.length} strings to have ${r.length - 1} values`);
      let n = t.reduce((s, a) => s + (a instanceof e ? a.values.length : 1), 0);
      this.values = new Array(n), this.strings = new Array(n + 1), this.strings[0] = r[0];
      let i = 0, o = 0;
      for (;i < t.length; ) {
        let s = t[i++], a = r[i];
        if (s instanceof e) {
          this.strings[o] += s.strings[0];
          let l = 0;
          for (;l < s.values.length; )
            this.values[o++] = s.values[l++], this.strings[o] = s.strings[l];
          this.strings[o] += a;
        } else
          this.values[o++] = s, this.strings[o] = a;
      }
    }
    get text() {
      let r = this.strings.length, t = 1, n = this.strings[0];
      for (;t < r; )
        n += `\$${t}${this.strings[t++]}`;
      return n;
    }
    get sql() {
      let r = this.strings.length, t = 1, n = this.strings[0];
      for (;t < r; )
        n += `?${this.strings[t++]}`;
      return n;
    }
    get statement() {
      let r = this.strings.length, t = 1, n = this.strings[0];
      for (;t < r; )
        n += `:${t}${this.strings[t++]}`;
      return n;
    }
    inspect() {
      return { text: this.text, sql: this.sql, values: this.values };
    }
  };
  var as = yi("");
  var xe = class {
    constructor() {
      this._map = new Map;
    }
    get(r) {
      return this._map.get(r)?.value;
    }
    set(r, t) {
      this._map.set(r, { value: t });
    }
    getOrCreate(r, t) {
      let n = this._map.get(r);
      if (n)
        return n.value;
      let i = t();
      return this.set(r, i), i;
    }
  };
  var cs = import.meta.require("util");
  var Ht = { enumerable: true, configurable: true, writable: true };
  var ls = Symbol.for("nodejs.util.inspect.custom");
  var br = class {
    constructor(r = 0, t) {
      this.context = t;
      this.lines = [];
      this.currentLine = "";
      this.currentIndent = 0;
      this.currentIndent = r;
    }
    write(r) {
      return typeof r == "string" ? this.currentLine += r : r.write(this), this;
    }
    writeJoined(r, t) {
      let n = t.length - 1;
      for (let i = 0;i < t.length; i++)
        this.write(t[i]), i !== n && this.write(r);
      return this;
    }
    writeLine(r) {
      return this.write(r).newLine();
    }
    newLine() {
      this.lines.push(this.indentedCurrentLine()), this.currentLine = "", this.marginSymbol = undefined;
      let r = this.afterNextNewLineCallback;
      return this.afterNextNewLineCallback = undefined, r?.(), this;
    }
    withIndent(r) {
      return this.indent(), r(this), this.unindent(), this;
    }
    afterNextNewline(r) {
      return this.afterNextNewLineCallback = r, this;
    }
    indent() {
      return this.currentIndent++, this;
    }
    unindent() {
      return this.currentIndent > 0 && this.currentIndent--, this;
    }
    addMarginSymbol(r) {
      return this.marginSymbol = r, this;
    }
    toString() {
      return this.lines.concat(this.indentedCurrentLine()).join(`
`);
    }
    getCurrentLineLength() {
      return this.currentLine.length;
    }
    indentedCurrentLine() {
      let r = this.currentLine.padStart(this.currentLine.length + 2 * this.currentIndent);
      return this.marginSymbol ? this.marginSymbol + r.slice(1) : r;
    }
  };
  var xr = 9000000000000000;
  var Je = 1e9;
  var bi = "0123456789abcdef";
  var Yt = "2.3025850929940456840179914546843642076011014886287729760333279009675726096773524802359972050895982983419677840422862486334095254650828067566662873690987816894829072083255546808437998948262331985283935053089653777326288461633662222876982198867465436674744042432743651550489343149393914796194044002221051017141748003688084012647080685567743216228355220114804663715659121373450747856947683463616792101806445070648000277502684916746550586856935673420670581136429224554405758925724208241314695689016758940256776311356919292033376587141660230105703089634572075440370847469940168269282808481184289314848524948644871927809676271275775397027668605952496716674183485704422507197965004714951050492214776567636938662976979522110718264549734772662425709429322582798502585509785265383207606726317164309505995087807523710333101197857547331541421808427543863591778117054309827482385045648019095610299291824318237525357709750539565187697510374970888692180205189339507238539205144634197265287286965110862571492198849978748873771345686209167058";
  var Zt = "3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632789";
  var wi = { precision: 20, rounding: 4, modulo: 1, toExpNeg: -7, toExpPos: 21, minE: -xr, maxE: xr, crypto: false };
  var gs;
  var Oe;
  var x = true;
  var en = "[DecimalError] ";
  var Ge = en + "Invalid argument: ";
  var hs = en + "Precision limit exceeded";
  var ys = en + "crypto unavailable";
  var Es = "[object Decimal]";
  var re = Math.floor;
  var Q = Math.pow;
  var nc = /^0b([01]+(\.[01]*)?|\.[01]+)(p[+-]?\d+)?$/i;
  var ic = /^0x([0-9a-f]+(\.[0-9a-f]*)?|\.[0-9a-f]+)(p[+-]?\d+)?$/i;
  var oc = /^0o([0-7]+(\.[0-7]*)?|\.[0-7]+)(p[+-]?\d+)?$/i;
  var bs = /^(\d+(\.\d*)?|\.\d+)(e[+-]?\d+)?$/i;
  var he = 1e7;
  var b = 7;
  var sc = 9007199254740991;
  var ac = Yt.length - 1;
  var xi = Zt.length - 1;
  var d = { toStringTag: Es };
  d.absoluteValue = d.abs = function() {
    var e = new this.constructor(this);
    return e.s < 0 && (e.s = 1), y(e);
  };
  d.ceil = function() {
    return y(new this.constructor(this), this.e + 1, 2);
  };
  d.clampedTo = d.clamp = function(e, r) {
    var t, n = this, i = n.constructor;
    if (e = new i(e), r = new i(r), !e.s || !r.s)
      return new i(NaN);
    if (e.gt(r))
      throw Error(Ge + r);
    return t = n.cmp(e), t < 0 ? e : n.cmp(r) > 0 ? r : new i(n);
  };
  d.comparedTo = d.cmp = function(e) {
    var r, t, n, i, o = this, s = o.d, a = (e = new o.constructor(e)).d, l = o.s, u = e.s;
    if (!s || !a)
      return !l || !u ? NaN : l !== u ? l : s === a ? 0 : !s ^ l < 0 ? 1 : -1;
    if (!s[0] || !a[0])
      return s[0] ? l : a[0] ? -u : 0;
    if (l !== u)
      return l;
    if (o.e !== e.e)
      return o.e > e.e ^ l < 0 ? 1 : -1;
    for (n = s.length, i = a.length, r = 0, t = n < i ? n : i;r < t; ++r)
      if (s[r] !== a[r])
        return s[r] > a[r] ^ l < 0 ? 1 : -1;
    return n === i ? 0 : n > i ^ l < 0 ? 1 : -1;
  };
  d.cosine = d.cos = function() {
    var e, r, t = this, n = t.constructor;
    return t.d ? t.d[0] ? (e = n.precision, r = n.rounding, n.precision = e + Math.max(t.e, t.sd()) + b, n.rounding = 1, t = lc(n, Ts(n, t)), n.precision = e, n.rounding = r, y(Oe == 2 || Oe == 3 ? t.neg() : t, e, r, true)) : new n(1) : new n(NaN);
  };
  d.cubeRoot = d.cbrt = function() {
    var e, r, t, n, i, o, s, a, l, u, c = this, p = c.constructor;
    if (!c.isFinite() || c.isZero())
      return new p(c);
    for (x = false, o = c.s * Q(c.s * c, 1 / 3), !o || Math.abs(o) == 1 / 0 ? (t = z(c.d), e = c.e, (o = (e - t.length + 1) % 3) && (t += o == 1 || o == -2 ? "0" : "00"), o = Q(t, 1 / 3), e = re((e + 1) / 3) - (e % 3 == (e < 0 ? -1 : 2)), o == 1 / 0 ? t = "5e" + e : (t = o.toExponential(), t = t.slice(0, t.indexOf("e") + 1) + e), n = new p(t), n.s = c.s) : n = new p(o.toString()), s = (e = p.precision) + 3;; )
      if (a = n, l = a.times(a).times(a), u = l.plus(c), n = O(u.plus(c).times(a), u.plus(l), s + 2, 1), z(a.d).slice(0, s) === (t = z(n.d)).slice(0, s))
        if (t = t.slice(s - 3, s + 1), t == "9999" || !i && t == "4999") {
          if (!i && (y(a, e + 1, 0), a.times(a).times(a).eq(c))) {
            n = a;
            break;
          }
          s += 4, i = 1;
        } else {
          (!+t || !+t.slice(1) && t.charAt(0) == "5") && (y(n, e + 1, 1), r = !n.times(n).times(n).eq(c));
          break;
        }
    return x = true, y(n, e, p.rounding, r);
  };
  d.decimalPlaces = d.dp = function() {
    var e, r = this.d, t = NaN;
    if (r) {
      if (e = r.length - 1, t = (e - re(this.e / b)) * b, e = r[e], e)
        for (;e % 10 == 0; e /= 10)
          t--;
      t < 0 && (t = 0);
    }
    return t;
  };
  d.dividedBy = d.div = function(e) {
    return O(this, new this.constructor(e));
  };
  d.dividedToIntegerBy = d.divToInt = function(e) {
    var r = this, t = r.constructor;
    return y(O(r, new t(e), 0, 1, 1), t.precision, t.rounding);
  };
  d.equals = d.eq = function(e) {
    return this.cmp(e) === 0;
  };
  d.floor = function() {
    return y(new this.constructor(this), this.e + 1, 3);
  };
  d.greaterThan = d.gt = function(e) {
    return this.cmp(e) > 0;
  };
  d.greaterThanOrEqualTo = d.gte = function(e) {
    var r = this.cmp(e);
    return r == 1 || r === 0;
  };
  d.hyperbolicCosine = d.cosh = function() {
    var e, r, t, n, i, o = this, s = o.constructor, a = new s(1);
    if (!o.isFinite())
      return new s(o.s ? 1 / 0 : NaN);
    if (o.isZero())
      return a;
    t = s.precision, n = s.rounding, s.precision = t + Math.max(o.e, o.sd()) + 4, s.rounding = 1, i = o.d.length, i < 32 ? (e = Math.ceil(i / 3), r = (1 / tn(4, e)).toString()) : (e = 16, r = "2.3283064365386962890625e-10"), o = Pr(s, 1, o.times(r), new s(1), true);
    for (var l, u = e, c = new s(8);u--; )
      l = o.times(o), o = a.minus(l.times(c.minus(l.times(c))));
    return y(o, s.precision = t, s.rounding = n, true);
  };
  d.hyperbolicSine = d.sinh = function() {
    var e, r, t, n, i = this, o = i.constructor;
    if (!i.isFinite() || i.isZero())
      return new o(i);
    if (r = o.precision, t = o.rounding, o.precision = r + Math.max(i.e, i.sd()) + 4, o.rounding = 1, n = i.d.length, n < 3)
      i = Pr(o, 2, i, i, true);
    else {
      e = 1.4 * Math.sqrt(n), e = e > 16 ? 16 : e | 0, i = i.times(1 / tn(5, e)), i = Pr(o, 2, i, i, true);
      for (var s, a = new o(5), l = new o(16), u = new o(20);e--; )
        s = i.times(i), i = i.times(a.plus(s.times(l.times(s).plus(u))));
    }
    return o.precision = r, o.rounding = t, y(i, r, t, true);
  };
  d.hyperbolicTangent = d.tanh = function() {
    var e, r, t = this, n = t.constructor;
    return t.isFinite() ? t.isZero() ? new n(t) : (e = n.precision, r = n.rounding, n.precision = e + 7, n.rounding = 1, O(t.sinh(), t.cosh(), n.precision = e, n.rounding = r)) : new n(t.s);
  };
  d.inverseCosine = d.acos = function() {
    var e, r = this, t = r.constructor, n = r.abs().cmp(1), i = t.precision, o = t.rounding;
    return n !== -1 ? n === 0 ? r.isNeg() ? ge(t, i, o) : new t(0) : new t(NaN) : r.isZero() ? ge(t, i + 4, o).times(0.5) : (t.precision = i + 6, t.rounding = 1, r = r.asin(), e = ge(t, i + 4, o).times(0.5), t.precision = i, t.rounding = o, e.minus(r));
  };
  d.inverseHyperbolicCosine = d.acosh = function() {
    var e, r, t = this, n = t.constructor;
    return t.lte(1) ? new n(t.eq(1) ? 0 : NaN) : t.isFinite() ? (e = n.precision, r = n.rounding, n.precision = e + Math.max(Math.abs(t.e), t.sd()) + 4, n.rounding = 1, x = false, t = t.times(t).minus(1).sqrt().plus(t), x = true, n.precision = e, n.rounding = r, t.ln()) : new n(t);
  };
  d.inverseHyperbolicSine = d.asinh = function() {
    var e, r, t = this, n = t.constructor;
    return !t.isFinite() || t.isZero() ? new n(t) : (e = n.precision, r = n.rounding, n.precision = e + 2 * Math.max(Math.abs(t.e), t.sd()) + 6, n.rounding = 1, x = false, t = t.times(t).plus(1).sqrt().plus(t), x = true, n.precision = e, n.rounding = r, t.ln());
  };
  d.inverseHyperbolicTangent = d.atanh = function() {
    var e, r, t, n, i = this, o = i.constructor;
    return i.isFinite() ? i.e >= 0 ? new o(i.abs().eq(1) ? i.s / 0 : i.isZero() ? i : NaN) : (e = o.precision, r = o.rounding, n = i.sd(), Math.max(n, e) < 2 * -i.e - 1 ? y(new o(i), e, r, true) : (o.precision = t = n - i.e, i = O(i.plus(1), new o(1).minus(i), t + e, 1), o.precision = e + 4, o.rounding = 1, i = i.ln(), o.precision = e, o.rounding = r, i.times(0.5))) : new o(NaN);
  };
  d.inverseSine = d.asin = function() {
    var e, r, t, n, i = this, o = i.constructor;
    return i.isZero() ? new o(i) : (r = i.abs().cmp(1), t = o.precision, n = o.rounding, r !== -1 ? r === 0 ? (e = ge(o, t + 4, n).times(0.5), e.s = i.s, e) : new o(NaN) : (o.precision = t + 6, o.rounding = 1, i = i.div(new o(1).minus(i.times(i)).sqrt().plus(1)).atan(), o.precision = t, o.rounding = n, i.times(2)));
  };
  d.inverseTangent = d.atan = function() {
    var e, r, t, n, i, o, s, a, l, u = this, c = u.constructor, p = c.precision, m = c.rounding;
    if (u.isFinite()) {
      if (u.isZero())
        return new c(u);
      if (u.abs().eq(1) && p + 4 <= xi)
        return s = ge(c, p + 4, m).times(0.25), s.s = u.s, s;
    } else {
      if (!u.s)
        return new c(NaN);
      if (p + 4 <= xi)
        return s = ge(c, p + 4, m).times(0.5), s.s = u.s, s;
    }
    for (c.precision = a = p + 10, c.rounding = 1, t = Math.min(28, a / b + 2 | 0), e = t;e; --e)
      u = u.div(u.times(u).plus(1).sqrt().plus(1));
    for (x = false, r = Math.ceil(a / b), n = 1, l = u.times(u), s = new c(u), i = u;e !== -1; )
      if (i = i.times(l), o = s.minus(i.div(n += 2)), i = i.times(l), s = o.plus(i.div(n += 2)), s.d[r] !== undefined)
        for (e = r;s.d[e] === o.d[e] && e--; )
          ;
    return t && (s = s.times(2 << t - 1)), x = true, y(s, c.precision = p, c.rounding = m, true);
  };
  d.isFinite = function() {
    return !!this.d;
  };
  d.isInteger = d.isInt = function() {
    return !!this.d && re(this.e / b) > this.d.length - 2;
  };
  d.isNaN = function() {
    return !this.s;
  };
  d.isNegative = d.isNeg = function() {
    return this.s < 0;
  };
  d.isPositive = d.isPos = function() {
    return this.s > 0;
  };
  d.isZero = function() {
    return !!this.d && this.d[0] === 0;
  };
  d.lessThan = d.lt = function(e) {
    return this.cmp(e) < 0;
  };
  d.lessThanOrEqualTo = d.lte = function(e) {
    return this.cmp(e) < 1;
  };
  d.logarithm = d.log = function(e) {
    var r, t, n, i, o, s, a, l, u = this, c = u.constructor, p = c.precision, m = c.rounding, f = 5;
    if (e == null)
      e = new c(10), r = true;
    else {
      if (e = new c(e), t = e.d, e.s < 0 || !t || !t[0] || e.eq(1))
        return new c(NaN);
      r = e.eq(10);
    }
    if (t = u.d, u.s < 0 || !t || !t[0] || u.eq(1))
      return new c(t && !t[0] ? -1 / 0 : u.s != 1 ? NaN : t ? 0 : 1 / 0);
    if (r)
      if (t.length > 1)
        o = true;
      else {
        for (i = t[0];i % 10 === 0; )
          i /= 10;
        o = i !== 1;
      }
    if (x = false, a = p + f, s = Qe(u, a), n = r ? Xt(c, a + 10) : Qe(e, a), l = O(s, n, a, 1), it(l.d, i = p, m))
      do
        if (a += 10, s = Qe(u, a), n = r ? Xt(c, a + 10) : Qe(e, a), l = O(s, n, a, 1), !o) {
          +z(l.d).slice(i + 1, i + 15) + 1 == 100000000000000 && (l = y(l, p + 1, 0));
          break;
        }
      while (it(l.d, i += 10, m));
    return x = true, y(l, p, m);
  };
  d.minus = d.sub = function(e) {
    var r, t, n, i, o, s, a, l, u, c, p, m, f = this, g = f.constructor;
    if (e = new g(e), !f.d || !e.d)
      return !f.s || !e.s ? e = new g(NaN) : f.d ? e.s = -e.s : e = new g(e.d || f.s !== e.s ? f : NaN), e;
    if (f.s != e.s)
      return e.s = -e.s, f.plus(e);
    if (u = f.d, m = e.d, a = g.precision, l = g.rounding, !u[0] || !m[0]) {
      if (m[0])
        e.s = -e.s;
      else if (u[0])
        e = new g(f);
      else
        return new g(l === 3 ? -0 : 0);
      return x ? y(e, a, l) : e;
    }
    if (t = re(e.e / b), c = re(f.e / b), u = u.slice(), o = c - t, o) {
      for (p = o < 0, p ? (r = u, o = -o, s = m.length) : (r = m, t = c, s = u.length), n = Math.max(Math.ceil(a / b), s) + 2, o > n && (o = n, r.length = 1), r.reverse(), n = o;n--; )
        r.push(0);
      r.reverse();
    } else {
      for (n = u.length, s = m.length, p = n < s, p && (s = n), n = 0;n < s; n++)
        if (u[n] != m[n]) {
          p = u[n] < m[n];
          break;
        }
      o = 0;
    }
    for (p && (r = u, u = m, m = r, e.s = -e.s), s = u.length, n = m.length - s;n > 0; --n)
      u[s++] = 0;
    for (n = m.length;n > o; ) {
      if (u[--n] < m[n]) {
        for (i = n;i && u[--i] === 0; )
          u[i] = he - 1;
        --u[i], u[n] += he;
      }
      u[n] -= m[n];
    }
    for (;u[--s] === 0; )
      u.pop();
    for (;u[0] === 0; u.shift())
      --t;
    return u[0] ? (e.d = u, e.e = rn(u, t), x ? y(e, a, l) : e) : new g(l === 3 ? -0 : 0);
  };
  d.modulo = d.mod = function(e) {
    var r, t = this, n = t.constructor;
    return e = new n(e), !t.d || !e.s || e.d && !e.d[0] ? new n(NaN) : !e.d || t.d && !t.d[0] ? y(new n(t), n.precision, n.rounding) : (x = false, n.modulo == 9 ? (r = O(t, e.abs(), 0, 3, 1), r.s *= e.s) : r = O(t, e, 0, n.modulo, 1), r = r.times(e), x = true, t.minus(r));
  };
  d.naturalExponential = d.exp = function() {
    return Pi(this);
  };
  d.naturalLogarithm = d.ln = function() {
    return Qe(this);
  };
  d.negated = d.neg = function() {
    var e = new this.constructor(this);
    return e.s = -e.s, y(e);
  };
  d.plus = d.add = function(e) {
    var r, t, n, i, o, s, a, l, u, c, p = this, m = p.constructor;
    if (e = new m(e), !p.d || !e.d)
      return !p.s || !e.s ? e = new m(NaN) : p.d || (e = new m(e.d || p.s === e.s ? p : NaN)), e;
    if (p.s != e.s)
      return e.s = -e.s, p.minus(e);
    if (u = p.d, c = e.d, a = m.precision, l = m.rounding, !u[0] || !c[0])
      return c[0] || (e = new m(p)), x ? y(e, a, l) : e;
    if (o = re(p.e / b), n = re(e.e / b), u = u.slice(), i = o - n, i) {
      for (i < 0 ? (t = u, i = -i, s = c.length) : (t = c, n = o, s = u.length), o = Math.ceil(a / b), s = o > s ? o + 1 : s + 1, i > s && (i = s, t.length = 1), t.reverse();i--; )
        t.push(0);
      t.reverse();
    }
    for (s = u.length, i = c.length, s - i < 0 && (i = s, t = c, c = u, u = t), r = 0;i; )
      r = (u[--i] = u[i] + c[i] + r) / he | 0, u[i] %= he;
    for (r && (u.unshift(r), ++n), s = u.length;u[--s] == 0; )
      u.pop();
    return e.d = u, e.e = rn(u, n), x ? y(e, a, l) : e;
  };
  d.precision = d.sd = function(e) {
    var r, t = this;
    if (e !== undefined && e !== !!e && e !== 1 && e !== 0)
      throw Error(Ge + e);
    return t.d ? (r = ws(t.d), e && t.e + 1 > r && (r = t.e + 1)) : r = NaN, r;
  };
  d.round = function() {
    var e = this, r = e.constructor;
    return y(new r(e), e.e + 1, r.rounding);
  };
  d.sine = d.sin = function() {
    var e, r, t = this, n = t.constructor;
    return t.isFinite() ? t.isZero() ? new n(t) : (e = n.precision, r = n.rounding, n.precision = e + Math.max(t.e, t.sd()) + b, n.rounding = 1, t = cc(n, Ts(n, t)), n.precision = e, n.rounding = r, y(Oe > 2 ? t.neg() : t, e, r, true)) : new n(NaN);
  };
  d.squareRoot = d.sqrt = function() {
    var e, r, t, n, i, o, s = this, a = s.d, l = s.e, u = s.s, c = s.constructor;
    if (u !== 1 || !a || !a[0])
      return new c(!u || u < 0 && (!a || a[0]) ? NaN : a ? s : 1 / 0);
    for (x = false, u = Math.sqrt(+s), u == 0 || u == 1 / 0 ? (r = z(a), (r.length + l) % 2 == 0 && (r += "0"), u = Math.sqrt(r), l = re((l + 1) / 2) - (l < 0 || l % 2), u == 1 / 0 ? r = "5e" + l : (r = u.toExponential(), r = r.slice(0, r.indexOf("e") + 1) + l), n = new c(r)) : n = new c(u.toString()), t = (l = c.precision) + 3;; )
      if (o = n, n = o.plus(O(s, o, t + 2, 1)).times(0.5), z(o.d).slice(0, t) === (r = z(n.d)).slice(0, t))
        if (r = r.slice(t - 3, t + 1), r == "9999" || !i && r == "4999") {
          if (!i && (y(o, l + 1, 0), o.times(o).eq(s))) {
            n = o;
            break;
          }
          t += 4, i = 1;
        } else {
          (!+r || !+r.slice(1) && r.charAt(0) == "5") && (y(n, l + 1, 1), e = !n.times(n).eq(s));
          break;
        }
    return x = true, y(n, l, c.rounding, e);
  };
  d.tangent = d.tan = function() {
    var e, r, t = this, n = t.constructor;
    return t.isFinite() ? t.isZero() ? new n(t) : (e = n.precision, r = n.rounding, n.precision = e + 10, n.rounding = 1, t = t.sin(), t.s = 1, t = O(t, new n(1).minus(t.times(t)).sqrt(), e + 10, 0), n.precision = e, n.rounding = r, y(Oe == 2 || Oe == 4 ? t.neg() : t, e, r, true)) : new n(NaN);
  };
  d.times = d.mul = function(e) {
    var r, t, n, i, o, s, a, l, u, c = this, p = c.constructor, m = c.d, f = (e = new p(e)).d;
    if (e.s *= c.s, !m || !m[0] || !f || !f[0])
      return new p(!e.s || m && !m[0] && !f || f && !f[0] && !m ? NaN : !m || !f ? e.s / 0 : e.s * 0);
    for (t = re(c.e / b) + re(e.e / b), l = m.length, u = f.length, l < u && (o = m, m = f, f = o, s = l, l = u, u = s), o = [], s = l + u, n = s;n--; )
      o.push(0);
    for (n = u;--n >= 0; ) {
      for (r = 0, i = l + n;i > n; )
        a = o[i] + f[n] * m[i - n - 1] + r, o[i--] = a % he | 0, r = a / he | 0;
      o[i] = (o[i] + r) % he | 0;
    }
    for (;!o[--s]; )
      o.pop();
    return r ? ++t : o.shift(), e.d = o, e.e = rn(o, t), x ? y(e, p.precision, p.rounding) : e;
  };
  d.toBinary = function(e, r) {
    return Ti(this, 2, e, r);
  };
  d.toDecimalPlaces = d.toDP = function(e, r) {
    var t = this, n = t.constructor;
    return t = new n(t), e === undefined ? t : (se(e, 0, Je), r === undefined ? r = n.rounding : se(r, 0, 8), y(t, e + t.e + 1, r));
  };
  d.toExponential = function(e, r) {
    var t, n = this, i = n.constructor;
    return e === undefined ? t = ve(n, true) : (se(e, 0, Je), r === undefined ? r = i.rounding : se(r, 0, 8), n = y(new i(n), e + 1, r), t = ve(n, true, e + 1)), n.isNeg() && !n.isZero() ? "-" + t : t;
  };
  d.toFixed = function(e, r) {
    var t, n, i = this, o = i.constructor;
    return e === undefined ? t = ve(i) : (se(e, 0, Je), r === undefined ? r = o.rounding : se(r, 0, 8), n = y(new o(i), e + i.e + 1, r), t = ve(n, false, e + n.e + 1)), i.isNeg() && !i.isZero() ? "-" + t : t;
  };
  d.toFraction = function(e) {
    var r, t, n, i, o, s, a, l, u, c, p, m, f = this, g = f.d, h = f.constructor;
    if (!g)
      return new h(f);
    if (u = t = new h(1), n = l = new h(0), r = new h(n), o = r.e = ws(g) - f.e - 1, s = o % b, r.d[0] = Q(10, s < 0 ? b + s : s), e == null)
      e = o > 0 ? r : u;
    else {
      if (a = new h(e), !a.isInt() || a.lt(u))
        throw Error(Ge + a);
      e = a.gt(r) ? o > 0 ? r : u : a;
    }
    for (x = false, a = new h(z(g)), c = h.precision, h.precision = o = g.length * b * 2;p = O(a, r, 0, 1, 1), i = t.plus(p.times(n)), i.cmp(e) != 1; )
      t = n, n = i, i = u, u = l.plus(p.times(i)), l = i, i = r, r = a.minus(p.times(i)), a = i;
    return i = O(e.minus(t), n, 0, 1, 1), l = l.plus(i.times(u)), t = t.plus(i.times(n)), l.s = u.s = f.s, m = O(u, n, o, 1).minus(f).abs().cmp(O(l, t, o, 1).minus(f).abs()) < 1 ? [u, n] : [l, t], h.precision = c, x = true, m;
  };
  d.toHexadecimal = d.toHex = function(e, r) {
    return Ti(this, 16, e, r);
  };
  d.toNearest = function(e, r) {
    var t = this, n = t.constructor;
    if (t = new n(t), e == null) {
      if (!t.d)
        return t;
      e = new n(1), r = n.rounding;
    } else {
      if (e = new n(e), r === undefined ? r = n.rounding : se(r, 0, 8), !t.d)
        return e.s ? t : e;
      if (!e.d)
        return e.s && (e.s = t.s), e;
    }
    return e.d[0] ? (x = false, t = O(t, e, 0, r, 1).times(e), x = true, y(t)) : (e.s = t.s, t = e), t;
  };
  d.toNumber = function() {
    return +this;
  };
  d.toOctal = function(e, r) {
    return Ti(this, 8, e, r);
  };
  d.toPower = d.pow = function(e) {
    var r, t, n, i, o, s, a = this, l = a.constructor, u = +(e = new l(e));
    if (!a.d || !e.d || !a.d[0] || !e.d[0])
      return new l(Q(+a, u));
    if (a = new l(a), a.eq(1))
      return a;
    if (n = l.precision, o = l.rounding, e.eq(1))
      return y(a, n, o);
    if (r = re(e.e / b), r >= e.d.length - 1 && (t = u < 0 ? -u : u) <= sc)
      return i = xs(l, a, t, n), e.s < 0 ? new l(1).div(i) : y(i, n, o);
    if (s = a.s, s < 0) {
      if (r < e.d.length - 1)
        return new l(NaN);
      if (e.d[r] & 1 || (s = 1), a.e == 0 && a.d[0] == 1 && a.d.length == 1)
        return a.s = s, a;
    }
    return t = Q(+a, u), r = t == 0 || !isFinite(t) ? re(u * (Math.log("0." + z(a.d)) / Math.LN10 + a.e + 1)) : new l(t + "").e, r > l.maxE + 1 || r < l.minE - 1 ? new l(r > 0 ? s / 0 : 0) : (x = false, l.rounding = a.s = 1, t = Math.min(12, (r + "").length), i = Pi(e.times(Qe(a, n + t)), n), i.d && (i = y(i, n + 5, 1), it(i.d, n, o) && (r = n + 10, i = y(Pi(e.times(Qe(a, r + t)), r), r + 5, 1), +z(i.d).slice(n + 1, n + 15) + 1 == 100000000000000 && (i = y(i, n + 1, 0)))), i.s = s, x = true, l.rounding = o, y(i, n, o));
  };
  d.toPrecision = function(e, r) {
    var t, n = this, i = n.constructor;
    return e === undefined ? t = ve(n, n.e <= i.toExpNeg || n.e >= i.toExpPos) : (se(e, 1, Je), r === undefined ? r = i.rounding : se(r, 0, 8), n = y(new i(n), e, r), t = ve(n, e <= n.e || n.e <= i.toExpNeg, e)), n.isNeg() && !n.isZero() ? "-" + t : t;
  };
  d.toSignificantDigits = d.toSD = function(e, r) {
    var t = this, n = t.constructor;
    return e === undefined ? (e = n.precision, r = n.rounding) : (se(e, 1, Je), r === undefined ? r = n.rounding : se(r, 0, 8)), y(new n(t), e, r);
  };
  d.toString = function() {
    var e = this, r = e.constructor, t = ve(e, e.e <= r.toExpNeg || e.e >= r.toExpPos);
    return e.isNeg() && !e.isZero() ? "-" + t : t;
  };
  d.truncated = d.trunc = function() {
    return y(new this.constructor(this), this.e + 1, 1);
  };
  d.valueOf = d.toJSON = function() {
    var e = this, r = e.constructor, t = ve(e, e.e <= r.toExpNeg || e.e >= r.toExpPos);
    return e.isNeg() ? "-" + t : t;
  };
  var O = function() {
    function e(n, i, o) {
      var s, a = 0, l = n.length;
      for (n = n.slice();l--; )
        s = n[l] * i + a, n[l] = s % o | 0, a = s / o | 0;
      return a && n.unshift(a), n;
    }
    function r(n, i, o, s) {
      var a, l;
      if (o != s)
        l = o > s ? 1 : -1;
      else
        for (a = l = 0;a < o; a++)
          if (n[a] != i[a]) {
            l = n[a] > i[a] ? 1 : -1;
            break;
          }
      return l;
    }
    function t(n, i, o, s) {
      for (var a = 0;o--; )
        n[o] -= a, a = n[o] < i[o] ? 1 : 0, n[o] = a * s + n[o] - i[o];
      for (;!n[0] && n.length > 1; )
        n.shift();
    }
    return function(n, i, o, s, a, l) {
      var u, c, p, m, f, g, h, A, T, C, E, I, me, le, Nr, U, ne, Ae, Y, pr, Rt = n.constructor, kn = n.s == i.s ? 1 : -1, Z = n.d, L = i.d;
      if (!Z || !Z[0] || !L || !L[0])
        return new Rt(!n.s || !i.s || (Z ? L && Z[0] == L[0] : !L) ? NaN : Z && Z[0] == 0 || !L ? kn * 0 : kn / 0);
      for (l ? (f = 1, c = n.e - i.e) : (l = he, f = b, c = re(n.e / f) - re(i.e / f)), Y = L.length, ne = Z.length, T = new Rt(kn), C = T.d = [], p = 0;L[p] == (Z[p] || 0); p++)
        ;
      if (L[p] > (Z[p] || 0) && c--, o == null ? (le = o = Rt.precision, s = Rt.rounding) : a ? le = o + (n.e - i.e) + 1 : le = o, le < 0)
        C.push(1), g = true;
      else {
        if (le = le / f + 2 | 0, p = 0, Y == 1) {
          for (m = 0, L = L[0], le++;(p < ne || m) && le--; p++)
            Nr = m * l + (Z[p] || 0), C[p] = Nr / L | 0, m = Nr % L | 0;
          g = m || p < ne;
        } else {
          for (m = l / (L[0] + 1) | 0, m > 1 && (L = e(L, m, l), Z = e(Z, m, l), Y = L.length, ne = Z.length), U = Y, E = Z.slice(0, Y), I = E.length;I < Y; )
            E[I++] = 0;
          pr = L.slice(), pr.unshift(0), Ae = L[0], L[1] >= l / 2 && ++Ae;
          do
            m = 0, u = r(L, E, Y, I), u < 0 ? (me = E[0], Y != I && (me = me * l + (E[1] || 0)), m = me / Ae | 0, m > 1 ? (m >= l && (m = l - 1), h = e(L, m, l), A = h.length, I = E.length, u = r(h, E, A, I), u == 1 && (m--, t(h, Y < A ? pr : L, A, l))) : (m == 0 && (u = m = 1), h = L.slice()), A = h.length, A < I && h.unshift(0), t(E, h, I, l), u == -1 && (I = E.length, u = r(L, E, Y, I), u < 1 && (m++, t(E, Y < I ? pr : L, I, l))), I = E.length) : u === 0 && (m++, E = [0]), C[p++] = m, u && E[0] ? E[I++] = Z[U] || 0 : (E = [Z[U]], I = 1);
          while ((U++ < ne || E[0] !== undefined) && le--);
          g = E[0] !== undefined;
        }
        C[0] || C.shift();
      }
      if (f == 1)
        T.e = c, gs = g;
      else {
        for (p = 1, m = C[0];m >= 10; m /= 10)
          p++;
        T.e = p + c * f - 1, y(T, a ? o + T.e + 1 : o, s, g);
      }
      return T;
    };
  }();
  d[Symbol.for("nodejs.util.inspect.custom")] = d.toString;
  d[Symbol.toStringTag] = "Decimal";
  var or = d.constructor = Cs(wi);
  Yt = new or(Yt);
  Zt = new or(Zt);
  var Te = or;
  var ot = class {
    constructor(r, t, n, i, o) {
      this.modelName = r, this.name = t, this.typeName = n, this.isList = i, this.isEnum = o;
    }
    _toGraphQLInputType() {
      let r = this.isList ? "List" : "", t = this.isEnum ? "Enum" : "";
      return `${r}${t}${this.typeName}FieldRefInput<${this.modelName}>`;
    }
  };
  var nn = class {
    constructor(r) {
      this.value = r;
    }
    write(r) {
      r.write(this.value);
    }
    markAsError() {
      this.value.markAsError();
    }
  };
  var on = (e) => e;
  var sn = { bold: on, red: on, green: on, dim: on, enabled: false };
  var Ss = { bold: W, red: ce, green: $e, dim: Ie, enabled: true };
  var Cr = { write(e) {
    e.writeLine(",");
  } };
  var Ce = class {
    constructor(r) {
      this.contents = r;
      this.isUnderlined = false;
      this.color = (r2) => r2;
    }
    underline() {
      return this.isUnderlined = true, this;
    }
    setColor(r) {
      return this.color = r, this;
    }
    write(r) {
      let t = r.getCurrentLineLength();
      r.write(this.color(this.contents)), this.isUnderlined && r.afterNextNewline(() => {
        r.write(" ".repeat(t)).writeLine(this.color("~".repeat(this.contents.length)));
      });
    }
  };
  var He = class {
    constructor() {
      this.hasError = false;
    }
    markAsError() {
      return this.hasError = true, this;
    }
  };
  var Sr = class extends He {
    constructor() {
      super(...arguments);
      this.items = [];
    }
    addItem(t) {
      return this.items.push(new nn(t)), this;
    }
    getField(t) {
      return this.items[t];
    }
    getPrintWidth() {
      return this.items.length === 0 ? 2 : Math.max(...this.items.map((n) => n.value.getPrintWidth())) + 2;
    }
    write(t) {
      if (this.items.length === 0) {
        this.writeEmpty(t);
        return;
      }
      this.writeWithItems(t);
    }
    writeEmpty(t) {
      let n = new Ce("[]");
      this.hasError && n.setColor(t.context.colors.red).underline(), t.write(n);
    }
    writeWithItems(t) {
      let { colors: n } = t.context;
      t.writeLine("[").withIndent(() => t.writeJoined(Cr, this.items).newLine()).write("]"), this.hasError && t.afterNextNewline(() => {
        t.writeLine(n.red("~".repeat(this.getPrintWidth())));
      });
    }
  };
  var Rs = ": ";
  var an = class {
    constructor(r, t) {
      this.name = r;
      this.value = t;
      this.hasError = false;
    }
    markAsError() {
      this.hasError = true;
    }
    getPrintWidth() {
      return this.name.length + this.value.getPrintWidth() + Rs.length;
    }
    write(r) {
      let t = new Ce(this.name);
      this.hasError && t.underline().setColor(r.context.colors.red), r.write(t).write(Rs).write(this.value);
    }
  };
  var J = class e extends He {
    constructor() {
      super(...arguments);
      this.fields = {};
      this.suggestions = [];
    }
    addField(t) {
      this.fields[t.name] = t;
    }
    addSuggestion(t) {
      this.suggestions.push(t);
    }
    getField(t) {
      return this.fields[t];
    }
    getDeepField(t) {
      let [n, ...i] = t, o = this.getField(n);
      if (!o)
        return;
      let s = o;
      for (let a of i) {
        let l;
        if (s.value instanceof e ? l = s.value.getField(a) : s.value instanceof Sr && (l = s.value.getField(Number(a))), !l)
          return;
        s = l;
      }
      return s;
    }
    getDeepFieldValue(t) {
      return t.length === 0 ? this : this.getDeepField(t)?.value;
    }
    hasField(t) {
      return !!this.getField(t);
    }
    removeAllFields() {
      this.fields = {};
    }
    removeField(t) {
      delete this.fields[t];
    }
    getFields() {
      return this.fields;
    }
    isEmpty() {
      return Object.keys(this.fields).length === 0;
    }
    getFieldValue(t) {
      return this.getField(t)?.value;
    }
    getDeepSubSelectionValue(t) {
      let n = this;
      for (let i of t) {
        if (!(n instanceof e))
          return;
        let o = n.getSubSelectionValue(i);
        if (!o)
          return;
        n = o;
      }
      return n;
    }
    getDeepSelectionParent(t) {
      let n = this.getSelectionParent();
      if (!n)
        return;
      let i = n;
      for (let o of t) {
        let s = i.value.getFieldValue(o);
        if (!s || !(s instanceof e))
          return;
        let a = s.getSelectionParent();
        if (!a)
          return;
        i = a;
      }
      return i;
    }
    getSelectionParent() {
      let t = this.getField("select");
      if (t?.value instanceof e)
        return { kind: "select", value: t.value };
      let n = this.getField("include");
      if (n?.value instanceof e)
        return { kind: "include", value: n.value };
    }
    getSubSelectionValue(t) {
      return this.getSelectionParent()?.value.fields[t].value;
    }
    getPrintWidth() {
      let t = Object.values(this.fields);
      return t.length == 0 ? 2 : Math.max(...t.map((i) => i.getPrintWidth())) + 2;
    }
    write(t) {
      let n = Object.values(this.fields);
      if (n.length === 0 && this.suggestions.length === 0) {
        this.writeEmpty(t);
        return;
      }
      this.writeWithContents(t, n);
    }
    writeEmpty(t) {
      let n = new Ce("{}");
      this.hasError && n.setColor(t.context.colors.red).underline(), t.write(n);
    }
    writeWithContents(t, n) {
      t.writeLine("{").withIndent(() => {
        t.writeJoined(Cr, [...n, ...this.suggestions]).newLine();
      }), t.write("}"), this.hasError && t.afterNextNewline(() => {
        t.writeLine(t.context.colors.red("~".repeat(this.getPrintWidth())));
      });
    }
  };
  var H = class extends He {
    constructor(t) {
      super();
      this.text = t;
    }
    getPrintWidth() {
      return this.text.length;
    }
    write(t) {
      let n = new Ce(this.text);
      this.hasError && n.underline().setColor(t.context.colors.red), t.write(n);
    }
  };
  var Ci = class {
    constructor(r) {
      this.errorMessages = [];
      this.arguments = r;
    }
    write(r) {
      r.write(this.arguments);
    }
    addErrorMessage(r) {
      this.errorMessages.push(r);
    }
    renderAllMessages(r) {
      return this.errorMessages.map((t) => t(r)).join(`
`);
    }
  };
  var Zc = "P2037";
  var st = "<unknown>";
  var ep = /^\s*at (.*?) ?\(((?:file|https?|blob|chrome-extension|native|eval|webpack|<anonymous>|\/|[a-z]:\\|\\\\).*?)(?::(\d+))?(?::(\d+))?\)?\s*$/i;
  var rp = /\((\S*)(?::(\d+))(?::(\d+))\)/;
  var np = /^\s*at (?:((?:\[object object\])?.+) )?\(?((?:file|ms-appx|https?|webpack|blob):.*?):(\d+)(?::(\d+))?\)?\s*$/i;
  var op = /^\s*(.*?)(?:\((.*?)\))?(?:^|@)((?:file|https?|blob|chrome|webpack|resource|\[native).*?|[^@]*bundle)(?::(\d+))?(?::(\d+))?\s*$/i;
  var sp = /(\S+) line (\d+)(?: > eval line \d+)* > eval/i;
  var lp = /^\s*(?:([^@]*)(?:\((.*?)\))?@)?(\S.*?):(\d+)(?::(\d+))?\s*$/i;
  var cp = /^\s*at (?:((?:\[object object\])?[^\\/]+(?: \[as \S+\])?) )?\(?(.*?):(\d+)(?::(\d+))?\)?\s*$/i;
  var Si = class {
    getLocation() {
      return null;
    }
  };
  var Ri = class {
    constructor() {
      this._error = new Error;
    }
    getLocation() {
      let r = this._error.stack;
      if (!r)
        return null;
      let n = ks(r).find((i) => {
        if (!i.file)
          return false;
        let o = ni(i.file);
        return o !== "<anonymous>" && !o.includes("@prisma") && !o.includes("/packages/client/src/runtime/") && !o.endsWith("/runtime/binary.js") && !o.endsWith("/runtime/library.js") && !o.endsWith("/runtime/edge.js") && !o.endsWith("/runtime/edge-esm.js") && !o.startsWith("internal/") && !i.methodName.includes("new ") && !i.methodName.includes("getCallSite") && !i.methodName.includes("Proxy.") && i.methodName.split(".").length < 4;
      });
      return !n || !n.file ? null : { fileName: n.file, lineNumber: n.lineNumber, columnNumber: n.column };
    }
  };
  var Ds = { _avg: true, _count: true, _sum: true, _min: true, _max: true };
  var $s = (e) => Array.isArray(e) ? e : e.split(".");
  var Ai = (e, r) => $s(r).reduce((t, n) => t && t[n], e);
  var qs = (e, r, t) => $s(r).reduceRight((n, i, o, s) => Object.assign({}, Ai(e, s.slice(0, o)), { [i]: n }), t);
  var Gs = _(ii());
  var Qs = _(import.meta.require("fs"));
  var Bs = { keyword: _e, entity: _e, value: (e) => W(Ze(e)), punctuation: Ze, directive: _e, function: _e, variable: (e) => W(Ze(e)), string: (e) => W($e(e)), boolean: de, number: _e, comment: Fr };
  var xp = (e) => e;
  var cn = {};
  var Pp = 0;
  var P = { manual: cn.Prism && cn.Prism.manual, disableWorkerMessageHandler: cn.Prism && cn.Prism.disableWorkerMessageHandler, util: { encode: function(e) {
    if (e instanceof ye) {
      let r = e;
      return new ye(r.type, P.util.encode(r.content), r.alias);
    } else
      return Array.isArray(e) ? e.map(P.util.encode) : e.replace(/&/g, "&amp;").replace(/</g, "&lt;").replace(/\u00a0/g, " ");
  }, type: function(e) {
    return Object.prototype.toString.call(e).slice(8, -1);
  }, objId: function(e) {
    return e.__id || Object.defineProperty(e, "__id", { value: ++Pp }), e.__id;
  }, clone: function e(r, t) {
    let n, i, o = P.util.type(r);
    switch (t = t || {}, o) {
      case "Object":
        if (i = P.util.objId(r), t[i])
          return t[i];
        n = {}, t[i] = n;
        for (let s in r)
          r.hasOwnProperty(s) && (n[s] = e(r[s], t));
        return n;
      case "Array":
        return i = P.util.objId(r), t[i] ? t[i] : (n = [], t[i] = n, r.forEach(function(s, a) {
          n[a] = e(s, t);
        }), n);
      default:
        return r;
    }
  } }, languages: { extend: function(e, r) {
    let t = P.util.clone(P.languages[e]);
    for (let n in r)
      t[n] = r[n];
    return t;
  }, insertBefore: function(e, r, t, n) {
    n = n || P.languages;
    let i = n[e], o = {};
    for (let a in i)
      if (i.hasOwnProperty(a)) {
        if (a == r)
          for (let l in t)
            t.hasOwnProperty(l) && (o[l] = t[l]);
        t.hasOwnProperty(a) || (o[a] = i[a]);
      }
    let s = n[e];
    return n[e] = o, P.languages.DFS(P.languages, function(a, l) {
      l === s && a != e && (this[a] = o);
    }), o;
  }, DFS: function e(r, t, n, i) {
    i = i || {};
    let o = P.util.objId;
    for (let s in r)
      if (r.hasOwnProperty(s)) {
        t.call(r, s, r[s], n || s);
        let a = r[s], l = P.util.type(a);
        l === "Object" && !i[o(a)] ? (i[o(a)] = true, e(a, t, null, i)) : l === "Array" && !i[o(a)] && (i[o(a)] = true, e(a, t, s, i));
      }
  } }, plugins: {}, highlight: function(e, r, t) {
    let n = { code: e, grammar: r, language: t };
    return P.hooks.run("before-tokenize", n), n.tokens = P.tokenize(n.code, n.grammar), P.hooks.run("after-tokenize", n), ye.stringify(P.util.encode(n.tokens), n.language);
  }, matchGrammar: function(e, r, t, n, i, o, s) {
    for (let h in t) {
      if (!t.hasOwnProperty(h) || !t[h])
        continue;
      if (h == s)
        return;
      let A = t[h];
      A = P.util.type(A) === "Array" ? A : [A];
      for (let T = 0;T < A.length; ++T) {
        let C = A[T], E = C.inside, I = !!C.lookbehind, me = !!C.greedy, le = 0, Nr = C.alias;
        if (me && !C.pattern.global) {
          let U = C.pattern.toString().match(/[imuy]*$/)[0];
          C.pattern = RegExp(C.pattern.source, U + "g");
        }
        C = C.pattern || C;
        for (let U = n, ne = i;U < r.length; ne += r[U].length, ++U) {
          let Ae = r[U];
          if (r.length > e.length)
            return;
          if (Ae instanceof ye)
            continue;
          if (me && U != r.length - 1) {
            C.lastIndex = ne;
            var p = C.exec(e);
            if (!p)
              break;
            var c = p.index + (I ? p[1].length : 0), m = p.index + p[0].length, a = U, l = ne;
            for (let L = r.length;a < L && (l < m || !r[a].type && !r[a - 1].greedy); ++a)
              l += r[a].length, c >= l && (++U, ne = l);
            if (r[U] instanceof ye)
              continue;
            u = a - U, Ae = e.slice(ne, l), p.index -= ne;
          } else {
            C.lastIndex = 0;
            var p = C.exec(Ae), u = 1;
          }
          if (!p) {
            if (o)
              break;
            continue;
          }
          I && (le = p[1] ? p[1].length : 0);
          var c = p.index + le, p = p[0].slice(le), m = c + p.length, f = Ae.slice(0, c), g = Ae.slice(m);
          let Y = [U, u];
          f && (++U, ne += f.length, Y.push(f));
          let pr = new ye(h, E ? P.tokenize(p, E) : p, Nr, p, me);
          if (Y.push(pr), g && Y.push(g), Array.prototype.splice.apply(r, Y), u != 1 && P.matchGrammar(e, r, t, U, ne, true, h), o)
            break;
        }
      }
    }
  }, tokenize: function(e, r) {
    let t = [e], n = r.rest;
    if (n) {
      for (let i in n)
        r[i] = n[i];
      delete r.rest;
    }
    return P.matchGrammar(e, t, r, 0, 0, false), t;
  }, hooks: { all: {}, add: function(e, r) {
    let t = P.hooks.all;
    t[e] = t[e] || [], t[e].push(r);
  }, run: function(e, r) {
    let t = P.hooks.all[e];
    if (!(!t || !t.length))
      for (var n = 0, i;i = t[n++]; )
        i(r);
  } }, Token: ye };
  P.languages.clike = { comment: [{ pattern: /(^|[^\\])\/\*[\s\S]*?(?:\*\/|$)/, lookbehind: true }, { pattern: /(^|[^\\:])\/\/.*/, lookbehind: true, greedy: true }], string: { pattern: /(["'])(?:\\(?:\r\n|[\s\S])|(?!\1)[^\\\r\n])*\1/, greedy: true }, "class-name": { pattern: /((?:\b(?:class|interface|extends|implements|trait|instanceof|new)\s+)|(?:catch\s+\())[\w.\\]+/i, lookbehind: true, inside: { punctuation: /[.\\]/ } }, keyword: /\b(?:if|else|while|do|for|return|in|instanceof|function|new|try|throw|catch|finally|null|break|continue)\b/, boolean: /\b(?:true|false)\b/, function: /\w+(?=\()/, number: /\b0x[\da-f]+\b|(?:\b\d+\.?\d*|\B\.\d+)(?:e[+-]?\d+)?/i, operator: /--?|\+\+?|!=?=?|<=?|>=?|==?=?|&&?|\|\|?|\?|\*|\/|~|\^|%/, punctuation: /[{}[\];(),.:]/ };
  P.languages.javascript = P.languages.extend("clike", { "class-name": [P.languages.clike["class-name"], { pattern: /(^|[^$\w\xA0-\uFFFF])[_$A-Z\xA0-\uFFFF][$\w\xA0-\uFFFF]*(?=\.(?:prototype|constructor))/, lookbehind: true }], keyword: [{ pattern: /((?:^|})\s*)(?:catch|finally)\b/, lookbehind: true }, { pattern: /(^|[^.])\b(?:as|async(?=\s*(?:function\b|\(|[$\w\xA0-\uFFFF]|$))|await|break|case|class|const|continue|debugger|default|delete|do|else|enum|export|extends|for|from|function|get|if|implements|import|in|instanceof|interface|let|new|null|of|package|private|protected|public|return|set|static|super|switch|this|throw|try|typeof|undefined|var|void|while|with|yield)\b/, lookbehind: true }], number: /\b(?:(?:0[xX](?:[\dA-Fa-f](?:_[\dA-Fa-f])?)+|0[bB](?:[01](?:_[01])?)+|0[oO](?:[0-7](?:_[0-7])?)+)n?|(?:\d(?:_\d)?)+n|NaN|Infinity)\b|(?:\b(?:\d(?:_\d)?)+\.?(?:\d(?:_\d)?)*|\B\.(?:\d(?:_\d)?)+)(?:[Ee][+-]?(?:\d(?:_\d)?)+)?/, function: /[_$a-zA-Z\xA0-\uFFFF][$\w\xA0-\uFFFF]*(?=\s*(?:\.\s*(?:apply|bind|call)\s*)?\()/, operator: /-[-=]?|\+[+=]?|!=?=?|<<?=?|>>?>?=?|=(?:==?|>)?|&[&=]?|\|[|=]?|\*\*?=?|\/=?|~|\^=?|%=?|\?|\.{3}/ });
  P.languages.javascript["class-name"][0].pattern = /(\b(?:class|interface|extends|implements|instanceof|new)\s+)[\w.\\]+/;
  P.languages.insertBefore("javascript", "keyword", { regex: { pattern: /((?:^|[^$\w\xA0-\uFFFF."'\])\s])\s*)\/(\[(?:[^\]\\\r\n]|\\.)*]|\\.|[^/\\\[\r\n])+\/[gimyus]{0,6}(?=\s*($|[\r\n,.;})\]]))/, lookbehind: true, greedy: true }, "function-variable": { pattern: /[_$a-zA-Z\xA0-\uFFFF][$\w\xA0-\uFFFF]*(?=\s*[=:]\s*(?:async\s*)?(?:\bfunction\b|(?:\((?:[^()]|\([^()]*\))*\)|[_$a-zA-Z\xA0-\uFFFF][$\w\xA0-\uFFFF]*)\s*=>))/, alias: "function" }, parameter: [{ pattern: /(function(?:\s+[_$A-Za-z\xA0-\uFFFF][$\w\xA0-\uFFFF]*)?\s*\(\s*)(?!\s)(?:[^()]|\([^()]*\))+?(?=\s*\))/, lookbehind: true, inside: P.languages.javascript }, { pattern: /[_$a-z\xA0-\uFFFF][$\w\xA0-\uFFFF]*(?=\s*=>)/i, inside: P.languages.javascript }, { pattern: /(\(\s*)(?!\s)(?:[^()]|\([^()]*\))+?(?=\s*\)\s*=>)/, lookbehind: true, inside: P.languages.javascript }, { pattern: /((?:\b|\s|^)(?!(?:as|async|await|break|case|catch|class|const|continue|debugger|default|delete|do|else|enum|export|extends|finally|for|from|function|get|if|implements|import|in|instanceof|interface|let|new|null|of|package|private|protected|public|return|set|static|super|switch|this|throw|try|typeof|undefined|var|void|while|with|yield)(?![$\w\xA0-\uFFFF]))(?:[_$A-Za-z\xA0-\uFFFF][$\w\xA0-\uFFFF]*\s*)\(\s*)(?!\s)(?:[^()]|\([^()]*\))+?(?=\s*\)\s*\{)/, lookbehind: true, inside: P.languages.javascript }], constant: /\b[A-Z](?:[A-Z_]|\dx?)*\b/ });
  P.languages.markup && P.languages.markup.tag.addInlined("script", "javascript");
  P.languages.js = P.languages.javascript;
  P.languages.typescript = P.languages.extend("javascript", { keyword: /\b(?:abstract|as|async|await|break|case|catch|class|const|constructor|continue|debugger|declare|default|delete|do|else|enum|export|extends|finally|for|from|function|get|if|implements|import|in|instanceof|interface|is|keyof|let|module|namespace|new|null|of|package|private|protected|public|readonly|return|require|set|static|super|switch|this|throw|try|type|typeof|var|void|while|with|yield)\b/, builtin: /\b(?:string|Function|any|number|boolean|Array|symbol|console|Promise|unknown|never)\b/ });
  P.languages.ts = P.languages.typescript;
  ye.stringify = function(e, r) {
    return typeof e == "string" ? e : Array.isArray(e) ? e.map(function(t) {
      return ye.stringify(t, r);
    }).join("") : vp(e.type)(e.content);
  };
  var js = _(Wo());
  var pn = class e {
    static read(r) {
      let t;
      try {
        t = Qs.default.readFileSync(r, "utf-8");
      } catch {
        return null;
      }
      return e.fromContent(t);
    }
    static fromContent(r) {
      let t = r.split(/\r?\n/);
      return new e(1, t);
    }
    constructor(r, t) {
      this.firstLineNumber = r, this.lines = t;
    }
    get lastLineNumber() {
      return this.firstLineNumber + this.lines.length - 1;
    }
    mapLineAt(r, t) {
      if (r < this.firstLineNumber || r > this.lines.length + this.firstLineNumber)
        return this;
      let n = r - this.firstLineNumber, i = [...this.lines];
      return i[n] = t(i[n]), new e(this.firstLineNumber, i);
    }
    mapLines(r) {
      return new e(this.firstLineNumber, this.lines.map((t, n) => r(t, this.firstLineNumber + n)));
    }
    lineAt(r) {
      return this.lines[r - this.firstLineNumber];
    }
    prependSymbolAt(r, t) {
      return this.mapLines((n, i) => i === r ? `${t} ${n}` : `  ${n}`);
    }
    slice(r, t) {
      let n = this.lines.slice(r - 1, t).join(`
`);
      return new e(r, Us(n).split(`
`));
    }
    highlight() {
      let r = Vs(this.toString());
      return new e(this.firstLineNumber, r.split(`
`));
    }
    toString() {
      return this.lines.join(`
`);
    }
  };
  var Cp = { red: ce, gray: Fr, dim: Ie, bold: W, underline: ee, highlightSource: (e) => e.highlight() };
  var Sp = { red: (e) => e, gray: (e) => e, dim: (e) => e, bold: (e) => e, underline: (e) => e, highlightSource: (e) => e };
  var Np = ["findUnique", "findUniqueOrThrow", "findFirst", "findFirstOrThrow", "create", "update", "upsert", "delete"];
  var Op = ["aggregate", "count", "groupBy"];
  var ki = Symbol();
  var ea = (e) => e;
  var dn = class {
    constructor(r, t) {
      this.extension = r;
      this.previous = t;
      this.computedFieldsCache = new xe;
      this.modelExtensionsCache = new xe;
      this.queryCallbacksCache = new xe;
      this.clientExtensions = zr(() => this.extension.client ? { ...this.previous?.getAllClientExtensions(), ...this.extension.client } : this.previous?.getAllClientExtensions());
      this.batchCallbacks = zr(() => {
        let r2 = this.previous?.getAllBatchQueryCallbacks() ?? [], t2 = this.extension.query?.$__internalBatch;
        return t2 ? r2.concat(t2) : r2;
      });
    }
    getAllComputedFields(r) {
      return this.computedFieldsCache.getOrCreate(r, () => aa(this.previous?.getAllComputedFields(r), this.extension, r));
    }
    getAllClientExtensions() {
      return this.clientExtensions.get();
    }
    getAllModelExtensions(r) {
      return this.modelExtensionsCache.getOrCreate(r, () => {
        let t = Se(r);
        return !this.extension.model || !(this.extension.model[t] || this.extension.model.$allModels) ? this.previous?.getAllModelExtensions(r) : { ...this.previous?.getAllModelExtensions(r), ...this.extension.model.$allModels, ...this.extension.model[t] };
      });
    }
    getAllQueryCallbacks(r, t) {
      return this.queryCallbacksCache.getOrCreate(`${r}:${t}`, () => {
        let n = this.previous?.getAllQueryCallbacks(r, t) ?? [], i = [], o = this.extension.query;
        return !o || !(o[r] || o.$allModels || o[t] || o.$allOperations) ? n : (o[r] !== undefined && (o[r][t] !== undefined && i.push(o[r][t]), o[r].$allOperations !== undefined && i.push(o[r].$allOperations)), r !== "$none" && o.$allModels !== undefined && (o.$allModels[t] !== undefined && i.push(o.$allModels[t]), o.$allModels.$allOperations !== undefined && i.push(o.$allModels.$allOperations)), o[t] !== undefined && i.push(o[t]), o.$allOperations !== undefined && i.push(o.$allOperations), n.concat(i));
      });
    }
    getAllBatchQueryCallbacks() {
      return this.batchCallbacks.get();
    }
  };
  var fn = class e {
    constructor(r) {
      this.head = r;
    }
    static empty() {
      return new e;
    }
    static single(r) {
      return new e(new dn(r));
    }
    isEmpty() {
      return this.head === undefined;
    }
    append(r) {
      return new e(new dn(r, this.head));
    }
    getAllComputedFields(r) {
      return this.head?.getAllComputedFields(r);
    }
    getAllClientExtensions() {
      return this.head?.getAllClientExtensions();
    }
    getAllModelExtensions(r) {
      return this.head?.getAllModelExtensions(r);
    }
    getAllQueryCallbacks(r, t) {
      return this.head?.getAllQueryCallbacks(r, t) ?? [];
    }
    getAllBatchQueryCallbacks() {
      return this.head?.getAllBatchQueryCallbacks() ?? [];
    }
  };
  var ua = N("prisma:client");
  var ca = { Vercel: "vercel", "Netlify CI": "netlify" };
  var Gp = "Cloudflare-Workers";
  var Jp = "node";
  var ya = _(import.meta.require("fs"));
  var ut = _(import.meta.require("path"));
  var Wp = N("prisma:client:engines:resolveEnginePath");
  var Kp = () => new RegExp("runtime[\\\\/]library\\.m?js$");
  var Di = _(li());
  var Pa = _(ns());
  var bn = class extends Error {
    constructor(r, t) {
      super(r), this.clientVersion = t.clientVersion, this.cause = t.cause;
    }
    get [Symbol.toStringTag]() {
      return this.name;
    }
  };
  var ae = class extends bn {
    constructor(r, t) {
      super(r, t), this.isRetryable = t.isRetryable ?? true;
    }
  };
  var _r = class extends ae {
    constructor(t) {
      super("This request must be retried", R(t, true));
      this.name = "ForcedRetryError";
      this.code = "P5001";
    }
  };
  w(_r, "ForcedRetryError");
  var ar = class extends ae {
    constructor(t, n) {
      super(t, R(n, false));
      this.name = "InvalidDatasourceError";
      this.code = "P6001";
    }
  };
  w(ar, "InvalidDatasourceError");
  var lr = class extends ae {
    constructor(t, n) {
      super(t, R(n, false));
      this.name = "NotImplementedYetError";
      this.code = "P5004";
    }
  };
  w(lr, "NotImplementedYetError");
  var $ = class extends ae {
    constructor(r, t) {
      super(r, t), this.response = t.response;
      let n = this.response.headers.get("prisma-request-id");
      if (n) {
        let i = `(The request id was: ${n})`;
        this.message = this.message + " " + i;
      }
    }
  };
  var ur = class extends $ {
    constructor(t) {
      super("Schema needs to be uploaded", R(t, true));
      this.name = "SchemaMissingError";
      this.code = "P5005";
    }
  };
  w(ur, "SchemaMissingError");
  var Li = "This request could not be understood by the server";
  var ct = class extends $ {
    constructor(t, n, i) {
      super(n || Li, R(t, false));
      this.name = "BadRequestError";
      this.code = "P5000";
      i && (this.code = i);
    }
  };
  w(ct, "BadRequestError");
  var pt = class extends $ {
    constructor(t, n) {
      super("Engine not started: healthcheck timeout", R(t, true));
      this.name = "HealthcheckTimeoutError";
      this.code = "P5013";
      this.logs = n;
    }
  };
  w(pt, "HealthcheckTimeoutError");
  var mt = class extends $ {
    constructor(t, n, i) {
      super(n, R(t, true));
      this.name = "EngineStartupError";
      this.code = "P5014";
      this.logs = i;
    }
  };
  w(mt, "EngineStartupError");
  var dt = class extends $ {
    constructor(t) {
      super("Engine version is not supported", R(t, false));
      this.name = "EngineVersionNotSupportedError";
      this.code = "P5012";
    }
  };
  w(dt, "EngineVersionNotSupportedError");
  var Ni = "Request timed out";
  var ft = class extends $ {
    constructor(t, n = Ni) {
      super(n, R(t, false));
      this.name = "GatewayTimeoutError";
      this.code = "P5009";
    }
  };
  w(ft, "GatewayTimeoutError");
  var Yp = "Interactive transaction error";
  var gt = class extends $ {
    constructor(t, n = Yp) {
      super(n, R(t, false));
      this.name = "InteractiveTransactionError";
      this.code = "P5015";
    }
  };
  w(gt, "InteractiveTransactionError");
  var Zp = "Request parameters are invalid";
  var ht = class extends $ {
    constructor(t, n = Zp) {
      super(n, R(t, false));
      this.name = "InvalidRequestError";
      this.code = "P5011";
    }
  };
  w(ht, "InvalidRequestError");
  var Oi = "Requested resource does not exist";
  var yt = class extends $ {
    constructor(t, n = Oi) {
      super(n, R(t, false));
      this.name = "NotFoundError";
      this.code = "P5003";
    }
  };
  w(yt, "NotFoundError");
  var Fi = "Unknown server error";
  var kr = class extends $ {
    constructor(t, n, i) {
      super(n || Fi, R(t, true));
      this.name = "ServerError";
      this.code = "P5006";
      this.logs = i;
    }
  };
  w(kr, "ServerError");
  var Mi = "Unauthorized, check your connection string";
  var Et = class extends $ {
    constructor(t, n = Mi) {
      super(n, R(t, false));
      this.name = "UnauthorizedError";
      this.code = "P5007";
    }
  };
  w(Et, "UnauthorizedError");
  var $i = "Usage exceeded, retry again later";
  var bt = class extends $ {
    constructor(t, n = $i) {
      super(n, R(t, true));
      this.name = "UsageExceededError";
      this.code = "P5008";
    }
  };
  w(bt, "UsageExceededError");
  var Fe = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/";
  var Ia = { "@prisma/debug": "workspace:*", "@prisma/engines-version": "5.10.0-34.5a9203d0590c951969e85a7d07215503f4672eb9", "@prisma/fetch-engine": "workspace:*", "@prisma/get-platform": "workspace:*" };
  var xt = class extends ae {
    constructor(t, n) {
      super(`Cannot fetch data from service:
${t}`, R(n, true));
      this.name = "RequestError";
      this.code = "P5010";
    }
  };
  w(xt, "RequestError");
  var om = typeof import.meta.require < "u" ? import.meta.require : () => {
  };
  var Bi = class {
    constructor(r = {}) {
      this.headers = new Map;
      for (let [t, n] of Object.entries(r))
        if (typeof n == "string")
          this.headers.set(t, n);
        else if (Array.isArray(n))
          for (let i of n)
            this.headers.set(t, i);
    }
    append(r, t) {
      this.headers.set(r, t);
    }
    delete(r) {
      this.headers.delete(r);
    }
    get(r) {
      return this.headers.get(r) ?? null;
    }
    has(r) {
      return this.headers.has(r);
    }
    set(r, t) {
      this.headers.set(r, t);
    }
    forEach(r, t) {
      for (let [n, i] of this.headers)
        r.call(t, i, n, this);
    }
  };
  var sm = /^[1-9][0-9]*\.[0-9]+\.[0-9]+$/;
  var _a = N("prisma:client:dataproxyEngine");
  var Da = 3;
  var Vi = N("prisma:client:dataproxyEngine");
  var ji = class {
    constructor({ apiKey: r, tracingHelper: t, logLevel: n, logQueries: i, engineHash: o }) {
      this.apiKey = r, this.tracingHelper = t, this.logLevel = n, this.logQueries = i, this.engineHash = o;
    }
    build({ traceparent: r, interactiveTransaction: t } = {}) {
      let n = { Authorization: `Bearer ${this.apiKey}`, "Prisma-Engine-Hash": this.engineHash };
      this.tracingHelper.isEnabled() && (n.traceparent = r ?? this.tracingHelper.getTraceParent()), t && (n["X-transaction-id"] = t.id);
      let i = this.buildCaptureSettings();
      return i.length > 0 && (n["X-capture-telemetry"] = i.join(", ")), n;
    }
    buildCaptureSettings() {
      let r = [];
      return this.tracingHelper.isEnabled() && r.push("tracing"), this.logLevel && r.push(this.logLevel), this.logQueries && r.push("query"), r;
    }
  };
  var Pt = class {
    constructor(r) {
      this.name = "DataProxyEngine";
      Ra(r), this.config = r, this.env = { ...r.env, ...typeof process < "u" ? process.env : {} }, this.inlineSchema = Sa(r.inlineSchema), this.inlineDatasources = r.inlineDatasources, this.inlineSchemaHash = r.inlineSchemaHash, this.clientVersion = r.clientVersion, this.engineHash = r.engineVersion, this.logEmitter = r.logEmitter, this.tracingHelper = r.tracingHelper;
    }
    apiKey() {
      return this.headerBuilder.apiKey;
    }
    version() {
      return this.engineHash;
    }
    async start() {
      this.startPromise !== undefined && await this.startPromise, this.startPromise = (async () => {
        let [r, t] = this.extractHostAndApiKey();
        this.host = r, this.headerBuilder = new ji({ apiKey: t, tracingHelper: this.tracingHelper, logLevel: this.config.logLevel, logQueries: this.config.logQueries, engineHash: this.engineHash }), this.remoteClientVersion = await ka(r, this.config), Vi("host", this.host);
      })(), await this.startPromise;
    }
    async stop() {
    }
    propagateResponseExtensions(r) {
      r?.logs?.length && r.logs.forEach((t) => {
        switch (t.level) {
          case "debug":
          case "error":
          case "trace":
          case "warn":
          case "info":
            break;
          case "query": {
            let n = typeof t.attributes.query == "string" ? t.attributes.query : "";
            if (!this.tracingHelper.isEnabled()) {
              let [i] = n.split("/* traceparent");
              n = i;
            }
            this.logEmitter.emit("query", { query: n, timestamp: Aa(t.timestamp), duration: Number(t.attributes.duration_ms), params: t.attributes.params, target: t.attributes.target });
          }
        }
      }), r?.traces?.length && this.tracingHelper.createEngineSpan({ span: true, spans: r.traces });
    }
    onBeforeExit() {
      throw new Error('"beforeExit" hook is not applicable to the remote query engine');
    }
    async url(r) {
      return await this.start(), `https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${r}`;
    }
    async uploadSchema() {
      let r = { name: "schemaUpload", internal: true };
      return this.tracingHelper.runInChildSpan(r, async () => {
        let t = await cr(await this.url("schema"), { method: "PUT", headers: this.headerBuilder.build(), body: this.inlineSchema, clientVersion: this.clientVersion });
        t.ok || Vi("schema response status", t.status);
        let n = await wt(t, this.clientVersion);
        if (n)
          throw this.logEmitter.emit("warn", { message: `Error while uploading schema: ${n.message}`, timestamp: new Date, target: "" }), n;
        this.logEmitter.emit("info", { message: `Schema (re)uploaded (hash: ${this.inlineSchemaHash})`, timestamp: new Date, target: "" });
      });
    }
    request(r, { traceparent: t, interactiveTransaction: n, customDataProxyFetch: i }) {
      return this.requestInternal({ body: r, traceparent: t, interactiveTransaction: n, customDataProxyFetch: i });
    }
    async requestBatch(r, { traceparent: t, transaction: n, customDataProxyFetch: i }) {
      let o = n?.kind === "itx" ? n.options : undefined, s = Er(r, n), { batchResult: a, elapsed: l } = await this.requestInternal({ body: s, customDataProxyFetch: i, interactiveTransaction: o, traceparent: t });
      return a.map((u) => ("errors" in u) && u.errors.length > 0 ? sr(u.errors[0], this.clientVersion, this.config.activeProvider) : { data: u, elapsed: l });
    }
    requestInternal({ body: r, traceparent: t, customDataProxyFetch: n, interactiveTransaction: i }) {
      return this.withRetry({ actionGerund: "querying", callback: async ({ logHttpCall: o }) => {
        let s = i ? `${i.payload.endpoint}/graphql` : await this.url("graphql");
        o(s);
        let a = await cr(s, { method: "POST", headers: this.headerBuilder.build({ traceparent: t, interactiveTransaction: i }), body: JSON.stringify(r), clientVersion: this.clientVersion }, n);
        a.ok || Vi("graphql response status", a.status), await this.handleError(await wt(a, this.clientVersion));
        let l = await a.json(), u = l.extensions;
        if (u && this.propagateResponseExtensions(u), l.errors)
          throw l.errors.length === 1 ? sr(l.errors[0], this.config.clientVersion, this.config.activeProvider) : new j(l.errors, { clientVersion: this.config.clientVersion });
        return l;
      } });
    }
    async transaction(r, t, n) {
      let i = { start: "starting", commit: "committing", rollback: "rolling back" };
      return this.withRetry({ actionGerund: `${i[r]} transaction`, callback: async ({ logHttpCall: o }) => {
        if (r === "start") {
          let s = JSON.stringify({ max_wait: n.maxWait, timeout: n.timeout, isolation_level: n.isolationLevel }), a = await this.url("transaction/start");
          o(a);
          let l = await cr(a, { method: "POST", headers: this.headerBuilder.build({ traceparent: t.traceparent }), body: s, clientVersion: this.clientVersion });
          await this.handleError(await wt(l, this.clientVersion));
          let u = await l.json(), c = u.extensions;
          c && this.propagateResponseExtensions(c);
          let p = u.id, m = u["data-proxy"].endpoint;
          return { id: p, payload: { endpoint: m } };
        } else {
          let s = `${n.payload.endpoint}/${r}`;
          o(s);
          let a = await cr(s, { method: "POST", headers: this.headerBuilder.build({ traceparent: t.traceparent }), clientVersion: this.clientVersion });
          await this.handleError(await wt(a, this.clientVersion));
          let u = (await a.json()).extensions;
          u && this.propagateResponseExtensions(u);
          return;
        }
      } });
    }
    extractHostAndApiKey() {
      let r = { clientVersion: this.clientVersion }, t = Object.keys(this.inlineDatasources)[0], n = Ir({ inlineDatasources: this.inlineDatasources, overrideDatasources: this.config.overrideDatasources, clientVersion: this.clientVersion, env: this.env }), i;
      try {
        i = new URL(n);
      } catch {
        throw new ar(`Error validating datasource \`${t}\`: the URL must start with the protocol \`prisma://\``, r);
      }
      let { protocol: o, host: s, searchParams: a } = i;
      if (o !== "prisma:")
        throw new ar(`Error validating datasource \`${t}\`: the URL must start with the protocol \`prisma://\``, r);
      let l = a.get("api_key");
      if (l === null || l.length < 1)
        throw new ar(`Error validating datasource \`${t}\`: the URL must contain a valid API key`, r);
      return [s, l];
    }
    metrics() {
      throw new lr("Metrics are not yet supported for Accelerate", { clientVersion: this.clientVersion });
    }
    async withRetry(r) {
      for (let t = 0;; t++) {
        let n = (i) => {
          this.logEmitter.emit("info", { message: `Calling ${i} (n=${t})`, timestamp: new Date, target: "" });
        };
        try {
          return await r.callback({ logHttpCall: n });
        } catch (i) {
          if (!(i instanceof ae) || !i.isRetryable)
            throw i;
          if (t >= Da)
            throw i instanceof _r ? i.cause : i;
          this.logEmitter.emit("warn", { message: `Attempt ${t + 1}/${Da} failed for ${r.actionGerund}: ${i.message ?? "(unknown)"}`, timestamp: new Date, target: "" });
          let o = await Ca(t);
          this.logEmitter.emit("warn", { message: `Retrying after ${o}ms`, timestamp: new Date, target: "" });
        }
      }
    }
    async handleError(r) {
      if (r instanceof ur)
        throw await this.uploadSchema(), new _r({ clientVersion: this.clientVersion, cause: r });
      if (r)
        throw r;
    }
  };
  var Qi = _(import.meta.require("os"));
  var Na = _(import.meta.require("path"));
  var Ui = Symbol("PrismaLibraryEngineCache");
  var Oa = { async loadLibrary(e) {
    let r = await Wn(), t = await Ea("library", e);
    try {
      return e.tracingHelper.runInChildSpan({ name: "loadLibrary", internal: true }, () => cm(t));
    } catch (n) {
      let i = ti({ e: n, platformInfo: r, id: t });
      throw new S(i, e.clientVersion);
    }
  } };
  var Gi;
  var Fa = { async loadLibrary(e) {
    let { clientVersion: r, adapter: t, engineWasm: n } = e;
    if (t === undefined)
      throw new S(`The \`adapter\` option for \`PrismaClient\` is required in this context (${gn()})`, r);
    if (n === undefined)
      throw new S("WASM engine was unexpectedly `undefined`", r);
    Gi === undefined && (Gi = (async () => {
      let o = n.getRuntime(), s = await n.getQueryEngineWasmModule();
      if (s == null)
        throw new S("The loaded wasm module was unexpectedly `undefined` or `null` once loaded", r);
      let a = { "./query_engine_bg.js": o }, l = new WebAssembly.Instance(s, a);
      return o.__wbg_set_wasm(l.exports), o.QueryEngine;
    })());
    let i = await Gi;
    return { debugPanic() {
      return Promise.reject("{}");
    }, dmmf() {
      return Promise.resolve("{}");
    }, version() {
      return { commit: "unknown", version: "unknown" };
    }, QueryEngine: i };
  } };
  var pm = "P2036";
  var Re = N("prisma:client:libraryEngine");
  var Ma = [...Mn, "native"];
  var $a = 0;
  var vt = class {
    constructor(r, t) {
      this.name = "LibraryEngine";
      this.libraryLoader = t ?? Oa, r.engineWasm !== undefined && (this.libraryLoader = t ?? Fa), this.config = r, this.libraryStarted = false, this.logQueries = r.logQueries ?? false, this.logLevel = r.logLevel ?? "error", this.logEmitter = r.logEmitter, this.datamodel = r.inlineSchema, r.enableDebugLogs && (this.logLevel = "debug");
      let n = Object.keys(r.overrideDatasources)[0], i = r.overrideDatasources[n]?.url;
      n !== undefined && i !== undefined && (this.datasourceOverrides = { [n]: i }), this.libraryInstantiationPromise = this.instantiateLibrary(), this.checkForTooManyEngines();
    }
    checkForTooManyEngines() {
      $a === 10 && console.warn(`${de("warn(prisma-client)")} This is the 10th instance of Prisma Client being started. Make sure this is intentional.`);
    }
    async transaction(r, t, n) {
      await this.start();
      let i = JSON.stringify(t), o;
      if (r === "start") {
        let a = JSON.stringify({ max_wait: n.maxWait, timeout: n.timeout, isolation_level: n.isolationLevel });
        o = await this.engine?.startTransaction(a, i);
      } else
        r === "commit" ? o = await this.engine?.commitTransaction(n.id, i) : r === "rollback" && (o = await this.engine?.rollbackTransaction(n.id, i));
      let s = this.parseEngineResponse(o);
      if (fm(s)) {
        let a = this.getExternalAdapterError(s);
        throw a ? a.error : new V(s.message, { code: s.error_code, clientVersion: this.config.clientVersion, meta: s.meta });
      }
      return s;
    }
    async instantiateLibrary() {
      if (Re("internalSetup"), this.libraryInstantiationPromise)
        return this.libraryInstantiationPromise;
      Fn(), this.binaryTarget = await this.getCurrentBinaryTarget(), await this.loadEngine(), this.version();
    }
    async getCurrentBinaryTarget() {
      {
        if (this.binaryTarget)
          return this.binaryTarget;
        let r = await rr();
        if (!Ma.includes(r))
          throw new S(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(W(r))}. Possible binaryTargets: ${$e(Ma.join(", "))} or a path to the query engine library.
You may have to run ${$e("prisma generate")} for your changes to take effect.`, this.config.clientVersion);
        return r;
      }
    }
    parseEngineResponse(r) {
      if (!r)
        throw new j("Response from the Engine was empty", { clientVersion: this.config.clientVersion });
      try {
        return JSON.parse(r);
      } catch {
        throw new j("Unable to JSON.parse response from engine", { clientVersion: this.config.clientVersion });
      }
    }
    async loadEngine() {
      if (!this.engine) {
        this.QueryEngineConstructor || (this.library = await this.libraryLoader.loadLibrary(this.config), this.QueryEngineConstructor = this.library.QueryEngine);
        try {
          let r = new WeakRef(this), { adapter: t } = this.config;
          t && Re("Using driver adapter: %O", t), this.engine = new this.QueryEngineConstructor({ datamodel: this.datamodel, env: process.env, logQueries: this.config.logQueries ?? false, ignoreEnvVarErrors: true, datasourceOverrides: this.datasourceOverrides ?? {}, logLevel: this.logLevel, configDir: this.config.cwd, engineProtocol: "json" }, (n) => {
            r.deref()?.logger(n);
          }, t), $a++;
        } catch (r) {
          let t = r, n = this.parseInitError(t.message);
          throw typeof n == "string" ? t : new S(n.message, this.config.clientVersion, n.error_code);
        }
      }
    }
    logger(r) {
      let t = this.parseEngineResponse(r);
      if (t) {
        if ("span" in t) {
          this.config.tracingHelper.createEngineSpan(t);
          return;
        }
        t.level = t?.level.toLowerCase() ?? "unknown", mm(t) ? this.logEmitter.emit("query", { timestamp: new Date, query: t.query, params: t.params, duration: Number(t.duration_ms), target: t.module_path }) : dm(t) ? this.loggerRustPanic = new ue(Ji(this, `${t.message}: ${t.reason} in ${t.file}:${t.line}:${t.column}`), this.config.clientVersion) : this.logEmitter.emit(t.level, { timestamp: new Date, message: t.message, target: t.module_path });
      }
    }
    parseInitError(r) {
      try {
        return JSON.parse(r);
      } catch {
      }
      return r;
    }
    parseRequestError(r) {
      try {
        return JSON.parse(r);
      } catch {
      }
      return r;
    }
    onBeforeExit() {
      throw new Error('"beforeExit" hook is not applicable to the library engine since Prisma 5.0.0, it is only relevant and implemented for the binary engine. Please add your event listener to the `process` object directly instead.');
    }
    async start() {
      if (await this.libraryInstantiationPromise, await this.libraryStoppingPromise, this.libraryStartingPromise)
        return Re(`library already starting, this.libraryStarted: ${this.libraryStarted}`), this.libraryStartingPromise;
      if (this.libraryStarted)
        return;
      let r = async () => {
        Re("library starting");
        try {
          let t = { traceparent: this.config.tracingHelper.getTraceParent() };
          await this.engine?.connect(JSON.stringify(t)), this.libraryStarted = true, Re("library started");
        } catch (t) {
          let n = this.parseInitError(t.message);
          throw typeof n == "string" ? t : new S(n.message, this.config.clientVersion, n.error_code);
        } finally {
          this.libraryStartingPromise = undefined;
        }
      };
      return this.libraryStartingPromise = this.config.tracingHelper.runInChildSpan("connect", r), this.libraryStartingPromise;
    }
    async stop() {
      if (await this.libraryStartingPromise, await this.executingQueryPromise, this.libraryStoppingPromise)
        return Re("library is already stopping"), this.libraryStoppingPromise;
      if (!this.libraryStarted)
        return;
      let r = async () => {
        await new Promise((n) => setTimeout(n, 5)), Re("library stopping");
        let t = { traceparent: this.config.tracingHelper.getTraceParent() };
        await this.engine?.disconnect(JSON.stringify(t)), this.libraryStarted = false, this.libraryStoppingPromise = undefined, Re("library stopped");
      };
      return this.libraryStoppingPromise = this.config.tracingHelper.runInChildSpan("disconnect", r), this.libraryStoppingPromise;
    }
    version() {
      return this.versionInfo = this.library?.version(), this.versionInfo?.version ?? "unknown";
    }
    debugPanic(r) {
      return this.library?.debugPanic(r);
    }
    async request(r, { traceparent: t, interactiveTransaction: n }) {
      Re(`sending request, this.libraryStarted: ${this.libraryStarted}`);
      let i = JSON.stringify({ traceparent: t }), o = JSON.stringify(r);
      try {
        await this.start(), this.executingQueryPromise = this.engine?.query(o, i, n?.id), this.lastQuery = o;
        let s = this.parseEngineResponse(await this.executingQueryPromise);
        if (s.errors)
          throw s.errors.length === 1 ? this.buildQueryError(s.errors[0]) : new j(JSON.stringify(s.errors), { clientVersion: this.config.clientVersion });
        if (this.loggerRustPanic)
          throw this.loggerRustPanic;
        return { data: s, elapsed: 0 };
      } catch (s) {
        if (s instanceof S)
          throw s;
        if (s.code === "GenericFailure" && s.message?.startsWith("PANIC:"))
          throw new ue(Ji(this, s.message), this.config.clientVersion);
        let a = this.parseRequestError(s.message);
        throw typeof a == "string" ? s : new j(`${a.message}
${a.backtrace}`, { clientVersion: this.config.clientVersion });
      }
    }
    async requestBatch(r, { transaction: t, traceparent: n }) {
      Re("requestBatch");
      let i = Er(r, t);
      await this.start(), this.lastQuery = JSON.stringify(i), this.executingQueryPromise = this.engine.query(this.lastQuery, JSON.stringify({ traceparent: n }), La(t));
      let o = await this.executingQueryPromise, s = this.parseEngineResponse(o);
      if (s.errors)
        throw s.errors.length === 1 ? this.buildQueryError(s.errors[0]) : new j(JSON.stringify(s.errors), { clientVersion: this.config.clientVersion });
      let { batchResult: a, errors: l } = s;
      if (Array.isArray(a))
        return a.map((u) => u.errors && u.errors.length > 0 ? this.loggerRustPanic ?? this.buildQueryError(u.errors[0]) : { data: u, elapsed: 0 });
      throw l && l.length === 1 ? new Error(l[0].error) : new Error(JSON.stringify(s));
    }
    buildQueryError(r) {
      if (r.user_facing_error.is_panic)
        return new ue(Ji(this, r.user_facing_error.message), this.config.clientVersion);
      let t = this.getExternalAdapterError(r.user_facing_error);
      return t ? t.error : sr(r, this.config.clientVersion, this.config.activeProvider);
    }
    getExternalAdapterError(r) {
      if (r.error_code === pm && this.config.adapter) {
        let t = r.meta?.id;
        jt(typeof t == "number", "Malformed external JS error received from the engine");
        let n = this.config.adapter.errorRegistry.consumeError(t);
        return jt(n, "External error with reported id was not registered"), n;
      }
    }
    async metrics(r) {
      await this.start();
      let t = await this.engine.metrics(JSON.stringify(r));
      return r.format === "prometheus" ? t : this.parseEngineResponse(t);
    }
  };
  var Ga = _(Hi());
  var Me = class {
    constructor(r, t) {
      this.name = r;
      this.value = t;
      this.isRequired = false;
    }
    makeRequired() {
      return this.isRequired = true, this;
    }
    write(r) {
      let { colors: { green: t } } = r.context;
      r.addMarginSymbol(t(this.isRequired ? "+" : "?")), r.write(t(this.name)), this.isRequired || r.write(t("?")), r.write(t(": ")), typeof this.value == "string" ? r.write(t(this.value)) : r.write(this.value);
    }
  };
  var Pn = class {
    constructor() {
      this.fields = [];
    }
    addField(r, t) {
      return this.fields.push({ write(n) {
        let { green: i, dim: o } = n.context.colors;
        n.write(i(o(`${r}: ${t}`))).addMarginSymbol(i(o("+")));
      } }), this;
    }
    write(r) {
      let { colors: { green: t } } = r.context;
      r.writeLine(t("{")).withIndent(() => {
        r.writeJoined(Cr, this.fields).newLine();
      }).write(t("}")).addMarginSymbol(t("+"));
    }
  };
  var km = 3;
  var Lm = { findUnique: "findUnique", findUniqueOrThrow: "findUniqueOrThrow", findFirst: "findFirst", findFirstOrThrow: "findFirstOrThrow", findMany: "findMany", count: "aggregate", create: "createOne", createMany: "createMany", update: "updateOne", updateMany: "updateMany", upsert: "upsertOne", delete: "deleteOne", deleteMany: "deleteMany", executeRaw: "executeRaw", queryRaw: "queryRaw", aggregate: "aggregate", groupBy: "groupBy", runCommandRaw: "runCommandRaw", findRaw: "findRaw", aggregateRaw: "aggregateRaw" };
  var Wi = class e {
    constructor(r) {
      this.params = r;
      this.params.modelName && (this.model = this.params.runtimeDataModel.models[this.params.modelName]);
    }
    throwValidationError(r) {
      Cn({ errors: [r], originalMethod: this.params.originalMethod, args: this.params.rootArgs ?? {}, callsite: this.params.callsite, errorFormat: this.params.errorFormat, clientVersion: this.params.clientVersion });
    }
    getSelectionPath() {
      return this.params.selectionPath;
    }
    getArgumentPath() {
      return this.params.argumentPath;
    }
    getArgumentName() {
      return this.params.argumentPath[this.params.argumentPath.length - 1];
    }
    getOutputTypeDescription() {
      if (!(!this.params.modelName || !this.model))
        return { name: this.params.modelName, fields: this.model.fields.map((r) => ({ name: r.name, typeName: "boolean", isRelation: r.kind === "object" })) };
    }
    isRawAction() {
      return ["executeRaw", "queryRaw", "runCommandRaw", "findRaw", "aggregateRaw"].includes(this.params.action);
    }
    getComputedFields() {
      if (this.params.modelName)
        return this.params.extensions.getAllComputedFields(this.params.modelName);
    }
    findField(r) {
      return this.model?.fields.find((t) => t.name === r);
    }
    nestSelection(r) {
      let t = this.findField(r), n = t?.kind === "object" ? t.type : undefined;
      return new e({ ...this.params, modelName: n, selectionPath: this.params.selectionPath.concat(r) });
    }
    nestArgument(r) {
      return new e({ ...this.params, argumentPath: this.params.argumentPath.concat(r) });
    }
  };
  var Xa = (e) => ({ command: e });
  var el = (e) => e.strings.reduce((r, t, n) => `${r}@P${n}${t}`);
  var Um = /^(\s*alter\s)/i;
  var il = N("prisma:client");
  var Yi = ({ clientMethod: e, activeProvider: r }) => (t) => {
    let n = "", i;
    if (Array.isArray(t)) {
      let [o, ...s] = t;
      n = o, i = { values: Ct(s || []), __prismaRawParameters__: true };
    } else
      switch (r) {
        case "sqlite":
        case "mysql": {
          n = t.sql, i = { values: Ct(t.values), __prismaRawParameters__: true };
          break;
        }
        case "cockroachdb":
        case "postgresql":
        case "postgres": {
          n = t.text, i = { values: Ct(t.values), __prismaRawParameters__: true };
          break;
        }
        case "sqlserver": {
          n = el(t), i = { values: Ct(t.values), __prismaRawParameters__: true };
          break;
        }
        default:
          throw new Error(`The ${r} provider does not support ${e}`);
      }
    return i?.values ? il(`prisma.${e}(${n}, ${i.values})`) : il(`prisma.${e}(${n})`), { query: n, parameters: i };
  };
  var ol = { requestArgsToMiddlewareArgs(e) {
    return [e.strings, ...e.values];
  }, middlewareArgsToRequestArgs(e) {
    let [r, ...t] = e;
    return new oe(r, t);
  } };
  var sl = { requestArgsToMiddlewareArgs(e) {
    return [e];
  }, middlewareArgsToRequestArgs(e) {
    return e[0];
  } };
  var ll = { isEnabled() {
    return false;
  }, getTraceParent() {
    return "00-10-10-00";
  }, async createEngineSpan() {
  }, getActiveContext() {
  }, runInChildSpan(e, r) {
    return r();
  } };
  var Xi = class {
    isEnabled() {
      return this.getGlobalTracingHelper().isEnabled();
    }
    getTraceParent(r) {
      return this.getGlobalTracingHelper().getTraceParent(r);
    }
    createEngineSpan(r) {
      return this.getGlobalTracingHelper().createEngineSpan(r);
    }
    getActiveContext() {
      return this.getGlobalTracingHelper().getActiveContext();
    }
    runInChildSpan(r, t) {
      return this.getGlobalTracingHelper().runInChildSpan(r, t);
    }
    getGlobalTracingHelper() {
      return globalThis.PRISMA_INSTRUMENTATION?.helper ?? ll;
    }
  };
  var Qm = ["$connect", "$disconnect", "$on", "$transaction", "$use", "$extends"];
  var pl = Qm;
  var Sn = class {
    constructor() {
      this._middlewares = [];
    }
    use(r) {
      this._middlewares.push(r);
    }
    get(r) {
      return this._middlewares[r];
    }
    has(r) {
      return !!this._middlewares[r];
    }
    length() {
      return this._middlewares.length;
    }
  };
  var fl = _(li());
  var Hm = { aggregate: false, aggregateRaw: false, createMany: true, createOne: true, deleteMany: true, deleteOne: true, executeRaw: true, findFirst: false, findFirstOrThrow: false, findMany: false, findRaw: false, findUnique: false, findUniqueOrThrow: false, groupBy: false, queryRaw: false, runCommandRaw: true, updateMany: true, updateOne: true, upsertOne: true };
  var In = class {
    constructor(r) {
      this.options = r;
      this.tickActive = false;
      this.batches = {};
    }
    request(r) {
      let t = this.options.batchBy(r);
      return t ? (this.batches[t] || (this.batches[t] = [], this.tickActive || (this.tickActive = true, process.nextTick(() => {
        this.dispatchBatches(), this.tickActive = false;
      }))), new Promise((n, i) => {
        this.batches[t].push({ request: r, resolve: n, reject: i });
      })) : this.options.singleLoader(r);
    }
    dispatchBatches() {
      for (let r in this.batches) {
        let t = this.batches[r];
        delete this.batches[r], t.length === 1 ? this.options.singleLoader(t[0].request).then((n) => {
          n instanceof Error ? t[0].reject(n) : t[0].resolve(n);
        }).catch((n) => {
          t[0].reject(n);
        }) : (t.sort((n, i) => this.options.batchOrder(n.request, i.request)), this.options.batchLoader(t.map((n) => n.request)).then((n) => {
          if (n instanceof Error)
            for (let i = 0;i < t.length; i++)
              t[i].reject(n);
          else
            for (let i = 0;i < t.length; i++) {
              let o = n[i];
              o instanceof Error ? t[i].reject(o) : t[i].resolve(o);
            }
        }).catch((n) => {
          for (let i = 0;i < t.length; i++)
            t[i].reject(n);
        }));
      }
    }
    get [Symbol.toStringTag]() {
      return "DataLoader";
    }
  };
  var Wm = N("prisma:client:request_handler");
  var _n = class {
    constructor(r, t) {
      this.logEmitter = t, this.client = r, this.dataloader = new In({ batchLoader: na(async ({ requests: n, customDataProxyFetch: i }) => {
        let { transaction: o, otelParentCtx: s } = n[0], a = n.map((p) => p.protocolQuery), l = this.client._tracingHelper.getTraceParent(s), u = n.some((p) => ro(p.protocolQuery.action));
        return (await this.client._engine.requestBatch(a, { traceparent: l, transaction: Km(o), containsWrite: u, customDataProxyFetch: i })).map((p, m) => {
          if (p instanceof Error)
            return p;
          try {
            return this.mapQueryEngineResult(n[m], p);
          } catch (f) {
            return f;
          }
        });
      }), singleLoader: async (n) => {
        let i = n.transaction?.kind === "itx" ? gl(n.transaction) : undefined, o = await this.client._engine.request(n.protocolQuery, { traceparent: this.client._tracingHelper.getTraceParent(), interactiveTransaction: i, isWrite: ro(n.protocolQuery.action), customDataProxyFetch: n.customDataProxyFetch });
        return this.mapQueryEngineResult(n, o);
      }, batchBy: (n) => n.transaction?.id ? `transaction-${n.transaction.id}` : dl(n.protocolQuery), batchOrder(n, i) {
        return n.transaction?.kind === "batch" && i.transaction?.kind === "batch" ? n.transaction.index - i.transaction.index : 0;
      } });
    }
    async request(r) {
      try {
        return await this.dataloader.request(r);
      } catch (t) {
        let { clientMethod: n, callsite: i, transaction: o, args: s, modelName: a } = r;
        this.handleAndLogRequestError({ error: t, clientMethod: n, callsite: i, transaction: o, args: s, modelName: a });
      }
    }
    mapQueryEngineResult({ dataPath: r, unpacker: t }, n) {
      let i = n?.data, o = n?.elapsed, s = this.unpack(i, r, t);
      return process.env.PRISMA_CLIENT_GET_TIME ? { data: s, elapsed: o } : s;
    }
    handleAndLogRequestError(r) {
      try {
        this.handleRequestError(r);
      } catch (t) {
        throw this.logEmitter && this.logEmitter.emit("error", { message: t.message, target: r.clientMethod, timestamp: new Date }), t;
      }
    }
    handleRequestError({ error: r, clientMethod: t, callsite: n, transaction: i, args: o, modelName: s }) {
      if (Wm(r), zm(r, i) || r instanceof Le)
        throw r;
      if (r instanceof V && Ym(r)) {
        let l = hl(r.meta);
        Cn({ args: o, errors: [l], callsite: n, errorFormat: this.client._errorFormat, originalMethod: t, clientVersion: this.client._clientVersion });
      }
      let a = r.message;
      if (n && (a = Ar({ callsite: n, originalMethod: t, isPanic: r.isPanic, showColors: this.client._errorFormat === "pretty", message: a })), a = this.sanitizeMessage(a), r.code) {
        let l = s ? { modelName: s, ...r.meta } : r.meta;
        throw new V(a, { code: r.code, clientVersion: this.client._clientVersion, meta: l, batchRequestIdx: r.batchRequestIdx });
      } else {
        if (r.isPanic)
          throw new ue(a, this.client._clientVersion);
        if (r instanceof j)
          throw new j(a, { clientVersion: this.client._clientVersion, batchRequestIdx: r.batchRequestIdx });
        if (r instanceof S)
          throw new S(a, this.client._clientVersion);
        if (r instanceof ue)
          throw new ue(a, this.client._clientVersion);
      }
      throw r.clientVersion = this.client._clientVersion, r;
    }
    sanitizeMessage(r) {
      return this.client._errorFormat && this.client._errorFormat !== "pretty" ? (0, fl.default)(r) : r;
    }
    unpack(r, t, n) {
      if (!r || (r.data && (r = r.data), !r))
        return r;
      let i = Object.values(r)[0], o = t.filter((a) => a !== "select" && a !== "include"), s = An(Ai(i, o));
      return n ? n(s) : s;
    }
    get [Symbol.toStringTag]() {
      return "RequestHandler";
    }
  };
  var yl = "5.10.2";
  var El = yl;
  var Tl = _(Hi());
  var q = class extends Error {
    constructor(r) {
      super(r + `
Read more at https://pris.ly/d/client-constructor`), this.name = "PrismaClientConstructorValidationError";
    }
    get [Symbol.toStringTag]() {
      return "PrismaClientConstructorValidationError";
    }
  };
  w(q, "PrismaClientConstructorValidationError");
  var xl = ["datasources", "datasourceUrl", "errorFormat", "adapter", "log", "transactionOptions", "__internal"];
  var Pl = ["pretty", "colorless", "minimal"];
  var vl = ["info", "query", "warn", "error"];
  var Xm = { datasources: (e, { datasourceNames: r }) => {
    if (e) {
      if (typeof e != "object" || Array.isArray(e))
        throw new q(`Invalid value ${JSON.stringify(e)} for "datasources" provided to PrismaClient constructor`);
      for (let [t, n] of Object.entries(e)) {
        if (!r.includes(t)) {
          let i = Lr(t, r) || ` Available datasources: ${r.join(", ")}`;
          throw new q(`Unknown datasource ${t} provided to PrismaClient constructor.${i}`);
        }
        if (typeof n != "object" || Array.isArray(n))
          throw new q(`Invalid value ${JSON.stringify(e)} for datasource "${t}" provided to PrismaClient constructor.
It should have this form: { url: "CONNECTION_STRING" }`);
        if (n && typeof n == "object")
          for (let [i, o] of Object.entries(n)) {
            if (i !== "url")
              throw new q(`Invalid value ${JSON.stringify(e)} for datasource "${t}" provided to PrismaClient constructor.
It should have this form: { url: "CONNECTION_STRING" }`);
            if (typeof o != "string")
              throw new q(`Invalid value ${JSON.stringify(o)} for datasource "${t}" provided to PrismaClient constructor.
It should have this form: { url: "CONNECTION_STRING" }`);
          }
      }
    }
  }, adapter: (e, r) => {
    if (e === null)
      return;
    if (e === undefined)
      throw new q('"adapter" property must not be undefined, use null to conditionally disable driver adapters.');
    if (!wn(r).includes("driverAdapters"))
      throw new q('"adapter" property can only be provided to PrismaClient constructor when "driverAdapters" preview feature is enabled.');
    if (Gr() === "binary")
      throw new q('Cannot use a driver adapter with the "binary" Query Engine. Please use the "library" Query Engine.');
  }, datasourceUrl: (e) => {
    if (typeof e < "u" && typeof e != "string")
      throw new q(`Invalid value ${JSON.stringify(e)} for "datasourceUrl" provided to PrismaClient constructor.
Expected string or undefined.`);
  }, errorFormat: (e) => {
    if (e) {
      if (typeof e != "string")
        throw new q(`Invalid value ${JSON.stringify(e)} for "errorFormat" provided to PrismaClient constructor.`);
      if (!Pl.includes(e)) {
        let r = Lr(e, Pl);
        throw new q(`Invalid errorFormat ${e} provided to PrismaClient constructor.${r}`);
      }
    }
  }, log: (e) => {
    if (!e)
      return;
    if (!Array.isArray(e))
      throw new q(`Invalid value ${JSON.stringify(e)} for "log" provided to PrismaClient constructor.`);
    function r(t) {
      if (typeof t == "string" && !vl.includes(t)) {
        let n = Lr(t, vl);
        throw new q(`Invalid log level "${t}" provided to PrismaClient constructor.${n}`);
      }
    }
    for (let t of e) {
      r(t);
      let n = { level: r, emit: (i) => {
        let o = ["stdout", "event"];
        if (!o.includes(i)) {
          let s = Lr(i, o);
          throw new q(`Invalid value ${JSON.stringify(i)} for "emit" in logLevel provided to PrismaClient constructor.${s}`);
        }
      } };
      if (t && typeof t == "object")
        for (let [i, o] of Object.entries(t))
          if (n[i])
            n[i](o);
          else
            throw new q(`Invalid property ${i} for "log" provided to PrismaClient constructor`);
    }
  }, transactionOptions: (e) => {
    if (!e)
      return;
    let r = e.maxWait;
    if (r != null && r <= 0)
      throw new q(`Invalid value ${r} for maxWait in "transactionOptions" provided to PrismaClient constructor. maxWait needs to be greater than 0`);
    let t = e.timeout;
    if (t != null && t <= 0)
      throw new q(`Invalid value ${t} for timeout in "transactionOptions" provided to PrismaClient constructor. timeout needs to be greater than 0`);
  }, __internal: (e) => {
    if (!e)
      return;
    let r = ["debug", "engine", "configOverride"];
    if (typeof e != "object")
      throw new q(`Invalid value ${JSON.stringify(e)} for "__internal" to PrismaClient constructor`);
    for (let [t] of Object.entries(e))
      if (!r.includes(t)) {
        let n = Lr(t, r);
        throw new q(`Invalid property ${JSON.stringify(t)} for "__internal" provided to PrismaClient constructor.${n}`);
      }
  } };
  var Ye = N("prisma:client");
  typeof globalThis == "object" && (globalThis.NODE_CLIENT = true);
  var rd = { requestArgsToMiddlewareArgs: (e) => e, middlewareArgsToRequestArgs: (e) => e };
  var td = Symbol.for("prisma.client.transaction.id");
  var nd = { id: 0, nextId() {
    return ++this.id;
  } };
  var od = new Set(["toJSON", "$$typeof", "asymmetricMatch", Symbol.iterator, Symbol.toStringTag, Symbol.isConcatSpreadable, Symbol.toPrimitive]);
  /*! Bundled license information:
  
  decimal.js/decimal.mjs:
    (*!
     *  decimal.js v10.4.3
     *  An arbitrary-precision Decimal type for JavaScript.
     *  https://github.com/MikeMcl/decimal.js
     *  Copyright (c) 2022 Michael Mclaughlin <M8ch88l@gmail.com>
     *  MIT Licence
     *)
  */
});

// node_modules/.prisma/client/index.js
var require_client = __commonJS((exports) => {
  var __dirname = "/Users/ambassador4ik/WebstormProjects/The Brainiest/AuthService/Backend/node_modules/.prisma/client";
  Object.defineProperty(exports, "__esModule", { value: true });
  var {
    PrismaClientKnownRequestError: PrismaClientKnownRequestError2,
    PrismaClientUnknownRequestError: PrismaClientUnknownRequestError2,
    PrismaClientRustPanicError: PrismaClientRustPanicError2,
    PrismaClientInitializationError: PrismaClientInitializationError2,
    PrismaClientValidationError: PrismaClientValidationError2,
    NotFoundError: NotFoundError2,
    getPrismaClient: getPrismaClient2,
    sqltag: sqltag2,
    empty: empty2,
    join: join2,
    raw: raw3,
    Decimal: Decimal2,
    Debug: Debug2,
    objectEnumValues: objectEnumValues2,
    makeStrictEnum: makeStrictEnum2,
    Extensions: Extensions2,
    warnOnce: warnOnce2,
    defineDmmfProperty: defineDmmfProperty2,
    Public: Public2,
    detectRuntime: detectRuntime2
  } = require_library();
  var Prisma = {};
  exports.Prisma = Prisma;
  exports.$Enums = {};
  Prisma.prismaVersion = {
    client: "5.10.2",
    engine: "5a9203d0590c951969e85a7d07215503f4672eb9"
  };
  Prisma.PrismaClientKnownRequestError = PrismaClientKnownRequestError2;
  Prisma.PrismaClientUnknownRequestError = PrismaClientUnknownRequestError2;
  Prisma.PrismaClientRustPanicError = PrismaClientRustPanicError2;
  Prisma.PrismaClientInitializationError = PrismaClientInitializationError2;
  Prisma.PrismaClientValidationError = PrismaClientValidationError2;
  Prisma.NotFoundError = NotFoundError2;
  Prisma.Decimal = Decimal2;
  Prisma.sql = sqltag2;
  Prisma.empty = empty2;
  Prisma.join = join2;
  Prisma.raw = raw3;
  Prisma.validator = Public2.validator;
  Prisma.getExtensionContext = Extensions2.getExtensionContext;
  Prisma.defineExtension = Extensions2.defineExtension;
  Prisma.DbNull = objectEnumValues2.instances.DbNull;
  Prisma.JsonNull = objectEnumValues2.instances.JsonNull;
  Prisma.AnyNull = objectEnumValues2.instances.AnyNull;
  Prisma.NullTypes = {
    DbNull: objectEnumValues2.classes.DbNull,
    JsonNull: objectEnumValues2.classes.JsonNull,
    AnyNull: objectEnumValues2.classes.AnyNull
  };
  var path = import.meta.require("path");
  exports.Prisma.TransactionIsolationLevel = makeStrictEnum2({
    ReadUncommitted: "ReadUncommitted",
    ReadCommitted: "ReadCommitted",
    RepeatableRead: "RepeatableRead",
    Serializable: "Serializable"
  });
  exports.Prisma.UserScalarFieldEnum = {
    id: "id",
    username: "username",
    password: "password",
    isUserProfileInitialised: "isUserProfileInitialised",
    isGameProfileInitialised: "isGameProfileInitialised"
  };
  exports.Prisma.RefreshTokenScalarFieldEnum = {
    id: "id",
    token: "token",
    userId: "userId",
    device: "device",
    createdAt: "createdAt",
    expiresAt: "expiresAt"
  };
  exports.Prisma.SortOrder = {
    asc: "asc",
    desc: "desc"
  };
  exports.Prisma.QueryMode = {
    default: "default",
    insensitive: "insensitive"
  };
  exports.Prisma.ModelName = {
    User: "User",
    RefreshToken: "RefreshToken"
  };
  var config4 = {
    generator: {
      name: "client",
      provider: {
        fromEnvVar: null,
        value: "prisma-client-js"
      },
      output: {
        value: "/Users/ambassador4ik/WebstormProjects/The Brainiest/AuthService/Backend/node_modules/@prisma/client",
        fromEnvVar: null
      },
      config: {
        engineType: "library"
      },
      binaryTargets: [
        {
          fromEnvVar: null,
          value: "darwin-arm64",
          native: true
        }
      ],
      previewFeatures: []
    },
    relativeEnvPaths: {
      rootEnvPath: null,
      schemaEnvPath: "../../../.env"
    },
    relativePath: "../../../prisma",
    clientVersion: "5.10.2",
    engineVersion: "5a9203d0590c951969e85a7d07215503f4672eb9",
    datasourceNames: [
      "db"
    ],
    activeProvider: "postgresql",
    inlineDatasources: {
      db: {
        url: {
          fromEnvVar: "DATABASE_URL",
          value: null
        }
      }
    },
    inlineSchema: "generator client {\n  provider = \"prisma-client-js\"\n}\n\ndatasource db {\n  provider = \"postgresql\"\n  url      = env(\"DATABASE_URL\")\n}\n\nmodel User {\n  id                   Int     @id @default(autoincrement())\n  username             String  @unique\n  password             String\n  isUserProfileInitialised Boolean @default(false)\n  isGameProfileInitialised Boolean @default(false)\n  // Relationships\n  refreshTokens        RefreshToken[]\n}\n\nmodel RefreshToken {\n  id        Int      @id @default(autoincrement())\n  token     String   @unique\n  userId    Int      // Foreign key referencing the User model\n  user      User     @relation(fields: [userId], references: [id])\n  // Optional fields for better control and auditing\n  device    String   @unique  // Device name or identifier\n  createdAt DateTime @default(now())\n  expiresAt DateTime\n}\n",
    inlineSchemaHash: "b4f745ef8c0a7d49e6fedc920599a00d5afbd0274d36422764aa796a574d9a0e",
    copyEngine: true
  };
  var fs = import.meta.require("fs");
  config4.dirname = __dirname;
  if (!fs.existsSync(path.join(__dirname, "schema.prisma"))) {
    const alternativePaths = [
      "node_modules/.prisma/client",
      ".prisma/client"
    ];
    const alternativePath = alternativePaths.find((altPath) => {
      return fs.existsSync(path.join(process.cwd(), altPath, "schema.prisma"));
    }) ?? alternativePaths[0];
    config4.dirname = path.join(process.cwd(), alternativePath);
    config4.isBundled = true;
  }
  config4.runtimeDataModel = JSON.parse("{\"models\":{\"User\":{\"dbName\":null,\"fields\":[{\"name\":\"id\",\"kind\":\"scalar\",\"isList\":false,\"isRequired\":true,\"isUnique\":false,\"isId\":true,\"isReadOnly\":false,\"hasDefaultValue\":true,\"type\":\"Int\",\"default\":{\"name\":\"autoincrement\",\"args\":[]},\"isGenerated\":false,\"isUpdatedAt\":false},{\"name\":\"username\",\"kind\":\"scalar\",\"isList\":false,\"isRequired\":true,\"isUnique\":true,\"isId\":false,\"isReadOnly\":false,\"hasDefaultValue\":false,\"type\":\"String\",\"isGenerated\":false,\"isUpdatedAt\":false},{\"name\":\"password\",\"kind\":\"scalar\",\"isList\":false,\"isRequired\":true,\"isUnique\":false,\"isId\":false,\"isReadOnly\":false,\"hasDefaultValue\":false,\"type\":\"String\",\"isGenerated\":false,\"isUpdatedAt\":false},{\"name\":\"isUserProfileInitialised\",\"kind\":\"scalar\",\"isList\":false,\"isRequired\":true,\"isUnique\":false,\"isId\":false,\"isReadOnly\":false,\"hasDefaultValue\":true,\"type\":\"Boolean\",\"default\":false,\"isGenerated\":false,\"isUpdatedAt\":false},{\"name\":\"isGameProfileInitialised\",\"kind\":\"scalar\",\"isList\":false,\"isRequired\":true,\"isUnique\":false,\"isId\":false,\"isReadOnly\":false,\"hasDefaultValue\":true,\"type\":\"Boolean\",\"default\":false,\"isGenerated\":false,\"isUpdatedAt\":false},{\"name\":\"refreshTokens\",\"kind\":\"object\",\"isList\":true,\"isRequired\":true,\"isUnique\":false,\"isId\":false,\"isReadOnly\":false,\"hasDefaultValue\":false,\"type\":\"RefreshToken\",\"relationName\":\"RefreshTokenToUser\",\"relationFromFields\":[],\"relationToFields\":[],\"isGenerated\":false,\"isUpdatedAt\":false}],\"primaryKey\":null,\"uniqueFields\":[],\"uniqueIndexes\":[],\"isGenerated\":false},\"RefreshToken\":{\"dbName\":null,\"fields\":[{\"name\":\"id\",\"kind\":\"scalar\",\"isList\":false,\"isRequired\":true,\"isUnique\":false,\"isId\":true,\"isReadOnly\":false,\"hasDefaultValue\":true,\"type\":\"Int\",\"default\":{\"name\":\"autoincrement\",\"args\":[]},\"isGenerated\":false,\"isUpdatedAt\":false},{\"name\":\"token\",\"kind\":\"scalar\",\"isList\":false,\"isRequired\":true,\"isUnique\":true,\"isId\":false,\"isReadOnly\":false,\"hasDefaultValue\":false,\"type\":\"String\",\"isGenerated\":false,\"isUpdatedAt\":false},{\"name\":\"userId\",\"kind\":\"scalar\",\"isList\":false,\"isRequired\":true,\"isUnique\":false,\"isId\":false,\"isReadOnly\":true,\"hasDefaultValue\":false,\"type\":\"Int\",\"isGenerated\":false,\"isUpdatedAt\":false},{\"name\":\"user\",\"kind\":\"object\",\"isList\":false,\"isRequired\":true,\"isUnique\":false,\"isId\":false,\"isReadOnly\":false,\"hasDefaultValue\":false,\"type\":\"User\",\"relationName\":\"RefreshTokenToUser\",\"relationFromFields\":[\"userId\"],\"relationToFields\":[\"id\"],\"isGenerated\":false,\"isUpdatedAt\":false},{\"name\":\"device\",\"kind\":\"scalar\",\"isList\":false,\"isRequired\":true,\"isUnique\":true,\"isId\":false,\"isReadOnly\":false,\"hasDefaultValue\":false,\"type\":\"String\",\"isGenerated\":false,\"isUpdatedAt\":false},{\"name\":\"createdAt\",\"kind\":\"scalar\",\"isList\":false,\"isRequired\":true,\"isUnique\":false,\"isId\":false,\"isReadOnly\":false,\"hasDefaultValue\":true,\"type\":\"DateTime\",\"default\":{\"name\":\"now\",\"args\":[]},\"isGenerated\":false,\"isUpdatedAt\":false},{\"name\":\"expiresAt\",\"kind\":\"scalar\",\"isList\":false,\"isRequired\":true,\"isUnique\":false,\"isId\":false,\"isReadOnly\":false,\"hasDefaultValue\":false,\"type\":\"DateTime\",\"isGenerated\":false,\"isUpdatedAt\":false}],\"primaryKey\":null,\"uniqueFields\":[],\"uniqueIndexes\":[],\"isGenerated\":false}},\"enums\":{},\"types\":{}}");
  defineDmmfProperty2(exports.Prisma, config4.runtimeDataModel);
  config4.engineWasm = undefined;
  var { warnEnvConflicts: warnEnvConflicts2 } = require_library();
  warnEnvConflicts2({
    rootEnvPath: config4.relativeEnvPaths.rootEnvPath && path.resolve(config4.dirname, config4.relativeEnvPaths.rootEnvPath),
    schemaEnvPath: config4.relativeEnvPaths.schemaEnvPath && path.resolve(config4.dirname, config4.relativeEnvPaths.schemaEnvPath)
  });
  var PrismaClient = getPrismaClient2(config4);
  exports.PrismaClient = PrismaClient;
  Object.assign(exports, Prisma);
  path.join(__dirname, "libquery_engine-darwin-arm64.dylib.node");
  path.join(process.cwd(), "node_modules/.prisma/client/libquery_engine-darwin-arm64.dylib.node");
  path.join(__dirname, "schema.prisma");
  path.join(process.cwd(), "node_modules/.prisma/client/schema.prisma");
});

// node_modules/.prisma/client/default.js
var require_default2 = __commonJS((exports, module) => {
  module.exports = { ...require_client() };
});

// node_modules/@prisma/client/default.js
var require_default3 = __commonJS((exports, module) => {
  module.exports = {
    ...require_default2()
  };
});

// node_modules/safe-buffer/index.js
var require_safe_buffer = __commonJS((exports, module) => {
  var copyProps = function(src, dst) {
    for (var key in src) {
      dst[key] = src[key];
    }
  };
  var SafeBuffer = function(arg, encodingOrOffset, length) {
    return Buffer2(arg, encodingOrOffset, length);
  };
  /*! safe-buffer. MIT License. Feross Aboukhadijeh <https://feross.org/opensource> */
  var buffer = import.meta.require("buffer");
  var Buffer2 = buffer.Buffer;
  if (Buffer2.from && Buffer2.alloc && Buffer2.allocUnsafe && Buffer2.allocUnsafeSlow) {
    module.exports = buffer;
  } else {
    copyProps(buffer, exports);
    exports.Buffer = SafeBuffer;
  }
  SafeBuffer.prototype = Object.create(Buffer2.prototype);
  copyProps(Buffer2, SafeBuffer);
  SafeBuffer.from = function(arg, encodingOrOffset, length) {
    if (typeof arg === "number") {
      throw new TypeError("Argument must not be a number");
    }
    return Buffer2(arg, encodingOrOffset, length);
  };
  SafeBuffer.alloc = function(size, fill, encoding) {
    if (typeof size !== "number") {
      throw new TypeError("Argument must be a number");
    }
    var buf = Buffer2(size);
    if (fill !== undefined) {
      if (typeof encoding === "string") {
        buf.fill(fill, encoding);
      } else {
        buf.fill(fill);
      }
    } else {
      buf.fill(0);
    }
    return buf;
  };
  SafeBuffer.allocUnsafe = function(size) {
    if (typeof size !== "number") {
      throw new TypeError("Argument must be a number");
    }
    return Buffer2(size);
  };
  SafeBuffer.allocUnsafeSlow = function(size) {
    if (typeof size !== "number") {
      throw new TypeError("Argument must be a number");
    }
    return buffer.SlowBuffer(size);
  };
});

// node_modules/jws/lib/data-stream.js
var require_data_stream = __commonJS((exports, module) => {
  var DataStream = function(data) {
    this.buffer = null;
    this.writable = true;
    this.readable = true;
    if (!data) {
      this.buffer = Buffer2.alloc(0);
      return this;
    }
    if (typeof data.pipe === "function") {
      this.buffer = Buffer2.alloc(0);
      data.pipe(this);
      return this;
    }
    if (data.length || typeof data === "object") {
      this.buffer = data;
      this.writable = false;
      process.nextTick(function() {
        this.emit("end", data);
        this.readable = false;
        this.emit("close");
      }.bind(this));
      return this;
    }
    throw new TypeError("Unexpected data type (" + typeof data + ")");
  };
  var Buffer2 = require_safe_buffer().Buffer;
  var Stream = import.meta.require("stream");
  var util = import.meta.require("util");
  util.inherits(DataStream, Stream);
  DataStream.prototype.write = function write(data) {
    this.buffer = Buffer2.concat([this.buffer, Buffer2.from(data)]);
    this.emit("data", data);
  };
  DataStream.prototype.end = function end(data) {
    if (data)
      this.write(data);
    this.emit("end", data);
    this.emit("close");
    this.writable = false;
    this.readable = false;
  };
  module.exports = DataStream;
});

// node_modules/buffer-equal-constant-time/index.js
var require_buffer_equal_constant_time = __commonJS((exports, module) => {
  var bufferEq = function(a, b) {
    if (!Buffer2.isBuffer(a) || !Buffer2.isBuffer(b)) {
      return false;
    }
    if (a.length !== b.length) {
      return false;
    }
    var c = 0;
    for (var i = 0;i < a.length; i++) {
      c |= a[i] ^ b[i];
    }
    return c === 0;
  };
  var Buffer2 = import.meta.require("buffer").Buffer;
  var SlowBuffer = import.meta.require("buffer").SlowBuffer;
  module.exports = bufferEq;
  bufferEq.install = function() {
    Buffer2.prototype.equal = SlowBuffer.prototype.equal = function equal(that) {
      return bufferEq(this, that);
    };
  };
  var origBufEqual = Buffer2.prototype.equal;
  var origSlowBufEqual = SlowBuffer.prototype.equal;
  bufferEq.restore = function() {
    Buffer2.prototype.equal = origBufEqual;
    SlowBuffer.prototype.equal = origSlowBufEqual;
  };
});

// node_modules/ecdsa-sig-formatter/src/param-bytes-for-alg.js
var require_param_bytes_for_alg = __commonJS((exports, module) => {
  var getParamSize = function(keySize) {
    var result = (keySize / 8 | 0) + (keySize % 8 === 0 ? 0 : 1);
    return result;
  };
  var getParamBytesForAlg = function(alg) {
    var paramBytes = paramBytesForAlg[alg];
    if (paramBytes) {
      return paramBytes;
    }
    throw new Error('Unknown algorithm "' + alg + '"');
  };
  var paramBytesForAlg = {
    ES256: getParamSize(256),
    ES384: getParamSize(384),
    ES512: getParamSize(521)
  };
  module.exports = getParamBytesForAlg;
});

// node_modules/ecdsa-sig-formatter/src/ecdsa-sig-formatter.js
var require_ecdsa_sig_formatter = __commonJS((exports, module) => {
  var base64Url = function(base64) {
    return base64.replace(/=/g, "").replace(/\+/g, "-").replace(/\//g, "_");
  };
  var signatureAsBuffer = function(signature) {
    if (Buffer2.isBuffer(signature)) {
      return signature;
    } else if (typeof signature === "string") {
      return Buffer2.from(signature, "base64");
    }
    throw new TypeError("ECDSA signature must be a Base64 string or a Buffer");
  };
  var derToJose = function(signature, alg) {
    signature = signatureAsBuffer(signature);
    var paramBytes = getParamBytesForAlg(alg);
    var maxEncodedParamLength = paramBytes + 1;
    var inputLength = signature.length;
    var offset = 0;
    if (signature[offset++] !== ENCODED_TAG_SEQ) {
      throw new Error('Could not find expected "seq"');
    }
    var seqLength = signature[offset++];
    if (seqLength === (MAX_OCTET | 1)) {
      seqLength = signature[offset++];
    }
    if (inputLength - offset < seqLength) {
      throw new Error('"seq" specified length of "' + seqLength + '", only "' + (inputLength - offset) + '" remaining');
    }
    if (signature[offset++] !== ENCODED_TAG_INT) {
      throw new Error('Could not find expected "int" for "r"');
    }
    var rLength = signature[offset++];
    if (inputLength - offset - 2 < rLength) {
      throw new Error('"r" specified length of "' + rLength + '", only "' + (inputLength - offset - 2) + '" available');
    }
    if (maxEncodedParamLength < rLength) {
      throw new Error('"r" specified length of "' + rLength + '", max of "' + maxEncodedParamLength + '" is acceptable');
    }
    var rOffset = offset;
    offset += rLength;
    if (signature[offset++] !== ENCODED_TAG_INT) {
      throw new Error('Could not find expected "int" for "s"');
    }
    var sLength = signature[offset++];
    if (inputLength - offset !== sLength) {
      throw new Error('"s" specified length of "' + sLength + '", expected "' + (inputLength - offset) + '"');
    }
    if (maxEncodedParamLength < sLength) {
      throw new Error('"s" specified length of "' + sLength + '", max of "' + maxEncodedParamLength + '" is acceptable');
    }
    var sOffset = offset;
    offset += sLength;
    if (offset !== inputLength) {
      throw new Error('Expected to consume entire buffer, but "' + (inputLength - offset) + '" bytes remain');
    }
    var rPadding = paramBytes - rLength, sPadding = paramBytes - sLength;
    var dst = Buffer2.allocUnsafe(rPadding + rLength + sPadding + sLength);
    for (offset = 0;offset < rPadding; ++offset) {
      dst[offset] = 0;
    }
    signature.copy(dst, offset, rOffset + Math.max(-rPadding, 0), rOffset + rLength);
    offset = paramBytes;
    for (var o = offset;offset < o + sPadding; ++offset) {
      dst[offset] = 0;
    }
    signature.copy(dst, offset, sOffset + Math.max(-sPadding, 0), sOffset + sLength);
    dst = dst.toString("base64");
    dst = base64Url(dst);
    return dst;
  };
  var countPadding = function(buf, start, stop) {
    var padding = 0;
    while (start + padding < stop && buf[start + padding] === 0) {
      ++padding;
    }
    var needsSign = buf[start + padding] >= MAX_OCTET;
    if (needsSign) {
      --padding;
    }
    return padding;
  };
  var joseToDer = function(signature, alg) {
    signature = signatureAsBuffer(signature);
    var paramBytes = getParamBytesForAlg(alg);
    var signatureBytes = signature.length;
    if (signatureBytes !== paramBytes * 2) {
      throw new TypeError('"' + alg + '" signatures must be "' + paramBytes * 2 + '" bytes, saw "' + signatureBytes + '"');
    }
    var rPadding = countPadding(signature, 0, paramBytes);
    var sPadding = countPadding(signature, paramBytes, signature.length);
    var rLength = paramBytes - rPadding;
    var sLength = paramBytes - sPadding;
    var rsBytes = 1 + 1 + rLength + 1 + 1 + sLength;
    var shortLength = rsBytes < MAX_OCTET;
    var dst = Buffer2.allocUnsafe((shortLength ? 2 : 3) + rsBytes);
    var offset = 0;
    dst[offset++] = ENCODED_TAG_SEQ;
    if (shortLength) {
      dst[offset++] = rsBytes;
    } else {
      dst[offset++] = MAX_OCTET | 1;
      dst[offset++] = rsBytes & 255;
    }
    dst[offset++] = ENCODED_TAG_INT;
    dst[offset++] = rLength;
    if (rPadding < 0) {
      dst[offset++] = 0;
      offset += signature.copy(dst, offset, 0, paramBytes);
    } else {
      offset += signature.copy(dst, offset, rPadding, paramBytes);
    }
    dst[offset++] = ENCODED_TAG_INT;
    dst[offset++] = sLength;
    if (sPadding < 0) {
      dst[offset++] = 0;
      signature.copy(dst, offset, paramBytes);
    } else {
      signature.copy(dst, offset, paramBytes + sPadding);
    }
    return dst;
  };
  var Buffer2 = require_safe_buffer().Buffer;
  var getParamBytesForAlg = require_param_bytes_for_alg();
  var MAX_OCTET = 128;
  var CLASS_UNIVERSAL = 0;
  var PRIMITIVE_BIT = 32;
  var TAG_SEQ = 16;
  var TAG_INT = 2;
  var ENCODED_TAG_SEQ = TAG_SEQ | PRIMITIVE_BIT | CLASS_UNIVERSAL << 6;
  var ENCODED_TAG_INT = TAG_INT | CLASS_UNIVERSAL << 6;
  module.exports = {
    derToJose,
    joseToDer
  };
});

// node_modules/jwa/index.js
var require_jwa = __commonJS((exports, module) => {
  var checkIsPublicKey = function(key) {
    if (Buffer2.isBuffer(key)) {
      return;
    }
    if (typeof key === "string") {
      return;
    }
    if (!supportsKeyObjects) {
      throw typeError(MSG_INVALID_VERIFIER_KEY);
    }
    if (typeof key !== "object") {
      throw typeError(MSG_INVALID_VERIFIER_KEY);
    }
    if (typeof key.type !== "string") {
      throw typeError(MSG_INVALID_VERIFIER_KEY);
    }
    if (typeof key.asymmetricKeyType !== "string") {
      throw typeError(MSG_INVALID_VERIFIER_KEY);
    }
    if (typeof key.export !== "function") {
      throw typeError(MSG_INVALID_VERIFIER_KEY);
    }
  };
  var checkIsPrivateKey = function(key) {
    if (Buffer2.isBuffer(key)) {
      return;
    }
    if (typeof key === "string") {
      return;
    }
    if (typeof key === "object") {
      return;
    }
    throw typeError(MSG_INVALID_SIGNER_KEY);
  };
  var checkIsSecretKey = function(key) {
    if (Buffer2.isBuffer(key)) {
      return;
    }
    if (typeof key === "string") {
      return key;
    }
    if (!supportsKeyObjects) {
      throw typeError(MSG_INVALID_SECRET);
    }
    if (typeof key !== "object") {
      throw typeError(MSG_INVALID_SECRET);
    }
    if (key.type !== "secret") {
      throw typeError(MSG_INVALID_SECRET);
    }
    if (typeof key.export !== "function") {
      throw typeError(MSG_INVALID_SECRET);
    }
  };
  var fromBase64 = function(base64) {
    return base64.replace(/=/g, "").replace(/\+/g, "-").replace(/\//g, "_");
  };
  var toBase64 = function(base64url) {
    base64url = base64url.toString();
    var padding = 4 - base64url.length % 4;
    if (padding !== 4) {
      for (var i = 0;i < padding; ++i) {
        base64url += "=";
      }
    }
    return base64url.replace(/\-/g, "+").replace(/_/g, "/");
  };
  var typeError = function(template) {
    var args = [].slice.call(arguments, 1);
    var errMsg = util.format.bind(util, template).apply(null, args);
    return new TypeError(errMsg);
  };
  var bufferOrString = function(obj) {
    return Buffer2.isBuffer(obj) || typeof obj === "string";
  };
  var normalizeInput = function(thing) {
    if (!bufferOrString(thing))
      thing = JSON.stringify(thing);
    return thing;
  };
  var createHmacSigner = function(bits) {
    return function sign(thing, secret) {
      checkIsSecretKey(secret);
      thing = normalizeInput(thing);
      var hmac = crypto2.createHmac("sha" + bits, secret);
      var sig = (hmac.update(thing), hmac.digest("base64"));
      return fromBase64(sig);
    };
  };
  var createHmacVerifier = function(bits) {
    return function verify(thing, signature, secret) {
      var computedSig = createHmacSigner(bits)(thing, secret);
      return bufferEqual(Buffer2.from(signature), Buffer2.from(computedSig));
    };
  };
  var createKeySigner = function(bits) {
    return function sign(thing, privateKey) {
      checkIsPrivateKey(privateKey);
      thing = normalizeInput(thing);
      var signer = crypto2.createSign("RSA-SHA" + bits);
      var sig = (signer.update(thing), signer.sign(privateKey, "base64"));
      return fromBase64(sig);
    };
  };
  var createKeyVerifier = function(bits) {
    return function verify(thing, signature, publicKey) {
      checkIsPublicKey(publicKey);
      thing = normalizeInput(thing);
      signature = toBase64(signature);
      var verifier = crypto2.createVerify("RSA-SHA" + bits);
      verifier.update(thing);
      return verifier.verify(publicKey, signature, "base64");
    };
  };
  var createPSSKeySigner = function(bits) {
    return function sign(thing, privateKey) {
      checkIsPrivateKey(privateKey);
      thing = normalizeInput(thing);
      var signer = crypto2.createSign("RSA-SHA" + bits);
      var sig = (signer.update(thing), signer.sign({
        key: privateKey,
        padding: crypto2.constants.RSA_PKCS1_PSS_PADDING,
        saltLength: crypto2.constants.RSA_PSS_SALTLEN_DIGEST
      }, "base64"));
      return fromBase64(sig);
    };
  };
  var createPSSKeyVerifier = function(bits) {
    return function verify(thing, signature, publicKey) {
      checkIsPublicKey(publicKey);
      thing = normalizeInput(thing);
      signature = toBase64(signature);
      var verifier = crypto2.createVerify("RSA-SHA" + bits);
      verifier.update(thing);
      return verifier.verify({
        key: publicKey,
        padding: crypto2.constants.RSA_PKCS1_PSS_PADDING,
        saltLength: crypto2.constants.RSA_PSS_SALTLEN_DIGEST
      }, signature, "base64");
    };
  };
  var createECDSASigner = function(bits) {
    var inner = createKeySigner(bits);
    return function sign() {
      var signature = inner.apply(null, arguments);
      signature = formatEcdsa.derToJose(signature, "ES" + bits);
      return signature;
    };
  };
  var createECDSAVerifer = function(bits) {
    var inner = createKeyVerifier(bits);
    return function verify(thing, signature, publicKey) {
      signature = formatEcdsa.joseToDer(signature, "ES" + bits).toString("base64");
      var result = inner(thing, signature, publicKey);
      return result;
    };
  };
  var createNoneSigner = function() {
    return function sign() {
      return "";
    };
  };
  var createNoneVerifier = function() {
    return function verify(thing, signature) {
      return signature === "";
    };
  };
  var bufferEqual = require_buffer_equal_constant_time();
  var Buffer2 = require_safe_buffer().Buffer;
  var crypto2 = import.meta.require("crypto");
  var formatEcdsa = require_ecdsa_sig_formatter();
  var util = import.meta.require("util");
  var MSG_INVALID_ALGORITHM = '"%s" is not a valid algorithm.\n  Supported algorithms are:\n  "HS256", "HS384", "HS512", "RS256", "RS384", "RS512", "PS256", "PS384", "PS512", "ES256", "ES384", "ES512" and "none".';
  var MSG_INVALID_SECRET = "secret must be a string or buffer";
  var MSG_INVALID_VERIFIER_KEY = "key must be a string or a buffer";
  var MSG_INVALID_SIGNER_KEY = "key must be a string, a buffer or an object";
  var supportsKeyObjects = typeof crypto2.createPublicKey === "function";
  if (supportsKeyObjects) {
    MSG_INVALID_VERIFIER_KEY += " or a KeyObject";
    MSG_INVALID_SECRET += "or a KeyObject";
  }
  module.exports = function jwa(algorithm) {
    var signerFactories = {
      hs: createHmacSigner,
      rs: createKeySigner,
      ps: createPSSKeySigner,
      es: createECDSASigner,
      none: createNoneSigner
    };
    var verifierFactories = {
      hs: createHmacVerifier,
      rs: createKeyVerifier,
      ps: createPSSKeyVerifier,
      es: createECDSAVerifer,
      none: createNoneVerifier
    };
    var match = algorithm.match(/^(RS|PS|ES|HS)(256|384|512)$|^(none)$/i);
    if (!match)
      throw typeError(MSG_INVALID_ALGORITHM, algorithm);
    var algo = (match[1] || match[3]).toLowerCase();
    var bits = match[2];
    return {
      sign: signerFactories[algo](bits),
      verify: verifierFactories[algo](bits)
    };
  };
});

// node_modules/jws/lib/tostring.js
var require_tostring = __commonJS((exports, module) => {
  var Buffer2 = import.meta.require("buffer").Buffer;
  module.exports = function toString(obj) {
    if (typeof obj === "string")
      return obj;
    if (typeof obj === "number" || Buffer2.isBuffer(obj))
      return obj.toString();
    return JSON.stringify(obj);
  };
});

// node_modules/jws/lib/sign-stream.js
var require_sign_stream = __commonJS((exports, module) => {
  var base64url = function(string, encoding) {
    return Buffer2.from(string, encoding).toString("base64").replace(/=/g, "").replace(/\+/g, "-").replace(/\//g, "_");
  };
  var jwsSecuredInput = function(header, payload, encoding) {
    encoding = encoding || "utf8";
    var encodedHeader = base64url(toString(header), "binary");
    var encodedPayload = base64url(toString(payload), encoding);
    return util.format("%s.%s", encodedHeader, encodedPayload);
  };
  var jwsSign = function(opts) {
    var header = opts.header;
    var payload = opts.payload;
    var secretOrKey = opts.secret || opts.privateKey;
    var encoding = opts.encoding;
    var algo = jwa(header.alg);
    var securedInput = jwsSecuredInput(header, payload, encoding);
    var signature = algo.sign(securedInput, secretOrKey);
    return util.format("%s.%s", securedInput, signature);
  };
  var SignStream = function(opts) {
    var secret = opts.secret || opts.privateKey || opts.key;
    var secretStream = new DataStream(secret);
    this.readable = true;
    this.header = opts.header;
    this.encoding = opts.encoding;
    this.secret = this.privateKey = this.key = secretStream;
    this.payload = new DataStream(opts.payload);
    this.secret.once("close", function() {
      if (!this.payload.writable && this.readable)
        this.sign();
    }.bind(this));
    this.payload.once("close", function() {
      if (!this.secret.writable && this.readable)
        this.sign();
    }.bind(this));
  };
  var Buffer2 = require_safe_buffer().Buffer;
  var DataStream = require_data_stream();
  var jwa = require_jwa();
  var Stream = import.meta.require("stream");
  var toString = require_tostring();
  var util = import.meta.require("util");
  util.inherits(SignStream, Stream);
  SignStream.prototype.sign = function sign() {
    try {
      var signature = jwsSign({
        header: this.header,
        payload: this.payload.buffer,
        secret: this.secret.buffer,
        encoding: this.encoding
      });
      this.emit("done", signature);
      this.emit("data", signature);
      this.emit("end");
      this.readable = false;
      return signature;
    } catch (e) {
      this.readable = false;
      this.emit("error", e);
      this.emit("close");
    }
  };
  SignStream.sign = jwsSign;
  module.exports = SignStream;
});

// node_modules/jws/lib/verify-stream.js
var require_verify_stream = __commonJS((exports, module) => {
  var isObject = function(thing) {
    return Object.prototype.toString.call(thing) === "[object Object]";
  };
  var safeJsonParse = function(thing) {
    if (isObject(thing))
      return thing;
    try {
      return JSON.parse(thing);
    } catch (e) {
      return;
    }
  };
  var headerFromJWS = function(jwsSig) {
    var encodedHeader = jwsSig.split(".", 1)[0];
    return safeJsonParse(Buffer2.from(encodedHeader, "base64").toString("binary"));
  };
  var securedInputFromJWS = function(jwsSig) {
    return jwsSig.split(".", 2).join(".");
  };
  var signatureFromJWS = function(jwsSig) {
    return jwsSig.split(".")[2];
  };
  var payloadFromJWS = function(jwsSig, encoding) {
    encoding = encoding || "utf8";
    var payload = jwsSig.split(".")[1];
    return Buffer2.from(payload, "base64").toString(encoding);
  };
  var isValidJws = function(string) {
    return JWS_REGEX.test(string) && !!headerFromJWS(string);
  };
  var jwsVerify = function(jwsSig, algorithm, secretOrKey) {
    if (!algorithm) {
      var err = new Error("Missing algorithm parameter for jws.verify");
      err.code = "MISSING_ALGORITHM";
      throw err;
    }
    jwsSig = toString(jwsSig);
    var signature = signatureFromJWS(jwsSig);
    var securedInput = securedInputFromJWS(jwsSig);
    var algo = jwa(algorithm);
    return algo.verify(securedInput, signature, secretOrKey);
  };
  var jwsDecode = function(jwsSig, opts) {
    opts = opts || {};
    jwsSig = toString(jwsSig);
    if (!isValidJws(jwsSig))
      return null;
    var header = headerFromJWS(jwsSig);
    if (!header)
      return null;
    var payload = payloadFromJWS(jwsSig);
    if (header.typ === "JWT" || opts.json)
      payload = JSON.parse(payload, opts.encoding);
    return {
      header,
      payload,
      signature: signatureFromJWS(jwsSig)
    };
  };
  var VerifyStream = function(opts) {
    opts = opts || {};
    var secretOrKey = opts.secret || opts.publicKey || opts.key;
    var secretStream = new DataStream(secretOrKey);
    this.readable = true;
    this.algorithm = opts.algorithm;
    this.encoding = opts.encoding;
    this.secret = this.publicKey = this.key = secretStream;
    this.signature = new DataStream(opts.signature);
    this.secret.once("close", function() {
      if (!this.signature.writable && this.readable)
        this.verify();
    }.bind(this));
    this.signature.once("close", function() {
      if (!this.secret.writable && this.readable)
        this.verify();
    }.bind(this));
  };
  var Buffer2 = require_safe_buffer().Buffer;
  var DataStream = require_data_stream();
  var jwa = require_jwa();
  var Stream = import.meta.require("stream");
  var toString = require_tostring();
  var util = import.meta.require("util");
  var JWS_REGEX = /^[a-zA-Z0-9\-_]+?\.[a-zA-Z0-9\-_]+?\.([a-zA-Z0-9\-_]+)?$/;
  util.inherits(VerifyStream, Stream);
  VerifyStream.prototype.verify = function verify() {
    try {
      var valid = jwsVerify(this.signature.buffer, this.algorithm, this.key.buffer);
      var obj = jwsDecode(this.signature.buffer, this.encoding);
      this.emit("done", valid, obj);
      this.emit("data", valid);
      this.emit("end");
      this.readable = false;
      return valid;
    } catch (e) {
      this.readable = false;
      this.emit("error", e);
      this.emit("close");
    }
  };
  VerifyStream.decode = jwsDecode;
  VerifyStream.isValid = isValidJws;
  VerifyStream.verify = jwsVerify;
  module.exports = VerifyStream;
});

// node_modules/jws/index.js
var require_jws = __commonJS((exports) => {
  var SignStream = require_sign_stream();
  var VerifyStream = require_verify_stream();
  var ALGORITHMS = [
    "HS256",
    "HS384",
    "HS512",
    "RS256",
    "RS384",
    "RS512",
    "PS256",
    "PS384",
    "PS512",
    "ES256",
    "ES384",
    "ES512"
  ];
  exports.ALGORITHMS = ALGORITHMS;
  exports.sign = SignStream.sign;
  exports.verify = VerifyStream.verify;
  exports.decode = VerifyStream.decode;
  exports.isValid = VerifyStream.isValid;
  exports.createSign = function createSign(opts) {
    return new SignStream(opts);
  };
  exports.createVerify = function createVerify(opts) {
    return new VerifyStream(opts);
  };
});

// node_modules/jsonwebtoken/decode.js
var require_decode = __commonJS((exports, module) => {
  var jws = require_jws();
  module.exports = function(jwt, options) {
    options = options || {};
    var decoded = jws.decode(jwt, options);
    if (!decoded) {
      return null;
    }
    var payload = decoded.payload;
    if (typeof payload === "string") {
      try {
        var obj = JSON.parse(payload);
        if (obj !== null && typeof obj === "object") {
          payload = obj;
        }
      } catch (e) {
      }
    }
    if (options.complete === true) {
      return {
        header: decoded.header,
        payload,
        signature: decoded.signature
      };
    }
    return payload;
  };
});

// node_modules/jsonwebtoken/lib/JsonWebTokenError.js
var require_JsonWebTokenError = __commonJS((exports, module) => {
  var JsonWebTokenError = function(message, error) {
    Error.call(this, message);
    if (Error.captureStackTrace) {
      Error.captureStackTrace(this, this.constructor);
    }
    this.name = "JsonWebTokenError";
    this.message = message;
    if (error)
      this.inner = error;
  };
  JsonWebTokenError.prototype = Object.create(Error.prototype);
  JsonWebTokenError.prototype.constructor = JsonWebTokenError;
  module.exports = JsonWebTokenError;
});

// node_modules/jsonwebtoken/lib/NotBeforeError.js
var require_NotBeforeError = __commonJS((exports, module) => {
  var JsonWebTokenError = require_JsonWebTokenError();
  var NotBeforeError = function(message, date) {
    JsonWebTokenError.call(this, message);
    this.name = "NotBeforeError";
    this.date = date;
  };
  NotBeforeError.prototype = Object.create(JsonWebTokenError.prototype);
  NotBeforeError.prototype.constructor = NotBeforeError;
  module.exports = NotBeforeError;
});

// node_modules/jsonwebtoken/lib/TokenExpiredError.js
var require_TokenExpiredError = __commonJS((exports, module) => {
  var JsonWebTokenError = require_JsonWebTokenError();
  var TokenExpiredError = function(message, expiredAt) {
    JsonWebTokenError.call(this, message);
    this.name = "TokenExpiredError";
    this.expiredAt = expiredAt;
  };
  TokenExpiredError.prototype = Object.create(JsonWebTokenError.prototype);
  TokenExpiredError.prototype.constructor = TokenExpiredError;
  module.exports = TokenExpiredError;
});

// node_modules/ms/index.js
var require_ms = __commonJS((exports, module) => {
  var parse2 = function(str) {
    str = String(str);
    if (str.length > 100) {
      return;
    }
    var match = /^(-?(?:\d+)?\.?\d+) *(milliseconds?|msecs?|ms|seconds?|secs?|s|minutes?|mins?|m|hours?|hrs?|h|days?|d|weeks?|w|years?|yrs?|y)?$/i.exec(str);
    if (!match) {
      return;
    }
    var n = parseFloat(match[1]);
    var type = (match[2] || "ms").toLowerCase();
    switch (type) {
      case "years":
      case "year":
      case "yrs":
      case "yr":
      case "y":
        return n * y;
      case "weeks":
      case "week":
      case "w":
        return n * w;
      case "days":
      case "day":
      case "d":
        return n * d;
      case "hours":
      case "hour":
      case "hrs":
      case "hr":
      case "h":
        return n * h;
      case "minutes":
      case "minute":
      case "mins":
      case "min":
      case "m":
        return n * m;
      case "seconds":
      case "second":
      case "secs":
      case "sec":
      case "s":
        return n * s;
      case "milliseconds":
      case "millisecond":
      case "msecs":
      case "msec":
      case "ms":
        return n;
      default:
        return;
    }
  };
  var fmtShort = function(ms) {
    var msAbs = Math.abs(ms);
    if (msAbs >= d) {
      return Math.round(ms / d) + "d";
    }
    if (msAbs >= h) {
      return Math.round(ms / h) + "h";
    }
    if (msAbs >= m) {
      return Math.round(ms / m) + "m";
    }
    if (msAbs >= s) {
      return Math.round(ms / s) + "s";
    }
    return ms + "ms";
  };
  var fmtLong = function(ms) {
    var msAbs = Math.abs(ms);
    if (msAbs >= d) {
      return plural(ms, msAbs, d, "day");
    }
    if (msAbs >= h) {
      return plural(ms, msAbs, h, "hour");
    }
    if (msAbs >= m) {
      return plural(ms, msAbs, m, "minute");
    }
    if (msAbs >= s) {
      return plural(ms, msAbs, s, "second");
    }
    return ms + " ms";
  };
  var plural = function(ms, msAbs, n, name) {
    var isPlural = msAbs >= n * 1.5;
    return Math.round(ms / n) + " " + name + (isPlural ? "s" : "");
  };
  var s = 1000;
  var m = s * 60;
  var h = m * 60;
  var d = h * 24;
  var w = d * 7;
  var y = d * 365.25;
  module.exports = function(val, options) {
    options = options || {};
    var type = typeof val;
    if (type === "string" && val.length > 0) {
      return parse2(val);
    } else if (type === "number" && isFinite(val)) {
      return options.long ? fmtLong(val) : fmtShort(val);
    }
    throw new Error("val is not a non-empty string or a valid number. val=" + JSON.stringify(val));
  };
});

// node_modules/jsonwebtoken/lib/timespan.js
var require_timespan = __commonJS((exports, module) => {
  var ms = require_ms();
  module.exports = function(time, iat) {
    var timestamp = iat || Math.floor(Date.now() / 1000);
    if (typeof time === "string") {
      var milliseconds = ms(time);
      if (typeof milliseconds === "undefined") {
        return;
      }
      return Math.floor(timestamp + milliseconds / 1000);
    } else if (typeof time === "number") {
      return timestamp + time;
    } else {
      return;
    }
  };
});

// node_modules/semver/internal/constants.js
var require_constants3 = __commonJS((exports, module) => {
  var SEMVER_SPEC_VERSION = "2.0.0";
  var MAX_LENGTH = 256;
  var MAX_SAFE_INTEGER = Number.MAX_SAFE_INTEGER || 9007199254740991;
  var MAX_SAFE_COMPONENT_LENGTH = 16;
  var MAX_SAFE_BUILD_LENGTH = MAX_LENGTH - 6;
  var RELEASE_TYPES = [
    "major",
    "premajor",
    "minor",
    "preminor",
    "patch",
    "prepatch",
    "prerelease"
  ];
  module.exports = {
    MAX_LENGTH,
    MAX_SAFE_COMPONENT_LENGTH,
    MAX_SAFE_BUILD_LENGTH,
    MAX_SAFE_INTEGER,
    RELEASE_TYPES,
    SEMVER_SPEC_VERSION,
    FLAG_INCLUDE_PRERELEASE: 1,
    FLAG_LOOSE: 2
  };
});

// node_modules/semver/internal/debug.js
var require_debug = __commonJS((exports, module) => {
  var debug = typeof process === "object" && process.env && process.env.NODE_DEBUG && /\bsemver\b/i.test(process.env.NODE_DEBUG) ? (...args) => console.error("SEMVER", ...args) : () => {
  };
  module.exports = debug;
});

// node_modules/semver/internal/re.js
var require_re = __commonJS((exports, module) => {
  var {
    MAX_SAFE_COMPONENT_LENGTH,
    MAX_SAFE_BUILD_LENGTH,
    MAX_LENGTH
  } = require_constants3();
  var debug = require_debug();
  exports = module.exports = {};
  var re = exports.re = [];
  var safeRe = exports.safeRe = [];
  var src = exports.src = [];
  var t = exports.t = {};
  var R = 0;
  var LETTERDASHNUMBER = "[a-zA-Z0-9-]";
  var safeRegexReplacements = [
    ["\\s", 1],
    ["\\d", MAX_LENGTH],
    [LETTERDASHNUMBER, MAX_SAFE_BUILD_LENGTH]
  ];
  var makeSafeRegex = (value) => {
    for (const [token, max] of safeRegexReplacements) {
      value = value.split(`${token}*`).join(`${token}{0,${max}}`).split(`${token}+`).join(`${token}{1,${max}}`);
    }
    return value;
  };
  var createToken = (name, value, isGlobal) => {
    const safe = makeSafeRegex(value);
    const index = R++;
    debug(name, index, value);
    t[name] = index;
    src[index] = value;
    re[index] = new RegExp(value, isGlobal ? "g" : undefined);
    safeRe[index] = new RegExp(safe, isGlobal ? "g" : undefined);
  };
  createToken("NUMERICIDENTIFIER", "0|[1-9]\\d*");
  createToken("NUMERICIDENTIFIERLOOSE", "\\d+");
  createToken("NONNUMERICIDENTIFIER", `\\d*[a-zA-Z-]${LETTERDASHNUMBER}*`);
  createToken("MAINVERSION", `(${src[t.NUMERICIDENTIFIER]})\\.` + `(${src[t.NUMERICIDENTIFIER]})\\.` + `(${src[t.NUMERICIDENTIFIER]})`);
  createToken("MAINVERSIONLOOSE", `(${src[t.NUMERICIDENTIFIERLOOSE]})\\.` + `(${src[t.NUMERICIDENTIFIERLOOSE]})\\.` + `(${src[t.NUMERICIDENTIFIERLOOSE]})`);
  createToken("PRERELEASEIDENTIFIER", `(?:${src[t.NUMERICIDENTIFIER]}|${src[t.NONNUMERICIDENTIFIER]})`);
  createToken("PRERELEASEIDENTIFIERLOOSE", `(?:${src[t.NUMERICIDENTIFIERLOOSE]}|${src[t.NONNUMERICIDENTIFIER]})`);
  createToken("PRERELEASE", `(?:-(${src[t.PRERELEASEIDENTIFIER]}(?:\\.${src[t.PRERELEASEIDENTIFIER]})*))`);
  createToken("PRERELEASELOOSE", `(?:-?(${src[t.PRERELEASEIDENTIFIERLOOSE]}(?:\\.${src[t.PRERELEASEIDENTIFIERLOOSE]})*))`);
  createToken("BUILDIDENTIFIER", `${LETTERDASHNUMBER}+`);
  createToken("BUILD", `(?:\\+(${src[t.BUILDIDENTIFIER]}(?:\\.${src[t.BUILDIDENTIFIER]})*))`);
  createToken("FULLPLAIN", `v?${src[t.MAINVERSION]}${src[t.PRERELEASE]}?${src[t.BUILD]}?`);
  createToken("FULL", `^${src[t.FULLPLAIN]}\$`);
  createToken("LOOSEPLAIN", `[v=\\s]*${src[t.MAINVERSIONLOOSE]}${src[t.PRERELEASELOOSE]}?${src[t.BUILD]}?`);
  createToken("LOOSE", `^${src[t.LOOSEPLAIN]}\$`);
  createToken("GTLT", "((?:<|>)?=?)");
  createToken("XRANGEIDENTIFIERLOOSE", `${src[t.NUMERICIDENTIFIERLOOSE]}|x|X|\\*`);
  createToken("XRANGEIDENTIFIER", `${src[t.NUMERICIDENTIFIER]}|x|X|\\*`);
  createToken("XRANGEPLAIN", `[v=\\s]*(${src[t.XRANGEIDENTIFIER]})` + `(?:\\.(${src[t.XRANGEIDENTIFIER]})` + `(?:\\.(${src[t.XRANGEIDENTIFIER]})` + `(?:${src[t.PRERELEASE]})?${src[t.BUILD]}?` + `)?)?`);
  createToken("XRANGEPLAINLOOSE", `[v=\\s]*(${src[t.XRANGEIDENTIFIERLOOSE]})` + `(?:\\.(${src[t.XRANGEIDENTIFIERLOOSE]})` + `(?:\\.(${src[t.XRANGEIDENTIFIERLOOSE]})` + `(?:${src[t.PRERELEASELOOSE]})?${src[t.BUILD]}?` + `)?)?`);
  createToken("XRANGE", `^${src[t.GTLT]}\\s*${src[t.XRANGEPLAIN]}\$`);
  createToken("XRANGELOOSE", `^${src[t.GTLT]}\\s*${src[t.XRANGEPLAINLOOSE]}\$`);
  createToken("COERCEPLAIN", `${"(^|[^\\d])(\\d{1,"}${MAX_SAFE_COMPONENT_LENGTH}})` + `(?:\\.(\\d{1,${MAX_SAFE_COMPONENT_LENGTH}}))?` + `(?:\\.(\\d{1,${MAX_SAFE_COMPONENT_LENGTH}}))?`);
  createToken("COERCE", `${src[t.COERCEPLAIN]}(?:\$|[^\\d])`);
  createToken("COERCEFULL", src[t.COERCEPLAIN] + `(?:${src[t.PRERELEASE]})?` + `(?:${src[t.BUILD]})?` + `(?:\$|[^\\d])`);
  createToken("COERCERTL", src[t.COERCE], true);
  createToken("COERCERTLFULL", src[t.COERCEFULL], true);
  createToken("LONETILDE", "(?:~>?)");
  createToken("TILDETRIM", `(\\s*)${src[t.LONETILDE]}\\s+`, true);
  exports.tildeTrimReplace = "$1~";
  createToken("TILDE", `^${src[t.LONETILDE]}${src[t.XRANGEPLAIN]}\$`);
  createToken("TILDELOOSE", `^${src[t.LONETILDE]}${src[t.XRANGEPLAINLOOSE]}\$`);
  createToken("LONECARET", "(?:\\^)");
  createToken("CARETTRIM", `(\\s*)${src[t.LONECARET]}\\s+`, true);
  exports.caretTrimReplace = "$1^";
  createToken("CARET", `^${src[t.LONECARET]}${src[t.XRANGEPLAIN]}\$`);
  createToken("CARETLOOSE", `^${src[t.LONECARET]}${src[t.XRANGEPLAINLOOSE]}\$`);
  createToken("COMPARATORLOOSE", `^${src[t.GTLT]}\\s*(${src[t.LOOSEPLAIN]})\$|^\$`);
  createToken("COMPARATOR", `^${src[t.GTLT]}\\s*(${src[t.FULLPLAIN]})\$|^\$`);
  createToken("COMPARATORTRIM", `(\\s*)${src[t.GTLT]}\\s*(${src[t.LOOSEPLAIN]}|${src[t.XRANGEPLAIN]})`, true);
  exports.comparatorTrimReplace = "$1$2$3";
  createToken("HYPHENRANGE", `^\\s*(${src[t.XRANGEPLAIN]})` + `\\s+-\\s+` + `(${src[t.XRANGEPLAIN]})` + `\\s*\$`);
  createToken("HYPHENRANGELOOSE", `^\\s*(${src[t.XRANGEPLAINLOOSE]})` + `\\s+-\\s+` + `(${src[t.XRANGEPLAINLOOSE]})` + `\\s*\$`);
  createToken("STAR", "(<|>)?=?\\s*\\*");
  createToken("GTE0", "^\\s*>=\\s*0\\.0\\.0\\s*$");
  createToken("GTE0PRE", "^\\s*>=\\s*0\\.0\\.0-0\\s*$");
});

// node_modules/semver/internal/parse-options.js
var require_parse_options = __commonJS((exports, module) => {
  var looseOption = Object.freeze({ loose: true });
  var emptyOpts = Object.freeze({});
  var parseOptions = (options) => {
    if (!options) {
      return emptyOpts;
    }
    if (typeof options !== "object") {
      return looseOption;
    }
    return options;
  };
  module.exports = parseOptions;
});

// node_modules/semver/internal/identifiers.js
var require_identifiers = __commonJS((exports, module) => {
  var numeric = /^[0-9]+$/;
  var compareIdentifiers = (a, b) => {
    const anum = numeric.test(a);
    const bnum = numeric.test(b);
    if (anum && bnum) {
      a = +a;
      b = +b;
    }
    return a === b ? 0 : anum && !bnum ? -1 : bnum && !anum ? 1 : a < b ? -1 : 1;
  };
  var rcompareIdentifiers = (a, b) => compareIdentifiers(b, a);
  module.exports = {
    compareIdentifiers,
    rcompareIdentifiers
  };
});

// node_modules/semver/classes/semver.js
var require_semver = __commonJS((exports, module) => {
  var debug = require_debug();
  var { MAX_LENGTH, MAX_SAFE_INTEGER } = require_constants3();
  var { safeRe: re, t } = require_re();
  var parseOptions = require_parse_options();
  var { compareIdentifiers } = require_identifiers();

  class SemVer {
    constructor(version, options) {
      options = parseOptions(options);
      if (version instanceof SemVer) {
        if (version.loose === !!options.loose && version.includePrerelease === !!options.includePrerelease) {
          return version;
        } else {
          version = version.version;
        }
      } else if (typeof version !== "string") {
        throw new TypeError(`Invalid version. Must be a string. Got type "${typeof version}".`);
      }
      if (version.length > MAX_LENGTH) {
        throw new TypeError(`version is longer than ${MAX_LENGTH} characters`);
      }
      debug("SemVer", version, options);
      this.options = options;
      this.loose = !!options.loose;
      this.includePrerelease = !!options.includePrerelease;
      const m = version.trim().match(options.loose ? re[t.LOOSE] : re[t.FULL]);
      if (!m) {
        throw new TypeError(`Invalid Version: ${version}`);
      }
      this.raw = version;
      this.major = +m[1];
      this.minor = +m[2];
      this.patch = +m[3];
      if (this.major > MAX_SAFE_INTEGER || this.major < 0) {
        throw new TypeError("Invalid major version");
      }
      if (this.minor > MAX_SAFE_INTEGER || this.minor < 0) {
        throw new TypeError("Invalid minor version");
      }
      if (this.patch > MAX_SAFE_INTEGER || this.patch < 0) {
        throw new TypeError("Invalid patch version");
      }
      if (!m[4]) {
        this.prerelease = [];
      } else {
        this.prerelease = m[4].split(".").map((id) => {
          if (/^[0-9]+$/.test(id)) {
            const num = +id;
            if (num >= 0 && num < MAX_SAFE_INTEGER) {
              return num;
            }
          }
          return id;
        });
      }
      this.build = m[5] ? m[5].split(".") : [];
      this.format();
    }
    format() {
      this.version = `${this.major}.${this.minor}.${this.patch}`;
      if (this.prerelease.length) {
        this.version += `-${this.prerelease.join(".")}`;
      }
      return this.version;
    }
    toString() {
      return this.version;
    }
    compare(other) {
      debug("SemVer.compare", this.version, this.options, other);
      if (!(other instanceof SemVer)) {
        if (typeof other === "string" && other === this.version) {
          return 0;
        }
        other = new SemVer(other, this.options);
      }
      if (other.version === this.version) {
        return 0;
      }
      return this.compareMain(other) || this.comparePre(other);
    }
    compareMain(other) {
      if (!(other instanceof SemVer)) {
        other = new SemVer(other, this.options);
      }
      return compareIdentifiers(this.major, other.major) || compareIdentifiers(this.minor, other.minor) || compareIdentifiers(this.patch, other.patch);
    }
    comparePre(other) {
      if (!(other instanceof SemVer)) {
        other = new SemVer(other, this.options);
      }
      if (this.prerelease.length && !other.prerelease.length) {
        return -1;
      } else if (!this.prerelease.length && other.prerelease.length) {
        return 1;
      } else if (!this.prerelease.length && !other.prerelease.length) {
        return 0;
      }
      let i = 0;
      do {
        const a = this.prerelease[i];
        const b = other.prerelease[i];
        debug("prerelease compare", i, a, b);
        if (a === undefined && b === undefined) {
          return 0;
        } else if (b === undefined) {
          return 1;
        } else if (a === undefined) {
          return -1;
        } else if (a === b) {
          continue;
        } else {
          return compareIdentifiers(a, b);
        }
      } while (++i);
    }
    compareBuild(other) {
      if (!(other instanceof SemVer)) {
        other = new SemVer(other, this.options);
      }
      let i = 0;
      do {
        const a = this.build[i];
        const b = other.build[i];
        debug("prerelease compare", i, a, b);
        if (a === undefined && b === undefined) {
          return 0;
        } else if (b === undefined) {
          return 1;
        } else if (a === undefined) {
          return -1;
        } else if (a === b) {
          continue;
        } else {
          return compareIdentifiers(a, b);
        }
      } while (++i);
    }
    inc(release, identifier, identifierBase) {
      switch (release) {
        case "premajor":
          this.prerelease.length = 0;
          this.patch = 0;
          this.minor = 0;
          this.major++;
          this.inc("pre", identifier, identifierBase);
          break;
        case "preminor":
          this.prerelease.length = 0;
          this.patch = 0;
          this.minor++;
          this.inc("pre", identifier, identifierBase);
          break;
        case "prepatch":
          this.prerelease.length = 0;
          this.inc("patch", identifier, identifierBase);
          this.inc("pre", identifier, identifierBase);
          break;
        case "prerelease":
          if (this.prerelease.length === 0) {
            this.inc("patch", identifier, identifierBase);
          }
          this.inc("pre", identifier, identifierBase);
          break;
        case "major":
          if (this.minor !== 0 || this.patch !== 0 || this.prerelease.length === 0) {
            this.major++;
          }
          this.minor = 0;
          this.patch = 0;
          this.prerelease = [];
          break;
        case "minor":
          if (this.patch !== 0 || this.prerelease.length === 0) {
            this.minor++;
          }
          this.patch = 0;
          this.prerelease = [];
          break;
        case "patch":
          if (this.prerelease.length === 0) {
            this.patch++;
          }
          this.prerelease = [];
          break;
        case "pre": {
          const base = Number(identifierBase) ? 1 : 0;
          if (!identifier && identifierBase === false) {
            throw new Error("invalid increment argument: identifier is empty");
          }
          if (this.prerelease.length === 0) {
            this.prerelease = [base];
          } else {
            let i = this.prerelease.length;
            while (--i >= 0) {
              if (typeof this.prerelease[i] === "number") {
                this.prerelease[i]++;
                i = -2;
              }
            }
            if (i === -1) {
              if (identifier === this.prerelease.join(".") && identifierBase === false) {
                throw new Error("invalid increment argument: identifier already exists");
              }
              this.prerelease.push(base);
            }
          }
          if (identifier) {
            let prerelease = [identifier, base];
            if (identifierBase === false) {
              prerelease = [identifier];
            }
            if (compareIdentifiers(this.prerelease[0], identifier) === 0) {
              if (isNaN(this.prerelease[1])) {
                this.prerelease = prerelease;
              }
            } else {
              this.prerelease = prerelease;
            }
          }
          break;
        }
        default:
          throw new Error(`invalid increment argument: ${release}`);
      }
      this.raw = this.format();
      if (this.build.length) {
        this.raw += `+${this.build.join(".")}`;
      }
      return this;
    }
  }
  module.exports = SemVer;
});

// node_modules/semver/functions/parse.js
var require_parse2 = __commonJS((exports, module) => {
  var SemVer = require_semver();
  var parse2 = (version, options, throwErrors = false) => {
    if (version instanceof SemVer) {
      return version;
    }
    try {
      return new SemVer(version, options);
    } catch (er) {
      if (!throwErrors) {
        return null;
      }
      throw er;
    }
  };
  module.exports = parse2;
});

// node_modules/semver/functions/valid.js
var require_valid = __commonJS((exports, module) => {
  var parse2 = require_parse2();
  var valid = (version, options) => {
    const v = parse2(version, options);
    return v ? v.version : null;
  };
  module.exports = valid;
});

// node_modules/semver/functions/clean.js
var require_clean = __commonJS((exports, module) => {
  var parse2 = require_parse2();
  var clean = (version, options) => {
    const s = parse2(version.trim().replace(/^[=v]+/, ""), options);
    return s ? s.version : null;
  };
  module.exports = clean;
});

// node_modules/semver/functions/inc.js
var require_inc = __commonJS((exports, module) => {
  var SemVer = require_semver();
  var inc = (version, release, options, identifier, identifierBase) => {
    if (typeof options === "string") {
      identifierBase = identifier;
      identifier = options;
      options = undefined;
    }
    try {
      return new SemVer(version instanceof SemVer ? version.version : version, options).inc(release, identifier, identifierBase).version;
    } catch (er) {
      return null;
    }
  };
  module.exports = inc;
});

// node_modules/semver/functions/diff.js
var require_diff = __commonJS((exports, module) => {
  var parse2 = require_parse2();
  var diff = (version1, version2) => {
    const v1 = parse2(version1, null, true);
    const v2 = parse2(version2, null, true);
    const comparison = v1.compare(v2);
    if (comparison === 0) {
      return null;
    }
    const v1Higher = comparison > 0;
    const highVersion = v1Higher ? v1 : v2;
    const lowVersion = v1Higher ? v2 : v1;
    const highHasPre = !!highVersion.prerelease.length;
    const lowHasPre = !!lowVersion.prerelease.length;
    if (lowHasPre && !highHasPre) {
      if (!lowVersion.patch && !lowVersion.minor) {
        return "major";
      }
      if (highVersion.patch) {
        return "patch";
      }
      if (highVersion.minor) {
        return "minor";
      }
      return "major";
    }
    const prefix = highHasPre ? "pre" : "";
    if (v1.major !== v2.major) {
      return prefix + "major";
    }
    if (v1.minor !== v2.minor) {
      return prefix + "minor";
    }
    if (v1.patch !== v2.patch) {
      return prefix + "patch";
    }
    return "prerelease";
  };
  module.exports = diff;
});

// node_modules/semver/functions/major.js
var require_major = __commonJS((exports, module) => {
  var SemVer = require_semver();
  var major = (a, loose) => new SemVer(a, loose).major;
  module.exports = major;
});

// node_modules/semver/functions/minor.js
var require_minor = __commonJS((exports, module) => {
  var SemVer = require_semver();
  var minor = (a, loose) => new SemVer(a, loose).minor;
  module.exports = minor;
});

// node_modules/semver/functions/patch.js
var require_patch = __commonJS((exports, module) => {
  var SemVer = require_semver();
  var patch = (a, loose) => new SemVer(a, loose).patch;
  module.exports = patch;
});

// node_modules/semver/functions/prerelease.js
var require_prerelease = __commonJS((exports, module) => {
  var parse2 = require_parse2();
  var prerelease = (version, options) => {
    const parsed = parse2(version, options);
    return parsed && parsed.prerelease.length ? parsed.prerelease : null;
  };
  module.exports = prerelease;
});

// node_modules/semver/functions/compare.js
var require_compare = __commonJS((exports, module) => {
  var SemVer = require_semver();
  var compare = (a, b, loose) => new SemVer(a, loose).compare(new SemVer(b, loose));
  module.exports = compare;
});

// node_modules/semver/functions/rcompare.js
var require_rcompare = __commonJS((exports, module) => {
  var compare = require_compare();
  var rcompare = (a, b, loose) => compare(b, a, loose);
  module.exports = rcompare;
});

// node_modules/semver/functions/compare-loose.js
var require_compare_loose = __commonJS((exports, module) => {
  var compare = require_compare();
  var compareLoose = (a, b) => compare(a, b, true);
  module.exports = compareLoose;
});

// node_modules/semver/functions/compare-build.js
var require_compare_build = __commonJS((exports, module) => {
  var SemVer = require_semver();
  var compareBuild = (a, b, loose) => {
    const versionA = new SemVer(a, loose);
    const versionB = new SemVer(b, loose);
    return versionA.compare(versionB) || versionA.compareBuild(versionB);
  };
  module.exports = compareBuild;
});

// node_modules/semver/functions/sort.js
var require_sort = __commonJS((exports, module) => {
  var compareBuild = require_compare_build();
  var sort = (list, loose) => list.sort((a, b) => compareBuild(a, b, loose));
  module.exports = sort;
});

// node_modules/semver/functions/rsort.js
var require_rsort = __commonJS((exports, module) => {
  var compareBuild = require_compare_build();
  var rsort = (list, loose) => list.sort((a, b) => compareBuild(b, a, loose));
  module.exports = rsort;
});

// node_modules/semver/functions/gt.js
var require_gt = __commonJS((exports, module) => {
  var compare = require_compare();
  var gt = (a, b, loose) => compare(a, b, loose) > 0;
  module.exports = gt;
});

// node_modules/semver/functions/lt.js
var require_lt = __commonJS((exports, module) => {
  var compare = require_compare();
  var lt = (a, b, loose) => compare(a, b, loose) < 0;
  module.exports = lt;
});

// node_modules/semver/functions/eq.js
var require_eq = __commonJS((exports, module) => {
  var compare = require_compare();
  var eq = (a, b, loose) => compare(a, b, loose) === 0;
  module.exports = eq;
});

// node_modules/semver/functions/neq.js
var require_neq = __commonJS((exports, module) => {
  var compare = require_compare();
  var neq = (a, b, loose) => compare(a, b, loose) !== 0;
  module.exports = neq;
});

// node_modules/semver/functions/gte.js
var require_gte = __commonJS((exports, module) => {
  var compare = require_compare();
  var gte = (a, b, loose) => compare(a, b, loose) >= 0;
  module.exports = gte;
});

// node_modules/semver/functions/lte.js
var require_lte = __commonJS((exports, module) => {
  var compare = require_compare();
  var lte = (a, b, loose) => compare(a, b, loose) <= 0;
  module.exports = lte;
});

// node_modules/semver/functions/cmp.js
var require_cmp = __commonJS((exports, module) => {
  var eq = require_eq();
  var neq = require_neq();
  var gt = require_gt();
  var gte = require_gte();
  var lt = require_lt();
  var lte = require_lte();
  var cmp = (a, op, b, loose) => {
    switch (op) {
      case "===":
        if (typeof a === "object") {
          a = a.version;
        }
        if (typeof b === "object") {
          b = b.version;
        }
        return a === b;
      case "!==":
        if (typeof a === "object") {
          a = a.version;
        }
        if (typeof b === "object") {
          b = b.version;
        }
        return a !== b;
      case "":
      case "=":
      case "==":
        return eq(a, b, loose);
      case "!=":
        return neq(a, b, loose);
      case ">":
        return gt(a, b, loose);
      case ">=":
        return gte(a, b, loose);
      case "<":
        return lt(a, b, loose);
      case "<=":
        return lte(a, b, loose);
      default:
        throw new TypeError(`Invalid operator: ${op}`);
    }
  };
  module.exports = cmp;
});

// node_modules/semver/functions/coerce.js
var require_coerce = __commonJS((exports, module) => {
  var SemVer = require_semver();
  var parse2 = require_parse2();
  var { safeRe: re, t } = require_re();
  var coerce = (version, options) => {
    if (version instanceof SemVer) {
      return version;
    }
    if (typeof version === "number") {
      version = String(version);
    }
    if (typeof version !== "string") {
      return null;
    }
    options = options || {};
    let match = null;
    if (!options.rtl) {
      match = version.match(options.includePrerelease ? re[t.COERCEFULL] : re[t.COERCE]);
    } else {
      const coerceRtlRegex = options.includePrerelease ? re[t.COERCERTLFULL] : re[t.COERCERTL];
      let next;
      while ((next = coerceRtlRegex.exec(version)) && (!match || match.index + match[0].length !== version.length)) {
        if (!match || next.index + next[0].length !== match.index + match[0].length) {
          match = next;
        }
        coerceRtlRegex.lastIndex = next.index + next[1].length + next[2].length;
      }
      coerceRtlRegex.lastIndex = -1;
    }
    if (match === null) {
      return null;
    }
    const major = match[2];
    const minor = match[3] || "0";
    const patch = match[4] || "0";
    const prerelease = options.includePrerelease && match[5] ? `-${match[5]}` : "";
    const build = options.includePrerelease && match[6] ? `+${match[6]}` : "";
    return parse2(`${major}.${minor}.${patch}${prerelease}${build}`, options);
  };
  module.exports = coerce;
});

// node_modules/yallist/iterator.js
var require_iterator = __commonJS((exports, module) => {
  module.exports = function(Yallist) {
    Yallist.prototype[Symbol.iterator] = function* () {
      for (let walker = this.head;walker; walker = walker.next) {
        yield walker.value;
      }
    };
  };
});

// node_modules/yallist/yallist.js
var require_yallist = __commonJS((exports, module) => {
  var Yallist = function(list) {
    var self = this;
    if (!(self instanceof Yallist)) {
      self = new Yallist;
    }
    self.tail = null;
    self.head = null;
    self.length = 0;
    if (list && typeof list.forEach === "function") {
      list.forEach(function(item) {
        self.push(item);
      });
    } else if (arguments.length > 0) {
      for (var i = 0, l = arguments.length;i < l; i++) {
        self.push(arguments[i]);
      }
    }
    return self;
  };
  var insert = function(self, node4, value) {
    var inserted = node4 === self.head ? new Node3(value, null, node4, self) : new Node3(value, node4, node4.next, self);
    if (inserted.next === null) {
      self.tail = inserted;
    }
    if (inserted.prev === null) {
      self.head = inserted;
    }
    self.length++;
    return inserted;
  };
  var push = function(self, item) {
    self.tail = new Node3(item, self.tail, null, self);
    if (!self.head) {
      self.head = self.tail;
    }
    self.length++;
  };
  var unshift = function(self, item) {
    self.head = new Node3(item, null, self.head, self);
    if (!self.tail) {
      self.tail = self.head;
    }
    self.length++;
  };
  var Node3 = function(value, prev, next, list) {
    if (!(this instanceof Node3)) {
      return new Node3(value, prev, next, list);
    }
    this.list = list;
    this.value = value;
    if (prev) {
      prev.next = this;
      this.prev = prev;
    } else {
      this.prev = null;
    }
    if (next) {
      next.prev = this;
      this.next = next;
    } else {
      this.next = null;
    }
  };
  module.exports = Yallist;
  Yallist.Node = Node3;
  Yallist.create = Yallist;
  Yallist.prototype.removeNode = function(node4) {
    if (node4.list !== this) {
      throw new Error("removing node which does not belong to this list");
    }
    var next = node4.next;
    var prev = node4.prev;
    if (next) {
      next.prev = prev;
    }
    if (prev) {
      prev.next = next;
    }
    if (node4 === this.head) {
      this.head = next;
    }
    if (node4 === this.tail) {
      this.tail = prev;
    }
    node4.list.length--;
    node4.next = null;
    node4.prev = null;
    node4.list = null;
    return next;
  };
  Yallist.prototype.unshiftNode = function(node4) {
    if (node4 === this.head) {
      return;
    }
    if (node4.list) {
      node4.list.removeNode(node4);
    }
    var head = this.head;
    node4.list = this;
    node4.next = head;
    if (head) {
      head.prev = node4;
    }
    this.head = node4;
    if (!this.tail) {
      this.tail = node4;
    }
    this.length++;
  };
  Yallist.prototype.pushNode = function(node4) {
    if (node4 === this.tail) {
      return;
    }
    if (node4.list) {
      node4.list.removeNode(node4);
    }
    var tail = this.tail;
    node4.list = this;
    node4.prev = tail;
    if (tail) {
      tail.next = node4;
    }
    this.tail = node4;
    if (!this.head) {
      this.head = node4;
    }
    this.length++;
  };
  Yallist.prototype.push = function() {
    for (var i = 0, l = arguments.length;i < l; i++) {
      push(this, arguments[i]);
    }
    return this.length;
  };
  Yallist.prototype.unshift = function() {
    for (var i = 0, l = arguments.length;i < l; i++) {
      unshift(this, arguments[i]);
    }
    return this.length;
  };
  Yallist.prototype.pop = function() {
    if (!this.tail) {
      return;
    }
    var res = this.tail.value;
    this.tail = this.tail.prev;
    if (this.tail) {
      this.tail.next = null;
    } else {
      this.head = null;
    }
    this.length--;
    return res;
  };
  Yallist.prototype.shift = function() {
    if (!this.head) {
      return;
    }
    var res = this.head.value;
    this.head = this.head.next;
    if (this.head) {
      this.head.prev = null;
    } else {
      this.tail = null;
    }
    this.length--;
    return res;
  };
  Yallist.prototype.forEach = function(fn, thisp) {
    thisp = thisp || this;
    for (var walker = this.head, i = 0;walker !== null; i++) {
      fn.call(thisp, walker.value, i, this);
      walker = walker.next;
    }
  };
  Yallist.prototype.forEachReverse = function(fn, thisp) {
    thisp = thisp || this;
    for (var walker = this.tail, i = this.length - 1;walker !== null; i--) {
      fn.call(thisp, walker.value, i, this);
      walker = walker.prev;
    }
  };
  Yallist.prototype.get = function(n) {
    for (var i = 0, walker = this.head;walker !== null && i < n; i++) {
      walker = walker.next;
    }
    if (i === n && walker !== null) {
      return walker.value;
    }
  };
  Yallist.prototype.getReverse = function(n) {
    for (var i = 0, walker = this.tail;walker !== null && i < n; i++) {
      walker = walker.prev;
    }
    if (i === n && walker !== null) {
      return walker.value;
    }
  };
  Yallist.prototype.map = function(fn, thisp) {
    thisp = thisp || this;
    var res = new Yallist;
    for (var walker = this.head;walker !== null; ) {
      res.push(fn.call(thisp, walker.value, this));
      walker = walker.next;
    }
    return res;
  };
  Yallist.prototype.mapReverse = function(fn, thisp) {
    thisp = thisp || this;
    var res = new Yallist;
    for (var walker = this.tail;walker !== null; ) {
      res.push(fn.call(thisp, walker.value, this));
      walker = walker.prev;
    }
    return res;
  };
  Yallist.prototype.reduce = function(fn, initial) {
    var acc;
    var walker = this.head;
    if (arguments.length > 1) {
      acc = initial;
    } else if (this.head) {
      walker = this.head.next;
      acc = this.head.value;
    } else {
      throw new TypeError("Reduce of empty list with no initial value");
    }
    for (var i = 0;walker !== null; i++) {
      acc = fn(acc, walker.value, i);
      walker = walker.next;
    }
    return acc;
  };
  Yallist.prototype.reduceReverse = function(fn, initial) {
    var acc;
    var walker = this.tail;
    if (arguments.length > 1) {
      acc = initial;
    } else if (this.tail) {
      walker = this.tail.prev;
      acc = this.tail.value;
    } else {
      throw new TypeError("Reduce of empty list with no initial value");
    }
    for (var i = this.length - 1;walker !== null; i--) {
      acc = fn(acc, walker.value, i);
      walker = walker.prev;
    }
    return acc;
  };
  Yallist.prototype.toArray = function() {
    var arr = new Array(this.length);
    for (var i = 0, walker = this.head;walker !== null; i++) {
      arr[i] = walker.value;
      walker = walker.next;
    }
    return arr;
  };
  Yallist.prototype.toArrayReverse = function() {
    var arr = new Array(this.length);
    for (var i = 0, walker = this.tail;walker !== null; i++) {
      arr[i] = walker.value;
      walker = walker.prev;
    }
    return arr;
  };
  Yallist.prototype.slice = function(from, to) {
    to = to || this.length;
    if (to < 0) {
      to += this.length;
    }
    from = from || 0;
    if (from < 0) {
      from += this.length;
    }
    var ret = new Yallist;
    if (to < from || to < 0) {
      return ret;
    }
    if (from < 0) {
      from = 0;
    }
    if (to > this.length) {
      to = this.length;
    }
    for (var i = 0, walker = this.head;walker !== null && i < from; i++) {
      walker = walker.next;
    }
    for (;walker !== null && i < to; i++, walker = walker.next) {
      ret.push(walker.value);
    }
    return ret;
  };
  Yallist.prototype.sliceReverse = function(from, to) {
    to = to || this.length;
    if (to < 0) {
      to += this.length;
    }
    from = from || 0;
    if (from < 0) {
      from += this.length;
    }
    var ret = new Yallist;
    if (to < from || to < 0) {
      return ret;
    }
    if (from < 0) {
      from = 0;
    }
    if (to > this.length) {
      to = this.length;
    }
    for (var i = this.length, walker = this.tail;walker !== null && i > to; i--) {
      walker = walker.prev;
    }
    for (;walker !== null && i > from; i--, walker = walker.prev) {
      ret.push(walker.value);
    }
    return ret;
  };
  Yallist.prototype.splice = function(start, deleteCount, ...nodes) {
    if (start > this.length) {
      start = this.length - 1;
    }
    if (start < 0) {
      start = this.length + start;
    }
    for (var i = 0, walker = this.head;walker !== null && i < start; i++) {
      walker = walker.next;
    }
    var ret = [];
    for (var i = 0;walker && i < deleteCount; i++) {
      ret.push(walker.value);
      walker = this.removeNode(walker);
    }
    if (walker === null) {
      walker = this.tail;
    }
    if (walker !== this.head && walker !== this.tail) {
      walker = walker.prev;
    }
    for (var i = 0;i < nodes.length; i++) {
      walker = insert(this, walker, nodes[i]);
    }
    return ret;
  };
  Yallist.prototype.reverse = function() {
    var head = this.head;
    var tail = this.tail;
    for (var walker = head;walker !== null; walker = walker.prev) {
      var p = walker.prev;
      walker.prev = walker.next;
      walker.next = p;
    }
    this.head = tail;
    this.tail = head;
    return this;
  };
  try {
    require_iterator()(Yallist);
  } catch (er) {
  }
});

// node_modules/lru-cache/index.js
var require_lru_cache = __commonJS((exports, module) => {
  var Yallist = require_yallist();
  var MAX = Symbol("max");
  var LENGTH = Symbol("length");
  var LENGTH_CALCULATOR = Symbol("lengthCalculator");
  var ALLOW_STALE = Symbol("allowStale");
  var MAX_AGE = Symbol("maxAge");
  var DISPOSE = Symbol("dispose");
  var NO_DISPOSE_ON_SET = Symbol("noDisposeOnSet");
  var LRU_LIST = Symbol("lruList");
  var CACHE = Symbol("cache");
  var UPDATE_AGE_ON_GET = Symbol("updateAgeOnGet");
  var naiveLength = () => 1;

  class LRUCache {
    constructor(options) {
      if (typeof options === "number")
        options = { max: options };
      if (!options)
        options = {};
      if (options.max && (typeof options.max !== "number" || options.max < 0))
        throw new TypeError("max must be a non-negative number");
      const max = this[MAX] = options.max || Infinity;
      const lc = options.length || naiveLength;
      this[LENGTH_CALCULATOR] = typeof lc !== "function" ? naiveLength : lc;
      this[ALLOW_STALE] = options.stale || false;
      if (options.maxAge && typeof options.maxAge !== "number")
        throw new TypeError("maxAge must be a number");
      this[MAX_AGE] = options.maxAge || 0;
      this[DISPOSE] = options.dispose;
      this[NO_DISPOSE_ON_SET] = options.noDisposeOnSet || false;
      this[UPDATE_AGE_ON_GET] = options.updateAgeOnGet || false;
      this.reset();
    }
    set max(mL) {
      if (typeof mL !== "number" || mL < 0)
        throw new TypeError("max must be a non-negative number");
      this[MAX] = mL || Infinity;
      trim(this);
    }
    get max() {
      return this[MAX];
    }
    set allowStale(allowStale) {
      this[ALLOW_STALE] = !!allowStale;
    }
    get allowStale() {
      return this[ALLOW_STALE];
    }
    set maxAge(mA) {
      if (typeof mA !== "number")
        throw new TypeError("maxAge must be a non-negative number");
      this[MAX_AGE] = mA;
      trim(this);
    }
    get maxAge() {
      return this[MAX_AGE];
    }
    set lengthCalculator(lC) {
      if (typeof lC !== "function")
        lC = naiveLength;
      if (lC !== this[LENGTH_CALCULATOR]) {
        this[LENGTH_CALCULATOR] = lC;
        this[LENGTH] = 0;
        this[LRU_LIST].forEach((hit) => {
          hit.length = this[LENGTH_CALCULATOR](hit.value, hit.key);
          this[LENGTH] += hit.length;
        });
      }
      trim(this);
    }
    get lengthCalculator() {
      return this[LENGTH_CALCULATOR];
    }
    get length() {
      return this[LENGTH];
    }
    get itemCount() {
      return this[LRU_LIST].length;
    }
    rforEach(fn, thisp) {
      thisp = thisp || this;
      for (let walker = this[LRU_LIST].tail;walker !== null; ) {
        const prev = walker.prev;
        forEachStep(this, fn, walker, thisp);
        walker = prev;
      }
    }
    forEach(fn, thisp) {
      thisp = thisp || this;
      for (let walker = this[LRU_LIST].head;walker !== null; ) {
        const next = walker.next;
        forEachStep(this, fn, walker, thisp);
        walker = next;
      }
    }
    keys() {
      return this[LRU_LIST].toArray().map((k) => k.key);
    }
    values() {
      return this[LRU_LIST].toArray().map((k) => k.value);
    }
    reset() {
      if (this[DISPOSE] && this[LRU_LIST] && this[LRU_LIST].length) {
        this[LRU_LIST].forEach((hit) => this[DISPOSE](hit.key, hit.value));
      }
      this[CACHE] = new Map;
      this[LRU_LIST] = new Yallist;
      this[LENGTH] = 0;
    }
    dump() {
      return this[LRU_LIST].map((hit) => isStale(this, hit) ? false : {
        k: hit.key,
        v: hit.value,
        e: hit.now + (hit.maxAge || 0)
      }).toArray().filter((h) => h);
    }
    dumpLru() {
      return this[LRU_LIST];
    }
    set(key, value, maxAge) {
      maxAge = maxAge || this[MAX_AGE];
      if (maxAge && typeof maxAge !== "number")
        throw new TypeError("maxAge must be a number");
      const now = maxAge ? Date.now() : 0;
      const len = this[LENGTH_CALCULATOR](value, key);
      if (this[CACHE].has(key)) {
        if (len > this[MAX]) {
          del(this, this[CACHE].get(key));
          return false;
        }
        const node4 = this[CACHE].get(key);
        const item = node4.value;
        if (this[DISPOSE]) {
          if (!this[NO_DISPOSE_ON_SET])
            this[DISPOSE](key, item.value);
        }
        item.now = now;
        item.maxAge = maxAge;
        item.value = value;
        this[LENGTH] += len - item.length;
        item.length = len;
        this.get(key);
        trim(this);
        return true;
      }
      const hit = new Entry(key, value, len, now, maxAge);
      if (hit.length > this[MAX]) {
        if (this[DISPOSE])
          this[DISPOSE](key, value);
        return false;
      }
      this[LENGTH] += hit.length;
      this[LRU_LIST].unshift(hit);
      this[CACHE].set(key, this[LRU_LIST].head);
      trim(this);
      return true;
    }
    has(key) {
      if (!this[CACHE].has(key))
        return false;
      const hit = this[CACHE].get(key).value;
      return !isStale(this, hit);
    }
    get(key) {
      return get(this, key, true);
    }
    peek(key) {
      return get(this, key, false);
    }
    pop() {
      const node4 = this[LRU_LIST].tail;
      if (!node4)
        return null;
      del(this, node4);
      return node4.value;
    }
    del(key) {
      del(this, this[CACHE].get(key));
    }
    load(arr) {
      this.reset();
      const now = Date.now();
      for (let l = arr.length - 1;l >= 0; l--) {
        const hit = arr[l];
        const expiresAt = hit.e || 0;
        if (expiresAt === 0)
          this.set(hit.k, hit.v);
        else {
          const maxAge = expiresAt - now;
          if (maxAge > 0) {
            this.set(hit.k, hit.v, maxAge);
          }
        }
      }
    }
    prune() {
      this[CACHE].forEach((value, key) => get(this, key, false));
    }
  }
  var get = (self, key, doUse) => {
    const node4 = self[CACHE].get(key);
    if (node4) {
      const hit = node4.value;
      if (isStale(self, hit)) {
        del(self, node4);
        if (!self[ALLOW_STALE])
          return;
      } else {
        if (doUse) {
          if (self[UPDATE_AGE_ON_GET])
            node4.value.now = Date.now();
          self[LRU_LIST].unshiftNode(node4);
        }
      }
      return hit.value;
    }
  };
  var isStale = (self, hit) => {
    if (!hit || !hit.maxAge && !self[MAX_AGE])
      return false;
    const diff = Date.now() - hit.now;
    return hit.maxAge ? diff > hit.maxAge : self[MAX_AGE] && diff > self[MAX_AGE];
  };
  var trim = (self) => {
    if (self[LENGTH] > self[MAX]) {
      for (let walker = self[LRU_LIST].tail;self[LENGTH] > self[MAX] && walker !== null; ) {
        const prev = walker.prev;
        del(self, walker);
        walker = prev;
      }
    }
  };
  var del = (self, node4) => {
    if (node4) {
      const hit = node4.value;
      if (self[DISPOSE])
        self[DISPOSE](hit.key, hit.value);
      self[LENGTH] -= hit.length;
      self[CACHE].delete(hit.key);
      self[LRU_LIST].removeNode(node4);
    }
  };

  class Entry {
    constructor(key, value, length, now, maxAge) {
      this.key = key;
      this.value = value;
      this.length = length;
      this.now = now;
      this.maxAge = maxAge || 0;
    }
  }
  var forEachStep = (self, fn, node4, thisp) => {
    let hit = node4.value;
    if (isStale(self, hit)) {
      del(self, node4);
      if (!self[ALLOW_STALE])
        hit = undefined;
    }
    if (hit)
      fn.call(thisp, hit.value, hit.key, self);
  };
  module.exports = LRUCache;
});

// node_modules/semver/classes/range.js
var require_range = __commonJS((exports, module) => {
  class Range {
    constructor(range, options) {
      options = parseOptions(options);
      if (range instanceof Range) {
        if (range.loose === !!options.loose && range.includePrerelease === !!options.includePrerelease) {
          return range;
        } else {
          return new Range(range.raw, options);
        }
      }
      if (range instanceof Comparator) {
        this.raw = range.value;
        this.set = [[range]];
        this.format();
        return this;
      }
      this.options = options;
      this.loose = !!options.loose;
      this.includePrerelease = !!options.includePrerelease;
      this.raw = range.trim().split(/\s+/).join(" ");
      this.set = this.raw.split("||").map((r) => this.parseRange(r.trim())).filter((c) => c.length);
      if (!this.set.length) {
        throw new TypeError(`Invalid SemVer Range: ${this.raw}`);
      }
      if (this.set.length > 1) {
        const first = this.set[0];
        this.set = this.set.filter((c) => !isNullSet(c[0]));
        if (this.set.length === 0) {
          this.set = [first];
        } else if (this.set.length > 1) {
          for (const c of this.set) {
            if (c.length === 1 && isAny(c[0])) {
              this.set = [c];
              break;
            }
          }
        }
      }
      this.format();
    }
    format() {
      this.range = this.set.map((comps) => comps.join(" ").trim()).join("||").trim();
      return this.range;
    }
    toString() {
      return this.range;
    }
    parseRange(range) {
      const memoOpts = (this.options.includePrerelease && FLAG_INCLUDE_PRERELEASE) | (this.options.loose && FLAG_LOOSE);
      const memoKey = memoOpts + ":" + range;
      const cached = cache.get(memoKey);
      if (cached) {
        return cached;
      }
      const loose = this.options.loose;
      const hr = loose ? re[t.HYPHENRANGELOOSE] : re[t.HYPHENRANGE];
      range = range.replace(hr, hyphenReplace(this.options.includePrerelease));
      debug("hyphen replace", range);
      range = range.replace(re[t.COMPARATORTRIM], comparatorTrimReplace);
      debug("comparator trim", range);
      range = range.replace(re[t.TILDETRIM], tildeTrimReplace);
      debug("tilde trim", range);
      range = range.replace(re[t.CARETTRIM], caretTrimReplace);
      debug("caret trim", range);
      let rangeList = range.split(" ").map((comp) => parseComparator(comp, this.options)).join(" ").split(/\s+/).map((comp) => replaceGTE0(comp, this.options));
      if (loose) {
        rangeList = rangeList.filter((comp) => {
          debug("loose invalid filter", comp, this.options);
          return !!comp.match(re[t.COMPARATORLOOSE]);
        });
      }
      debug("range list", rangeList);
      const rangeMap = new Map;
      const comparators = rangeList.map((comp) => new Comparator(comp, this.options));
      for (const comp of comparators) {
        if (isNullSet(comp)) {
          return [comp];
        }
        rangeMap.set(comp.value, comp);
      }
      if (rangeMap.size > 1 && rangeMap.has("")) {
        rangeMap.delete("");
      }
      const result = [...rangeMap.values()];
      cache.set(memoKey, result);
      return result;
    }
    intersects(range, options) {
      if (!(range instanceof Range)) {
        throw new TypeError("a Range is required");
      }
      return this.set.some((thisComparators) => {
        return isSatisfiable(thisComparators, options) && range.set.some((rangeComparators) => {
          return isSatisfiable(rangeComparators, options) && thisComparators.every((thisComparator) => {
            return rangeComparators.every((rangeComparator) => {
              return thisComparator.intersects(rangeComparator, options);
            });
          });
        });
      });
    }
    test(version) {
      if (!version) {
        return false;
      }
      if (typeof version === "string") {
        try {
          version = new SemVer(version, this.options);
        } catch (er) {
          return false;
        }
      }
      for (let i = 0;i < this.set.length; i++) {
        if (testSet(this.set[i], version, this.options)) {
          return true;
        }
      }
      return false;
    }
  }
  module.exports = Range;
  var LRU = require_lru_cache();
  var cache = new LRU({ max: 1000 });
  var parseOptions = require_parse_options();
  var Comparator = require_comparator();
  var debug = require_debug();
  var SemVer = require_semver();
  var {
    safeRe: re,
    t,
    comparatorTrimReplace,
    tildeTrimReplace,
    caretTrimReplace
  } = require_re();
  var { FLAG_INCLUDE_PRERELEASE, FLAG_LOOSE } = require_constants3();
  var isNullSet = (c) => c.value === "<0.0.0-0";
  var isAny = (c) => c.value === "";
  var isSatisfiable = (comparators, options) => {
    let result = true;
    const remainingComparators = comparators.slice();
    let testComparator = remainingComparators.pop();
    while (result && remainingComparators.length) {
      result = remainingComparators.every((otherComparator) => {
        return testComparator.intersects(otherComparator, options);
      });
      testComparator = remainingComparators.pop();
    }
    return result;
  };
  var parseComparator = (comp, options) => {
    debug("comp", comp, options);
    comp = replaceCarets(comp, options);
    debug("caret", comp);
    comp = replaceTildes(comp, options);
    debug("tildes", comp);
    comp = replaceXRanges(comp, options);
    debug("xrange", comp);
    comp = replaceStars(comp, options);
    debug("stars", comp);
    return comp;
  };
  var isX = (id) => !id || id.toLowerCase() === "x" || id === "*";
  var replaceTildes = (comp, options) => {
    return comp.trim().split(/\s+/).map((c) => replaceTilde(c, options)).join(" ");
  };
  var replaceTilde = (comp, options) => {
    const r = options.loose ? re[t.TILDELOOSE] : re[t.TILDE];
    return comp.replace(r, (_, M, m, p, pr) => {
      debug("tilde", comp, _, M, m, p, pr);
      let ret;
      if (isX(M)) {
        ret = "";
      } else if (isX(m)) {
        ret = `>=${M}.0.0 <${+M + 1}.0.0-0`;
      } else if (isX(p)) {
        ret = `>=${M}.${m}.0 <${M}.${+m + 1}.0-0`;
      } else if (pr) {
        debug("replaceTilde pr", pr);
        ret = `>=${M}.${m}.${p}-${pr} <${M}.${+m + 1}.0-0`;
      } else {
        ret = `>=${M}.${m}.${p} <${M}.${+m + 1}.0-0`;
      }
      debug("tilde return", ret);
      return ret;
    });
  };
  var replaceCarets = (comp, options) => {
    return comp.trim().split(/\s+/).map((c) => replaceCaret(c, options)).join(" ");
  };
  var replaceCaret = (comp, options) => {
    debug("caret", comp, options);
    const r = options.loose ? re[t.CARETLOOSE] : re[t.CARET];
    const z = options.includePrerelease ? "-0" : "";
    return comp.replace(r, (_, M, m, p, pr) => {
      debug("caret", comp, _, M, m, p, pr);
      let ret;
      if (isX(M)) {
        ret = "";
      } else if (isX(m)) {
        ret = `>=${M}.0.0${z} <${+M + 1}.0.0-0`;
      } else if (isX(p)) {
        if (M === "0") {
          ret = `>=${M}.${m}.0${z} <${M}.${+m + 1}.0-0`;
        } else {
          ret = `>=${M}.${m}.0${z} <${+M + 1}.0.0-0`;
        }
      } else if (pr) {
        debug("replaceCaret pr", pr);
        if (M === "0") {
          if (m === "0") {
            ret = `>=${M}.${m}.${p}-${pr} <${M}.${m}.${+p + 1}-0`;
          } else {
            ret = `>=${M}.${m}.${p}-${pr} <${M}.${+m + 1}.0-0`;
          }
        } else {
          ret = `>=${M}.${m}.${p}-${pr} <${+M + 1}.0.0-0`;
        }
      } else {
        debug("no pr");
        if (M === "0") {
          if (m === "0") {
            ret = `>=${M}.${m}.${p}${z} <${M}.${m}.${+p + 1}-0`;
          } else {
            ret = `>=${M}.${m}.${p}${z} <${M}.${+m + 1}.0-0`;
          }
        } else {
          ret = `>=${M}.${m}.${p} <${+M + 1}.0.0-0`;
        }
      }
      debug("caret return", ret);
      return ret;
    });
  };
  var replaceXRanges = (comp, options) => {
    debug("replaceXRanges", comp, options);
    return comp.split(/\s+/).map((c) => replaceXRange(c, options)).join(" ");
  };
  var replaceXRange = (comp, options) => {
    comp = comp.trim();
    const r = options.loose ? re[t.XRANGELOOSE] : re[t.XRANGE];
    return comp.replace(r, (ret, gtlt, M, m, p, pr) => {
      debug("xRange", comp, ret, gtlt, M, m, p, pr);
      const xM = isX(M);
      const xm = xM || isX(m);
      const xp = xm || isX(p);
      const anyX = xp;
      if (gtlt === "=" && anyX) {
        gtlt = "";
      }
      pr = options.includePrerelease ? "-0" : "";
      if (xM) {
        if (gtlt === ">" || gtlt === "<") {
          ret = "<0.0.0-0";
        } else {
          ret = "*";
        }
      } else if (gtlt && anyX) {
        if (xm) {
          m = 0;
        }
        p = 0;
        if (gtlt === ">") {
          gtlt = ">=";
          if (xm) {
            M = +M + 1;
            m = 0;
            p = 0;
          } else {
            m = +m + 1;
            p = 0;
          }
        } else if (gtlt === "<=") {
          gtlt = "<";
          if (xm) {
            M = +M + 1;
          } else {
            m = +m + 1;
          }
        }
        if (gtlt === "<") {
          pr = "-0";
        }
        ret = `${gtlt + M}.${m}.${p}${pr}`;
      } else if (xm) {
        ret = `>=${M}.0.0${pr} <${+M + 1}.0.0-0`;
      } else if (xp) {
        ret = `>=${M}.${m}.0${pr} <${M}.${+m + 1}.0-0`;
      }
      debug("xRange return", ret);
      return ret;
    });
  };
  var replaceStars = (comp, options) => {
    debug("replaceStars", comp, options);
    return comp.trim().replace(re[t.STAR], "");
  };
  var replaceGTE0 = (comp, options) => {
    debug("replaceGTE0", comp, options);
    return comp.trim().replace(re[options.includePrerelease ? t.GTE0PRE : t.GTE0], "");
  };
  var hyphenReplace = (incPr) => ($0, from, fM, fm, fp, fpr, fb, to, tM, tm, tp, tpr, tb) => {
    if (isX(fM)) {
      from = "";
    } else if (isX(fm)) {
      from = `>=${fM}.0.0${incPr ? "-0" : ""}`;
    } else if (isX(fp)) {
      from = `>=${fM}.${fm}.0${incPr ? "-0" : ""}`;
    } else if (fpr) {
      from = `>=${from}`;
    } else {
      from = `>=${from}${incPr ? "-0" : ""}`;
    }
    if (isX(tM)) {
      to = "";
    } else if (isX(tm)) {
      to = `<${+tM + 1}.0.0-0`;
    } else if (isX(tp)) {
      to = `<${tM}.${+tm + 1}.0-0`;
    } else if (tpr) {
      to = `<=${tM}.${tm}.${tp}-${tpr}`;
    } else if (incPr) {
      to = `<${tM}.${tm}.${+tp + 1}-0`;
    } else {
      to = `<=${to}`;
    }
    return `${from} ${to}`.trim();
  };
  var testSet = (set, version, options) => {
    for (let i = 0;i < set.length; i++) {
      if (!set[i].test(version)) {
        return false;
      }
    }
    if (version.prerelease.length && !options.includePrerelease) {
      for (let i = 0;i < set.length; i++) {
        debug(set[i].semver);
        if (set[i].semver === Comparator.ANY) {
          continue;
        }
        if (set[i].semver.prerelease.length > 0) {
          const allowed = set[i].semver;
          if (allowed.major === version.major && allowed.minor === version.minor && allowed.patch === version.patch) {
            return true;
          }
        }
      }
      return false;
    }
    return true;
  };
});

// node_modules/semver/classes/comparator.js
var require_comparator = __commonJS((exports, module) => {
  var ANY = Symbol("SemVer ANY");

  class Comparator {
    static get ANY() {
      return ANY;
    }
    constructor(comp, options) {
      options = parseOptions(options);
      if (comp instanceof Comparator) {
        if (comp.loose === !!options.loose) {
          return comp;
        } else {
          comp = comp.value;
        }
      }
      comp = comp.trim().split(/\s+/).join(" ");
      debug("comparator", comp, options);
      this.options = options;
      this.loose = !!options.loose;
      this.parse(comp);
      if (this.semver === ANY) {
        this.value = "";
      } else {
        this.value = this.operator + this.semver.version;
      }
      debug("comp", this);
    }
    parse(comp) {
      const r = this.options.loose ? re[t.COMPARATORLOOSE] : re[t.COMPARATOR];
      const m = comp.match(r);
      if (!m) {
        throw new TypeError(`Invalid comparator: ${comp}`);
      }
      this.operator = m[1] !== undefined ? m[1] : "";
      if (this.operator === "=") {
        this.operator = "";
      }
      if (!m[2]) {
        this.semver = ANY;
      } else {
        this.semver = new SemVer(m[2], this.options.loose);
      }
    }
    toString() {
      return this.value;
    }
    test(version) {
      debug("Comparator.test", version, this.options.loose);
      if (this.semver === ANY || version === ANY) {
        return true;
      }
      if (typeof version === "string") {
        try {
          version = new SemVer(version, this.options);
        } catch (er) {
          return false;
        }
      }
      return cmp(version, this.operator, this.semver, this.options);
    }
    intersects(comp, options) {
      if (!(comp instanceof Comparator)) {
        throw new TypeError("a Comparator is required");
      }
      if (this.operator === "") {
        if (this.value === "") {
          return true;
        }
        return new Range(comp.value, options).test(this.value);
      } else if (comp.operator === "") {
        if (comp.value === "") {
          return true;
        }
        return new Range(this.value, options).test(comp.semver);
      }
      options = parseOptions(options);
      if (options.includePrerelease && (this.value === "<0.0.0-0" || comp.value === "<0.0.0-0")) {
        return false;
      }
      if (!options.includePrerelease && (this.value.startsWith("<0.0.0") || comp.value.startsWith("<0.0.0"))) {
        return false;
      }
      if (this.operator.startsWith(">") && comp.operator.startsWith(">")) {
        return true;
      }
      if (this.operator.startsWith("<") && comp.operator.startsWith("<")) {
        return true;
      }
      if (this.semver.version === comp.semver.version && this.operator.includes("=") && comp.operator.includes("=")) {
        return true;
      }
      if (cmp(this.semver, "<", comp.semver, options) && this.operator.startsWith(">") && comp.operator.startsWith("<")) {
        return true;
      }
      if (cmp(this.semver, ">", comp.semver, options) && this.operator.startsWith("<") && comp.operator.startsWith(">")) {
        return true;
      }
      return false;
    }
  }
  module.exports = Comparator;
  var parseOptions = require_parse_options();
  var { safeRe: re, t } = require_re();
  var cmp = require_cmp();
  var debug = require_debug();
  var SemVer = require_semver();
  var Range = require_range();
});

// node_modules/semver/functions/satisfies.js
var require_satisfies = __commonJS((exports, module) => {
  var Range = require_range();
  var satisfies = (version, range, options) => {
    try {
      range = new Range(range, options);
    } catch (er) {
      return false;
    }
    return range.test(version);
  };
  module.exports = satisfies;
});

// node_modules/semver/ranges/to-comparators.js
var require_to_comparators = __commonJS((exports, module) => {
  var Range = require_range();
  var toComparators = (range, options) => new Range(range, options).set.map((comp) => comp.map((c) => c.value).join(" ").trim().split(" "));
  module.exports = toComparators;
});

// node_modules/semver/ranges/max-satisfying.js
var require_max_satisfying = __commonJS((exports, module) => {
  var SemVer = require_semver();
  var Range = require_range();
  var maxSatisfying = (versions, range, options) => {
    let max = null;
    let maxSV = null;
    let rangeObj = null;
    try {
      rangeObj = new Range(range, options);
    } catch (er) {
      return null;
    }
    versions.forEach((v) => {
      if (rangeObj.test(v)) {
        if (!max || maxSV.compare(v) === -1) {
          max = v;
          maxSV = new SemVer(max, options);
        }
      }
    });
    return max;
  };
  module.exports = maxSatisfying;
});

// node_modules/semver/ranges/min-satisfying.js
var require_min_satisfying = __commonJS((exports, module) => {
  var SemVer = require_semver();
  var Range = require_range();
  var minSatisfying = (versions, range, options) => {
    let min = null;
    let minSV = null;
    let rangeObj = null;
    try {
      rangeObj = new Range(range, options);
    } catch (er) {
      return null;
    }
    versions.forEach((v) => {
      if (rangeObj.test(v)) {
        if (!min || minSV.compare(v) === 1) {
          min = v;
          minSV = new SemVer(min, options);
        }
      }
    });
    return min;
  };
  module.exports = minSatisfying;
});

// node_modules/semver/ranges/min-version.js
var require_min_version = __commonJS((exports, module) => {
  var SemVer = require_semver();
  var Range = require_range();
  var gt = require_gt();
  var minVersion = (range, loose) => {
    range = new Range(range, loose);
    let minver = new SemVer("0.0.0");
    if (range.test(minver)) {
      return minver;
    }
    minver = new SemVer("0.0.0-0");
    if (range.test(minver)) {
      return minver;
    }
    minver = null;
    for (let i = 0;i < range.set.length; ++i) {
      const comparators = range.set[i];
      let setMin = null;
      comparators.forEach((comparator) => {
        const compver = new SemVer(comparator.semver.version);
        switch (comparator.operator) {
          case ">":
            if (compver.prerelease.length === 0) {
              compver.patch++;
            } else {
              compver.prerelease.push(0);
            }
            compver.raw = compver.format();
          case "":
          case ">=":
            if (!setMin || gt(compver, setMin)) {
              setMin = compver;
            }
            break;
          case "<":
          case "<=":
            break;
          default:
            throw new Error(`Unexpected operation: ${comparator.operator}`);
        }
      });
      if (setMin && (!minver || gt(minver, setMin))) {
        minver = setMin;
      }
    }
    if (minver && range.test(minver)) {
      return minver;
    }
    return null;
  };
  module.exports = minVersion;
});

// node_modules/semver/ranges/valid.js
var require_valid2 = __commonJS((exports, module) => {
  var Range = require_range();
  var validRange = (range, options) => {
    try {
      return new Range(range, options).range || "*";
    } catch (er) {
      return null;
    }
  };
  module.exports = validRange;
});

// node_modules/semver/ranges/outside.js
var require_outside = __commonJS((exports, module) => {
  var SemVer = require_semver();
  var Comparator = require_comparator();
  var { ANY } = Comparator;
  var Range = require_range();
  var satisfies = require_satisfies();
  var gt = require_gt();
  var lt = require_lt();
  var lte = require_lte();
  var gte = require_gte();
  var outside = (version, range, hilo, options) => {
    version = new SemVer(version, options);
    range = new Range(range, options);
    let gtfn, ltefn, ltfn, comp, ecomp;
    switch (hilo) {
      case ">":
        gtfn = gt;
        ltefn = lte;
        ltfn = lt;
        comp = ">";
        ecomp = ">=";
        break;
      case "<":
        gtfn = lt;
        ltefn = gte;
        ltfn = gt;
        comp = "<";
        ecomp = "<=";
        break;
      default:
        throw new TypeError('Must provide a hilo val of "<" or ">"');
    }
    if (satisfies(version, range, options)) {
      return false;
    }
    for (let i = 0;i < range.set.length; ++i) {
      const comparators = range.set[i];
      let high = null;
      let low = null;
      comparators.forEach((comparator) => {
        if (comparator.semver === ANY) {
          comparator = new Comparator(">=0.0.0");
        }
        high = high || comparator;
        low = low || comparator;
        if (gtfn(comparator.semver, high.semver, options)) {
          high = comparator;
        } else if (ltfn(comparator.semver, low.semver, options)) {
          low = comparator;
        }
      });
      if (high.operator === comp || high.operator === ecomp) {
        return false;
      }
      if ((!low.operator || low.operator === comp) && ltefn(version, low.semver)) {
        return false;
      } else if (low.operator === ecomp && ltfn(version, low.semver)) {
        return false;
      }
    }
    return true;
  };
  module.exports = outside;
});

// node_modules/semver/ranges/gtr.js
var require_gtr = __commonJS((exports, module) => {
  var outside = require_outside();
  var gtr = (version, range, options) => outside(version, range, ">", options);
  module.exports = gtr;
});

// node_modules/semver/ranges/ltr.js
var require_ltr = __commonJS((exports, module) => {
  var outside = require_outside();
  var ltr = (version, range, options) => outside(version, range, "<", options);
  module.exports = ltr;
});

// node_modules/semver/ranges/intersects.js
var require_intersects = __commonJS((exports, module) => {
  var Range = require_range();
  var intersects = (r1, r2, options) => {
    r1 = new Range(r1, options);
    r2 = new Range(r2, options);
    return r1.intersects(r2, options);
  };
  module.exports = intersects;
});

// node_modules/semver/ranges/simplify.js
var require_simplify = __commonJS((exports, module) => {
  var satisfies = require_satisfies();
  var compare = require_compare();
  module.exports = (versions, range, options) => {
    const set = [];
    let first = null;
    let prev = null;
    const v = versions.sort((a, b) => compare(a, b, options));
    for (const version of v) {
      const included = satisfies(version, range, options);
      if (included) {
        prev = version;
        if (!first) {
          first = version;
        }
      } else {
        if (prev) {
          set.push([first, prev]);
        }
        prev = null;
        first = null;
      }
    }
    if (first) {
      set.push([first, null]);
    }
    const ranges = [];
    for (const [min, max] of set) {
      if (min === max) {
        ranges.push(min);
      } else if (!max && min === v[0]) {
        ranges.push("*");
      } else if (!max) {
        ranges.push(`>=${min}`);
      } else if (min === v[0]) {
        ranges.push(`<=${max}`);
      } else {
        ranges.push(`${min} - ${max}`);
      }
    }
    const simplified = ranges.join(" || ");
    const original = typeof range.raw === "string" ? range.raw : String(range);
    return simplified.length < original.length ? simplified : range;
  };
});

// node_modules/semver/ranges/subset.js
var require_subset = __commonJS((exports, module) => {
  var Range = require_range();
  var Comparator = require_comparator();
  var { ANY } = Comparator;
  var satisfies = require_satisfies();
  var compare = require_compare();
  var subset = (sub, dom, options = {}) => {
    if (sub === dom) {
      return true;
    }
    sub = new Range(sub, options);
    dom = new Range(dom, options);
    let sawNonNull = false;
    OUTER:
      for (const simpleSub of sub.set) {
        for (const simpleDom of dom.set) {
          const isSub = simpleSubset(simpleSub, simpleDom, options);
          sawNonNull = sawNonNull || isSub !== null;
          if (isSub) {
            continue OUTER;
          }
        }
        if (sawNonNull) {
          return false;
        }
      }
    return true;
  };
  var minimumVersionWithPreRelease = [new Comparator(">=0.0.0-0")];
  var minimumVersion = [new Comparator(">=0.0.0")];
  var simpleSubset = (sub, dom, options) => {
    if (sub === dom) {
      return true;
    }
    if (sub.length === 1 && sub[0].semver === ANY) {
      if (dom.length === 1 && dom[0].semver === ANY) {
        return true;
      } else if (options.includePrerelease) {
        sub = minimumVersionWithPreRelease;
      } else {
        sub = minimumVersion;
      }
    }
    if (dom.length === 1 && dom[0].semver === ANY) {
      if (options.includePrerelease) {
        return true;
      } else {
        dom = minimumVersion;
      }
    }
    const eqSet = new Set;
    let gt, lt;
    for (const c of sub) {
      if (c.operator === ">" || c.operator === ">=") {
        gt = higherGT(gt, c, options);
      } else if (c.operator === "<" || c.operator === "<=") {
        lt = lowerLT(lt, c, options);
      } else {
        eqSet.add(c.semver);
      }
    }
    if (eqSet.size > 1) {
      return null;
    }
    let gtltComp;
    if (gt && lt) {
      gtltComp = compare(gt.semver, lt.semver, options);
      if (gtltComp > 0) {
        return null;
      } else if (gtltComp === 0 && (gt.operator !== ">=" || lt.operator !== "<=")) {
        return null;
      }
    }
    for (const eq of eqSet) {
      if (gt && !satisfies(eq, String(gt), options)) {
        return null;
      }
      if (lt && !satisfies(eq, String(lt), options)) {
        return null;
      }
      for (const c of dom) {
        if (!satisfies(eq, String(c), options)) {
          return false;
        }
      }
      return true;
    }
    let higher, lower;
    let hasDomLT, hasDomGT;
    let needDomLTPre = lt && !options.includePrerelease && lt.semver.prerelease.length ? lt.semver : false;
    let needDomGTPre = gt && !options.includePrerelease && gt.semver.prerelease.length ? gt.semver : false;
    if (needDomLTPre && needDomLTPre.prerelease.length === 1 && lt.operator === "<" && needDomLTPre.prerelease[0] === 0) {
      needDomLTPre = false;
    }
    for (const c of dom) {
      hasDomGT = hasDomGT || c.operator === ">" || c.operator === ">=";
      hasDomLT = hasDomLT || c.operator === "<" || c.operator === "<=";
      if (gt) {
        if (needDomGTPre) {
          if (c.semver.prerelease && c.semver.prerelease.length && c.semver.major === needDomGTPre.major && c.semver.minor === needDomGTPre.minor && c.semver.patch === needDomGTPre.patch) {
            needDomGTPre = false;
          }
        }
        if (c.operator === ">" || c.operator === ">=") {
          higher = higherGT(gt, c, options);
          if (higher === c && higher !== gt) {
            return false;
          }
        } else if (gt.operator === ">=" && !satisfies(gt.semver, String(c), options)) {
          return false;
        }
      }
      if (lt) {
        if (needDomLTPre) {
          if (c.semver.prerelease && c.semver.prerelease.length && c.semver.major === needDomLTPre.major && c.semver.minor === needDomLTPre.minor && c.semver.patch === needDomLTPre.patch) {
            needDomLTPre = false;
          }
        }
        if (c.operator === "<" || c.operator === "<=") {
          lower = lowerLT(lt, c, options);
          if (lower === c && lower !== lt) {
            return false;
          }
        } else if (lt.operator === "<=" && !satisfies(lt.semver, String(c), options)) {
          return false;
        }
      }
      if (!c.operator && (lt || gt) && gtltComp !== 0) {
        return false;
      }
    }
    if (gt && hasDomLT && !lt && gtltComp !== 0) {
      return false;
    }
    if (lt && hasDomGT && !gt && gtltComp !== 0) {
      return false;
    }
    if (needDomGTPre || needDomLTPre) {
      return false;
    }
    return true;
  };
  var higherGT = (a, b, options) => {
    if (!a) {
      return b;
    }
    const comp = compare(a.semver, b.semver, options);
    return comp > 0 ? a : comp < 0 ? b : b.operator === ">" && a.operator === ">=" ? b : a;
  };
  var lowerLT = (a, b, options) => {
    if (!a) {
      return b;
    }
    const comp = compare(a.semver, b.semver, options);
    return comp < 0 ? a : comp > 0 ? b : b.operator === "<" && a.operator === "<=" ? b : a;
  };
  module.exports = subset;
});

// node_modules/semver/index.js
var require_semver2 = __commonJS((exports, module) => {
  var internalRe = require_re();
  var constants = require_constants3();
  var SemVer = require_semver();
  var identifiers = require_identifiers();
  var parse2 = require_parse2();
  var valid = require_valid();
  var clean = require_clean();
  var inc = require_inc();
  var diff = require_diff();
  var major = require_major();
  var minor = require_minor();
  var patch = require_patch();
  var prerelease = require_prerelease();
  var compare = require_compare();
  var rcompare = require_rcompare();
  var compareLoose = require_compare_loose();
  var compareBuild = require_compare_build();
  var sort = require_sort();
  var rsort = require_rsort();
  var gt = require_gt();
  var lt = require_lt();
  var eq = require_eq();
  var neq = require_neq();
  var gte = require_gte();
  var lte = require_lte();
  var cmp = require_cmp();
  var coerce = require_coerce();
  var Comparator = require_comparator();
  var Range = require_range();
  var satisfies = require_satisfies();
  var toComparators = require_to_comparators();
  var maxSatisfying = require_max_satisfying();
  var minSatisfying = require_min_satisfying();
  var minVersion = require_min_version();
  var validRange = require_valid2();
  var outside = require_outside();
  var gtr = require_gtr();
  var ltr = require_ltr();
  var intersects = require_intersects();
  var simplifyRange = require_simplify();
  var subset = require_subset();
  module.exports = {
    parse: parse2,
    valid,
    clean,
    inc,
    diff,
    major,
    minor,
    patch,
    prerelease,
    compare,
    rcompare,
    compareLoose,
    compareBuild,
    sort,
    rsort,
    gt,
    lt,
    eq,
    neq,
    gte,
    lte,
    cmp,
    coerce,
    Comparator,
    Range,
    satisfies,
    toComparators,
    maxSatisfying,
    minSatisfying,
    minVersion,
    validRange,
    outside,
    gtr,
    ltr,
    intersects,
    simplifyRange,
    subset,
    SemVer,
    re: internalRe.re,
    src: internalRe.src,
    tokens: internalRe.t,
    SEMVER_SPEC_VERSION: constants.SEMVER_SPEC_VERSION,
    RELEASE_TYPES: constants.RELEASE_TYPES,
    compareIdentifiers: identifiers.compareIdentifiers,
    rcompareIdentifiers: identifiers.rcompareIdentifiers
  };
});

// node_modules/jsonwebtoken/lib/asymmetricKeyDetailsSupported.js
var require_asymmetricKeyDetailsSupported = __commonJS((exports, module) => {
  var semver = require_semver2();
  module.exports = semver.satisfies(process.version, ">=15.7.0");
});

// node_modules/jsonwebtoken/lib/rsaPssKeyDetailsSupported.js
var require_rsaPssKeyDetailsSupported = __commonJS((exports, module) => {
  var semver = require_semver2();
  module.exports = semver.satisfies(process.version, ">=16.9.0");
});

// node_modules/jsonwebtoken/lib/validateAsymmetricKey.js
var require_validateAsymmetricKey = __commonJS((exports, module) => {
  var ASYMMETRIC_KEY_DETAILS_SUPPORTED = require_asymmetricKeyDetailsSupported();
  var RSA_PSS_KEY_DETAILS_SUPPORTED = require_rsaPssKeyDetailsSupported();
  var allowedAlgorithmsForKeys = {
    ec: ["ES256", "ES384", "ES512"],
    rsa: ["RS256", "PS256", "RS384", "PS384", "RS512", "PS512"],
    "rsa-pss": ["PS256", "PS384", "PS512"]
  };
  var allowedCurves = {
    ES256: "prime256v1",
    ES384: "secp384r1",
    ES512: "secp521r1"
  };
  module.exports = function(algorithm, key) {
    if (!algorithm || !key)
      return;
    const keyType = key.asymmetricKeyType;
    if (!keyType)
      return;
    const allowedAlgorithms = allowedAlgorithmsForKeys[keyType];
    if (!allowedAlgorithms) {
      throw new Error(`Unknown key type "${keyType}".`);
    }
    if (!allowedAlgorithms.includes(algorithm)) {
      throw new Error(`"alg" parameter for "${keyType}" key type must be one of: ${allowedAlgorithms.join(", ")}.`);
    }
    if (ASYMMETRIC_KEY_DETAILS_SUPPORTED) {
      switch (keyType) {
        case "ec":
          const keyCurve = key.asymmetricKeyDetails.namedCurve;
          const allowedCurve = allowedCurves[algorithm];
          if (keyCurve !== allowedCurve) {
            throw new Error(`"alg" parameter "${algorithm}" requires curve "${allowedCurve}".`);
          }
          break;
        case "rsa-pss":
          if (RSA_PSS_KEY_DETAILS_SUPPORTED) {
            const length = parseInt(algorithm.slice(-3), 10);
            const { hashAlgorithm, mgf1HashAlgorithm, saltLength } = key.asymmetricKeyDetails;
            if (hashAlgorithm !== `sha${length}` || mgf1HashAlgorithm !== hashAlgorithm) {
              throw new Error(`Invalid key for this operation, its RSA-PSS parameters do not meet the requirements of "alg" ${algorithm}.`);
            }
            if (saltLength !== undefined && saltLength > length >> 3) {
              throw new Error(`Invalid key for this operation, its RSA-PSS parameter saltLength does not meet the requirements of "alg" ${algorithm}.`);
            }
          }
          break;
      }
    }
  };
});

// node_modules/jsonwebtoken/lib/psSupported.js
var require_psSupported = __commonJS((exports, module) => {
  var semver = require_semver2();
  module.exports = semver.satisfies(process.version, "^6.12.0 || >=8.0.0");
});

// node_modules/jsonwebtoken/verify.js
var require_verify = __commonJS((exports, module) => {
  var JsonWebTokenError = require_JsonWebTokenError();
  var NotBeforeError = require_NotBeforeError();
  var TokenExpiredError = require_TokenExpiredError();
  var decode = require_decode();
  var timespan = require_timespan();
  var validateAsymmetricKey = require_validateAsymmetricKey();
  var PS_SUPPORTED = require_psSupported();
  var jws = require_jws();
  var { KeyObject, createSecretKey, createPublicKey } = import.meta.require("crypto");
  var PUB_KEY_ALGS = ["RS256", "RS384", "RS512"];
  var EC_KEY_ALGS = ["ES256", "ES384", "ES512"];
  var RSA_KEY_ALGS = ["RS256", "RS384", "RS512"];
  var HS_ALGS = ["HS256", "HS384", "HS512"];
  if (PS_SUPPORTED) {
    PUB_KEY_ALGS.splice(PUB_KEY_ALGS.length, 0, "PS256", "PS384", "PS512");
    RSA_KEY_ALGS.splice(RSA_KEY_ALGS.length, 0, "PS256", "PS384", "PS512");
  }
  module.exports = function(jwtString, secretOrPublicKey, options, callback) {
    if (typeof options === "function" && !callback) {
      callback = options;
      options = {};
    }
    if (!options) {
      options = {};
    }
    options = Object.assign({}, options);
    let done;
    if (callback) {
      done = callback;
    } else {
      done = function(err, data) {
        if (err)
          throw err;
        return data;
      };
    }
    if (options.clockTimestamp && typeof options.clockTimestamp !== "number") {
      return done(new JsonWebTokenError("clockTimestamp must be a number"));
    }
    if (options.nonce !== undefined && (typeof options.nonce !== "string" || options.nonce.trim() === "")) {
      return done(new JsonWebTokenError("nonce must be a non-empty string"));
    }
    if (options.allowInvalidAsymmetricKeyTypes !== undefined && typeof options.allowInvalidAsymmetricKeyTypes !== "boolean") {
      return done(new JsonWebTokenError("allowInvalidAsymmetricKeyTypes must be a boolean"));
    }
    const clockTimestamp = options.clockTimestamp || Math.floor(Date.now() / 1000);
    if (!jwtString) {
      return done(new JsonWebTokenError("jwt must be provided"));
    }
    if (typeof jwtString !== "string") {
      return done(new JsonWebTokenError("jwt must be a string"));
    }
    const parts = jwtString.split(".");
    if (parts.length !== 3) {
      return done(new JsonWebTokenError("jwt malformed"));
    }
    let decodedToken;
    try {
      decodedToken = decode(jwtString, { complete: true });
    } catch (err) {
      return done(err);
    }
    if (!decodedToken) {
      return done(new JsonWebTokenError("invalid token"));
    }
    const header = decodedToken.header;
    let getSecret;
    if (typeof secretOrPublicKey === "function") {
      if (!callback) {
        return done(new JsonWebTokenError("verify must be called asynchronous if secret or public key is provided as a callback"));
      }
      getSecret = secretOrPublicKey;
    } else {
      getSecret = function(header2, secretCallback) {
        return secretCallback(null, secretOrPublicKey);
      };
    }
    return getSecret(header, function(err, secretOrPublicKey2) {
      if (err) {
        return done(new JsonWebTokenError("error in secret or public key callback: " + err.message));
      }
      const hasSignature = parts[2].trim() !== "";
      if (!hasSignature && secretOrPublicKey2) {
        return done(new JsonWebTokenError("jwt signature is required"));
      }
      if (hasSignature && !secretOrPublicKey2) {
        return done(new JsonWebTokenError("secret or public key must be provided"));
      }
      if (!hasSignature && !options.algorithms) {
        return done(new JsonWebTokenError('please specify "none" in "algorithms" to verify unsigned tokens'));
      }
      if (secretOrPublicKey2 != null && !(secretOrPublicKey2 instanceof KeyObject)) {
        try {
          secretOrPublicKey2 = createPublicKey(secretOrPublicKey2);
        } catch (_) {
          try {
            secretOrPublicKey2 = createSecretKey(typeof secretOrPublicKey2 === "string" ? Buffer.from(secretOrPublicKey2) : secretOrPublicKey2);
          } catch (_2) {
            return done(new JsonWebTokenError("secretOrPublicKey is not valid key material"));
          }
        }
      }
      if (!options.algorithms) {
        if (secretOrPublicKey2.type === "secret") {
          options.algorithms = HS_ALGS;
        } else if (["rsa", "rsa-pss"].includes(secretOrPublicKey2.asymmetricKeyType)) {
          options.algorithms = RSA_KEY_ALGS;
        } else if (secretOrPublicKey2.asymmetricKeyType === "ec") {
          options.algorithms = EC_KEY_ALGS;
        } else {
          options.algorithms = PUB_KEY_ALGS;
        }
      }
      if (options.algorithms.indexOf(decodedToken.header.alg) === -1) {
        return done(new JsonWebTokenError("invalid algorithm"));
      }
      if (header.alg.startsWith("HS") && secretOrPublicKey2.type !== "secret") {
        return done(new JsonWebTokenError(`secretOrPublicKey must be a symmetric key when using ${header.alg}`));
      } else if (/^(?:RS|PS|ES)/.test(header.alg) && secretOrPublicKey2.type !== "public") {
        return done(new JsonWebTokenError(`secretOrPublicKey must be an asymmetric key when using ${header.alg}`));
      }
      if (!options.allowInvalidAsymmetricKeyTypes) {
        try {
          validateAsymmetricKey(header.alg, secretOrPublicKey2);
        } catch (e) {
          return done(e);
        }
      }
      let valid;
      try {
        valid = jws.verify(jwtString, decodedToken.header.alg, secretOrPublicKey2);
      } catch (e) {
        return done(e);
      }
      if (!valid) {
        return done(new JsonWebTokenError("invalid signature"));
      }
      const payload = decodedToken.payload;
      if (typeof payload.nbf !== "undefined" && !options.ignoreNotBefore) {
        if (typeof payload.nbf !== "number") {
          return done(new JsonWebTokenError("invalid nbf value"));
        }
        if (payload.nbf > clockTimestamp + (options.clockTolerance || 0)) {
          return done(new NotBeforeError("jwt not active", new Date(payload.nbf * 1000)));
        }
      }
      if (typeof payload.exp !== "undefined" && !options.ignoreExpiration) {
        if (typeof payload.exp !== "number") {
          return done(new JsonWebTokenError("invalid exp value"));
        }
        if (clockTimestamp >= payload.exp + (options.clockTolerance || 0)) {
          return done(new TokenExpiredError("jwt expired", new Date(payload.exp * 1000)));
        }
      }
      if (options.audience) {
        const audiences = Array.isArray(options.audience) ? options.audience : [options.audience];
        const target = Array.isArray(payload.aud) ? payload.aud : [payload.aud];
        const match = target.some(function(targetAudience) {
          return audiences.some(function(audience) {
            return audience instanceof RegExp ? audience.test(targetAudience) : audience === targetAudience;
          });
        });
        if (!match) {
          return done(new JsonWebTokenError("jwt audience invalid. expected: " + audiences.join(" or ")));
        }
      }
      if (options.issuer) {
        const invalid_issuer = typeof options.issuer === "string" && payload.iss !== options.issuer || Array.isArray(options.issuer) && options.issuer.indexOf(payload.iss) === -1;
        if (invalid_issuer) {
          return done(new JsonWebTokenError("jwt issuer invalid. expected: " + options.issuer));
        }
      }
      if (options.subject) {
        if (payload.sub !== options.subject) {
          return done(new JsonWebTokenError("jwt subject invalid. expected: " + options.subject));
        }
      }
      if (options.jwtid) {
        if (payload.jti !== options.jwtid) {
          return done(new JsonWebTokenError("jwt jwtid invalid. expected: " + options.jwtid));
        }
      }
      if (options.nonce) {
        if (payload.nonce !== options.nonce) {
          return done(new JsonWebTokenError("jwt nonce invalid. expected: " + options.nonce));
        }
      }
      if (options.maxAge) {
        if (typeof payload.iat !== "number") {
          return done(new JsonWebTokenError("iat required when maxAge is specified"));
        }
        const maxAgeTimestamp = timespan(options.maxAge, payload.iat);
        if (typeof maxAgeTimestamp === "undefined") {
          return done(new JsonWebTokenError('"maxAge" should be a number of seconds or string representing a timespan eg: "1d", "20h", 60'));
        }
        if (clockTimestamp >= maxAgeTimestamp + (options.clockTolerance || 0)) {
          return done(new TokenExpiredError("maxAge exceeded", new Date(maxAgeTimestamp * 1000)));
        }
      }
      if (options.complete === true) {
        const signature = decodedToken.signature;
        return done(null, {
          header,
          payload,
          signature
        });
      }
      return done(null, payload);
    });
  };
});

// node_modules/lodash.includes/index.js
var require_lodash = __commonJS((exports, module) => {
  var arrayMap = function(array, iteratee) {
    var index = -1, length = array ? array.length : 0, result = Array(length);
    while (++index < length) {
      result[index] = iteratee(array[index], index, array);
    }
    return result;
  };
  var baseFindIndex = function(array, predicate, fromIndex, fromRight) {
    var length = array.length, index = fromIndex + (fromRight ? 1 : -1);
    while (fromRight ? index-- : ++index < length) {
      if (predicate(array[index], index, array)) {
        return index;
      }
    }
    return -1;
  };
  var baseIndexOf = function(array, value, fromIndex) {
    if (value !== value) {
      return baseFindIndex(array, baseIsNaN, fromIndex);
    }
    var index = fromIndex - 1, length = array.length;
    while (++index < length) {
      if (array[index] === value) {
        return index;
      }
    }
    return -1;
  };
  var baseIsNaN = function(value) {
    return value !== value;
  };
  var baseTimes = function(n, iteratee) {
    var index = -1, result = Array(n);
    while (++index < n) {
      result[index] = iteratee(index);
    }
    return result;
  };
  var baseValues = function(object, props) {
    return arrayMap(props, function(key) {
      return object[key];
    });
  };
  var overArg = function(func, transform) {
    return function(arg) {
      return func(transform(arg));
    };
  };
  var arrayLikeKeys = function(value, inherited) {
    var result = isArray(value) || isArguments(value) ? baseTimes(value.length, String) : [];
    var length = result.length, skipIndexes = !!length;
    for (var key in value) {
      if ((inherited || hasOwnProperty.call(value, key)) && !(skipIndexes && (key == "length" || isIndex(key, length)))) {
        result.push(key);
      }
    }
    return result;
  };
  var baseKeys = function(object) {
    if (!isPrototype(object)) {
      return nativeKeys(object);
    }
    var result = [];
    for (var key in Object(object)) {
      if (hasOwnProperty.call(object, key) && key != "constructor") {
        result.push(key);
      }
    }
    return result;
  };
  var isIndex = function(value, length) {
    length = length == null ? MAX_SAFE_INTEGER : length;
    return !!length && (typeof value == "number" || reIsUint.test(value)) && (value > -1 && value % 1 == 0 && value < length);
  };
  var isPrototype = function(value) {
    var Ctor = value && value.constructor, proto = typeof Ctor == "function" && Ctor.prototype || objectProto;
    return value === proto;
  };
  var includes = function(collection, value, fromIndex, guard) {
    collection = isArrayLike(collection) ? collection : values(collection);
    fromIndex = fromIndex && !guard ? toInteger(fromIndex) : 0;
    var length = collection.length;
    if (fromIndex < 0) {
      fromIndex = nativeMax(length + fromIndex, 0);
    }
    return isString(collection) ? fromIndex <= length && collection.indexOf(value, fromIndex) > -1 : !!length && baseIndexOf(collection, value, fromIndex) > -1;
  };
  var isArguments = function(value) {
    return isArrayLikeObject(value) && hasOwnProperty.call(value, "callee") && (!propertyIsEnumerable.call(value, "callee") || objectToString.call(value) == argsTag);
  };
  var isArrayLike = function(value) {
    return value != null && isLength(value.length) && !isFunction(value);
  };
  var isArrayLikeObject = function(value) {
    return isObjectLike(value) && isArrayLike(value);
  };
  var isFunction = function(value) {
    var tag = isObject(value) ? objectToString.call(value) : "";
    return tag == funcTag || tag == genTag;
  };
  var isLength = function(value) {
    return typeof value == "number" && value > -1 && value % 1 == 0 && value <= MAX_SAFE_INTEGER;
  };
  var isObject = function(value) {
    var type = typeof value;
    return !!value && (type == "object" || type == "function");
  };
  var isObjectLike = function(value) {
    return !!value && typeof value == "object";
  };
  var isString = function(value) {
    return typeof value == "string" || !isArray(value) && isObjectLike(value) && objectToString.call(value) == stringTag;
  };
  var isSymbol = function(value) {
    return typeof value == "symbol" || isObjectLike(value) && objectToString.call(value) == symbolTag;
  };
  var toFinite = function(value) {
    if (!value) {
      return value === 0 ? value : 0;
    }
    value = toNumber(value);
    if (value === INFINITY || value === -INFINITY) {
      var sign = value < 0 ? -1 : 1;
      return sign * MAX_INTEGER;
    }
    return value === value ? value : 0;
  };
  var toInteger = function(value) {
    var result = toFinite(value), remainder = result % 1;
    return result === result ? remainder ? result - remainder : result : 0;
  };
  var toNumber = function(value) {
    if (typeof value == "number") {
      return value;
    }
    if (isSymbol(value)) {
      return NAN;
    }
    if (isObject(value)) {
      var other = typeof value.valueOf == "function" ? value.valueOf() : value;
      value = isObject(other) ? other + "" : other;
    }
    if (typeof value != "string") {
      return value === 0 ? value : +value;
    }
    value = value.replace(reTrim, "");
    var isBinary = reIsBinary.test(value);
    return isBinary || reIsOctal.test(value) ? freeParseInt(value.slice(2), isBinary ? 2 : 8) : reIsBadHex.test(value) ? NAN : +value;
  };
  var keys = function(object) {
    return isArrayLike(object) ? arrayLikeKeys(object) : baseKeys(object);
  };
  var values = function(object) {
    return object ? baseValues(object, keys(object)) : [];
  };
  var INFINITY = 1 / 0;
  var MAX_SAFE_INTEGER = 9007199254740991;
  var MAX_INTEGER = 179769313486231570000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000;
  var NAN = 0 / 0;
  var argsTag = "[object Arguments]";
  var funcTag = "[object Function]";
  var genTag = "[object GeneratorFunction]";
  var stringTag = "[object String]";
  var symbolTag = "[object Symbol]";
  var reTrim = /^\s+|\s+$/g;
  var reIsBadHex = /^[-+]0x[0-9a-f]+$/i;
  var reIsBinary = /^0b[01]+$/i;
  var reIsOctal = /^0o[0-7]+$/i;
  var reIsUint = /^(?:0|[1-9]\d*)$/;
  var freeParseInt = parseInt;
  var objectProto = Object.prototype;
  var hasOwnProperty = objectProto.hasOwnProperty;
  var objectToString = objectProto.toString;
  var propertyIsEnumerable = objectProto.propertyIsEnumerable;
  var nativeKeys = overArg(Object.keys, Object);
  var nativeMax = Math.max;
  var isArray = Array.isArray;
  module.exports = includes;
});

// node_modules/lodash.isboolean/index.js
var require_lodash2 = __commonJS((exports, module) => {
  var isBoolean = function(value) {
    return value === true || value === false || isObjectLike(value) && objectToString.call(value) == boolTag;
  };
  var isObjectLike = function(value) {
    return !!value && typeof value == "object";
  };
  var boolTag = "[object Boolean]";
  var objectProto = Object.prototype;
  var objectToString = objectProto.toString;
  module.exports = isBoolean;
});

// node_modules/lodash.isinteger/index.js
var require_lodash3 = __commonJS((exports, module) => {
  var isInteger = function(value) {
    return typeof value == "number" && value == toInteger(value);
  };
  var isObject = function(value) {
    var type = typeof value;
    return !!value && (type == "object" || type == "function");
  };
  var isObjectLike = function(value) {
    return !!value && typeof value == "object";
  };
  var isSymbol = function(value) {
    return typeof value == "symbol" || isObjectLike(value) && objectToString.call(value) == symbolTag;
  };
  var toFinite = function(value) {
    if (!value) {
      return value === 0 ? value : 0;
    }
    value = toNumber(value);
    if (value === INFINITY || value === -INFINITY) {
      var sign = value < 0 ? -1 : 1;
      return sign * MAX_INTEGER;
    }
    return value === value ? value : 0;
  };
  var toInteger = function(value) {
    var result = toFinite(value), remainder = result % 1;
    return result === result ? remainder ? result - remainder : result : 0;
  };
  var toNumber = function(value) {
    if (typeof value == "number") {
      return value;
    }
    if (isSymbol(value)) {
      return NAN;
    }
    if (isObject(value)) {
      var other = typeof value.valueOf == "function" ? value.valueOf() : value;
      value = isObject(other) ? other + "" : other;
    }
    if (typeof value != "string") {
      return value === 0 ? value : +value;
    }
    value = value.replace(reTrim, "");
    var isBinary = reIsBinary.test(value);
    return isBinary || reIsOctal.test(value) ? freeParseInt(value.slice(2), isBinary ? 2 : 8) : reIsBadHex.test(value) ? NAN : +value;
  };
  var INFINITY = 1 / 0;
  var MAX_INTEGER = 179769313486231570000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000;
  var NAN = 0 / 0;
  var symbolTag = "[object Symbol]";
  var reTrim = /^\s+|\s+$/g;
  var reIsBadHex = /^[-+]0x[0-9a-f]+$/i;
  var reIsBinary = /^0b[01]+$/i;
  var reIsOctal = /^0o[0-7]+$/i;
  var freeParseInt = parseInt;
  var objectProto = Object.prototype;
  var objectToString = objectProto.toString;
  module.exports = isInteger;
});

// node_modules/lodash.isnumber/index.js
var require_lodash4 = __commonJS((exports, module) => {
  var isObjectLike = function(value) {
    return !!value && typeof value == "object";
  };
  var isNumber = function(value) {
    return typeof value == "number" || isObjectLike(value) && objectToString.call(value) == numberTag;
  };
  var numberTag = "[object Number]";
  var objectProto = Object.prototype;
  var objectToString = objectProto.toString;
  module.exports = isNumber;
});

// node_modules/lodash.isplainobject/index.js
var require_lodash5 = __commonJS((exports, module) => {
  var isHostObject = function(value) {
    var result = false;
    if (value != null && typeof value.toString != "function") {
      try {
        result = !!(value + "");
      } catch (e) {
      }
    }
    return result;
  };
  var overArg = function(func, transform) {
    return function(arg) {
      return func(transform(arg));
    };
  };
  var isObjectLike = function(value) {
    return !!value && typeof value == "object";
  };
  var isPlainObject = function(value) {
    if (!isObjectLike(value) || objectToString.call(value) != objectTag || isHostObject(value)) {
      return false;
    }
    var proto = getPrototype(value);
    if (proto === null) {
      return true;
    }
    var Ctor = hasOwnProperty.call(proto, "constructor") && proto.constructor;
    return typeof Ctor == "function" && Ctor instanceof Ctor && funcToString.call(Ctor) == objectCtorString;
  };
  var objectTag = "[object Object]";
  var funcProto = Function.prototype;
  var objectProto = Object.prototype;
  var funcToString = funcProto.toString;
  var hasOwnProperty = objectProto.hasOwnProperty;
  var objectCtorString = funcToString.call(Object);
  var objectToString = objectProto.toString;
  var getPrototype = overArg(Object.getPrototypeOf, Object);
  module.exports = isPlainObject;
});

// node_modules/lodash.isstring/index.js
var require_lodash6 = __commonJS((exports, module) => {
  var isObjectLike = function(value) {
    return !!value && typeof value == "object";
  };
  var isString = function(value) {
    return typeof value == "string" || !isArray(value) && isObjectLike(value) && objectToString.call(value) == stringTag;
  };
  var stringTag = "[object String]";
  var objectProto = Object.prototype;
  var objectToString = objectProto.toString;
  var isArray = Array.isArray;
  module.exports = isString;
});

// node_modules/lodash.once/index.js
var require_lodash7 = __commonJS((exports, module) => {
  var before = function(n, func) {
    var result;
    if (typeof func != "function") {
      throw new TypeError(FUNC_ERROR_TEXT);
    }
    n = toInteger(n);
    return function() {
      if (--n > 0) {
        result = func.apply(this, arguments);
      }
      if (n <= 1) {
        func = undefined;
      }
      return result;
    };
  };
  var once = function(func) {
    return before(2, func);
  };
  var isObject = function(value) {
    var type = typeof value;
    return !!value && (type == "object" || type == "function");
  };
  var isObjectLike = function(value) {
    return !!value && typeof value == "object";
  };
  var isSymbol = function(value) {
    return typeof value == "symbol" || isObjectLike(value) && objectToString.call(value) == symbolTag;
  };
  var toFinite = function(value) {
    if (!value) {
      return value === 0 ? value : 0;
    }
    value = toNumber(value);
    if (value === INFINITY || value === -INFINITY) {
      var sign = value < 0 ? -1 : 1;
      return sign * MAX_INTEGER;
    }
    return value === value ? value : 0;
  };
  var toInteger = function(value) {
    var result = toFinite(value), remainder = result % 1;
    return result === result ? remainder ? result - remainder : result : 0;
  };
  var toNumber = function(value) {
    if (typeof value == "number") {
      return value;
    }
    if (isSymbol(value)) {
      return NAN;
    }
    if (isObject(value)) {
      var other = typeof value.valueOf == "function" ? value.valueOf() : value;
      value = isObject(other) ? other + "" : other;
    }
    if (typeof value != "string") {
      return value === 0 ? value : +value;
    }
    value = value.replace(reTrim, "");
    var isBinary = reIsBinary.test(value);
    return isBinary || reIsOctal.test(value) ? freeParseInt(value.slice(2), isBinary ? 2 : 8) : reIsBadHex.test(value) ? NAN : +value;
  };
  var FUNC_ERROR_TEXT = "Expected a function";
  var INFINITY = 1 / 0;
  var MAX_INTEGER = 179769313486231570000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000;
  var NAN = 0 / 0;
  var symbolTag = "[object Symbol]";
  var reTrim = /^\s+|\s+$/g;
  var reIsBadHex = /^[-+]0x[0-9a-f]+$/i;
  var reIsBinary = /^0b[01]+$/i;
  var reIsOctal = /^0o[0-7]+$/i;
  var freeParseInt = parseInt;
  var objectProto = Object.prototype;
  var objectToString = objectProto.toString;
  module.exports = once;
});

// node_modules/jsonwebtoken/sign.js
var require_sign = __commonJS((exports, module) => {
  var validate = function(schema, allowUnknown, object, parameterName) {
    if (!isPlainObject(object)) {
      throw new Error('Expected "' + parameterName + '" to be a plain object.');
    }
    Object.keys(object).forEach(function(key) {
      const validator = schema[key];
      if (!validator) {
        if (!allowUnknown) {
          throw new Error('"' + key + '" is not allowed in "' + parameterName + '"');
        }
        return;
      }
      if (!validator.isValid(object[key])) {
        throw new Error(validator.message);
      }
    });
  };
  var validateOptions = function(options) {
    return validate(sign_options_schema, false, options, "options");
  };
  var validatePayload = function(payload) {
    return validate(registered_claims_schema, true, payload, "payload");
  };
  var timespan = require_timespan();
  var PS_SUPPORTED = require_psSupported();
  var validateAsymmetricKey = require_validateAsymmetricKey();
  var jws = require_jws();
  var includes = require_lodash();
  var isBoolean = require_lodash2();
  var isInteger = require_lodash3();
  var isNumber = require_lodash4();
  var isPlainObject = require_lodash5();
  var isString = require_lodash6();
  var once = require_lodash7();
  var { KeyObject, createSecretKey, createPrivateKey } = import.meta.require("crypto");
  var SUPPORTED_ALGS = ["RS256", "RS384", "RS512", "ES256", "ES384", "ES512", "HS256", "HS384", "HS512", "none"];
  if (PS_SUPPORTED) {
    SUPPORTED_ALGS.splice(3, 0, "PS256", "PS384", "PS512");
  }
  var sign_options_schema = {
    expiresIn: { isValid: function(value) {
      return isInteger(value) || isString(value) && value;
    }, message: '"expiresIn" should be a number of seconds or string representing a timespan' },
    notBefore: { isValid: function(value) {
      return isInteger(value) || isString(value) && value;
    }, message: '"notBefore" should be a number of seconds or string representing a timespan' },
    audience: { isValid: function(value) {
      return isString(value) || Array.isArray(value);
    }, message: '"audience" must be a string or array' },
    algorithm: { isValid: includes.bind(null, SUPPORTED_ALGS), message: '"algorithm" must be a valid string enum value' },
    header: { isValid: isPlainObject, message: '"header" must be an object' },
    encoding: { isValid: isString, message: '"encoding" must be a string' },
    issuer: { isValid: isString, message: '"issuer" must be a string' },
    subject: { isValid: isString, message: '"subject" must be a string' },
    jwtid: { isValid: isString, message: '"jwtid" must be a string' },
    noTimestamp: { isValid: isBoolean, message: '"noTimestamp" must be a boolean' },
    keyid: { isValid: isString, message: '"keyid" must be a string' },
    mutatePayload: { isValid: isBoolean, message: '"mutatePayload" must be a boolean' },
    allowInsecureKeySizes: { isValid: isBoolean, message: '"allowInsecureKeySizes" must be a boolean' },
    allowInvalidAsymmetricKeyTypes: { isValid: isBoolean, message: '"allowInvalidAsymmetricKeyTypes" must be a boolean' }
  };
  var registered_claims_schema = {
    iat: { isValid: isNumber, message: '"iat" should be a number of seconds' },
    exp: { isValid: isNumber, message: '"exp" should be a number of seconds' },
    nbf: { isValid: isNumber, message: '"nbf" should be a number of seconds' }
  };
  var options_to_payload = {
    audience: "aud",
    issuer: "iss",
    subject: "sub",
    jwtid: "jti"
  };
  var options_for_objects = [
    "expiresIn",
    "notBefore",
    "noTimestamp",
    "audience",
    "issuer",
    "subject",
    "jwtid"
  ];
  module.exports = function(payload, secretOrPrivateKey, options, callback) {
    if (typeof options === "function") {
      callback = options;
      options = {};
    } else {
      options = options || {};
    }
    const isObjectPayload = typeof payload === "object" && !Buffer.isBuffer(payload);
    const header = Object.assign({
      alg: options.algorithm || "HS256",
      typ: isObjectPayload ? "JWT" : undefined,
      kid: options.keyid
    }, options.header);
    function failure(err) {
      if (callback) {
        return callback(err);
      }
      throw err;
    }
    if (!secretOrPrivateKey && options.algorithm !== "none") {
      return failure(new Error("secretOrPrivateKey must have a value"));
    }
    if (secretOrPrivateKey != null && !(secretOrPrivateKey instanceof KeyObject)) {
      try {
        secretOrPrivateKey = createPrivateKey(secretOrPrivateKey);
      } catch (_) {
        try {
          secretOrPrivateKey = createSecretKey(typeof secretOrPrivateKey === "string" ? Buffer.from(secretOrPrivateKey) : secretOrPrivateKey);
        } catch (_2) {
          return failure(new Error("secretOrPrivateKey is not valid key material"));
        }
      }
    }
    if (header.alg.startsWith("HS") && secretOrPrivateKey.type !== "secret") {
      return failure(new Error(`secretOrPrivateKey must be a symmetric key when using ${header.alg}`));
    } else if (/^(?:RS|PS|ES)/.test(header.alg)) {
      if (secretOrPrivateKey.type !== "private") {
        return failure(new Error(`secretOrPrivateKey must be an asymmetric key when using ${header.alg}`));
      }
      if (!options.allowInsecureKeySizes && !header.alg.startsWith("ES") && secretOrPrivateKey.asymmetricKeyDetails !== undefined && secretOrPrivateKey.asymmetricKeyDetails.modulusLength < 2048) {
        return failure(new Error(`secretOrPrivateKey has a minimum key size of 2048 bits for ${header.alg}`));
      }
    }
    if (typeof payload === "undefined") {
      return failure(new Error("payload is required"));
    } else if (isObjectPayload) {
      try {
        validatePayload(payload);
      } catch (error) {
        return failure(error);
      }
      if (!options.mutatePayload) {
        payload = Object.assign({}, payload);
      }
    } else {
      const invalid_options = options_for_objects.filter(function(opt) {
        return typeof options[opt] !== "undefined";
      });
      if (invalid_options.length > 0) {
        return failure(new Error("invalid " + invalid_options.join(",") + " option for " + typeof payload + " payload"));
      }
    }
    if (typeof payload.exp !== "undefined" && typeof options.expiresIn !== "undefined") {
      return failure(new Error('Bad "options.expiresIn" option the payload already has an "exp" property.'));
    }
    if (typeof payload.nbf !== "undefined" && typeof options.notBefore !== "undefined") {
      return failure(new Error('Bad "options.notBefore" option the payload already has an "nbf" property.'));
    }
    try {
      validateOptions(options);
    } catch (error) {
      return failure(error);
    }
    if (!options.allowInvalidAsymmetricKeyTypes) {
      try {
        validateAsymmetricKey(header.alg, secretOrPrivateKey);
      } catch (error) {
        return failure(error);
      }
    }
    const timestamp = payload.iat || Math.floor(Date.now() / 1000);
    if (options.noTimestamp) {
      delete payload.iat;
    } else if (isObjectPayload) {
      payload.iat = timestamp;
    }
    if (typeof options.notBefore !== "undefined") {
      try {
        payload.nbf = timespan(options.notBefore, timestamp);
      } catch (err) {
        return failure(err);
      }
      if (typeof payload.nbf === "undefined") {
        return failure(new Error('"notBefore" should be a number of seconds or string representing a timespan eg: "1d", "20h", 60'));
      }
    }
    if (typeof options.expiresIn !== "undefined" && typeof payload === "object") {
      try {
        payload.exp = timespan(options.expiresIn, timestamp);
      } catch (err) {
        return failure(err);
      }
      if (typeof payload.exp === "undefined") {
        return failure(new Error('"expiresIn" should be a number of seconds or string representing a timespan eg: "1d", "20h", 60'));
      }
    }
    Object.keys(options_to_payload).forEach(function(key) {
      const claim = options_to_payload[key];
      if (typeof options[key] !== "undefined") {
        if (typeof payload[claim] !== "undefined") {
          return failure(new Error('Bad "options.' + key + '" option. The payload already has an "' + claim + '" property.'));
        }
        payload[claim] = options[key];
      }
    });
    const encoding = options.encoding || "utf8";
    if (typeof callback === "function") {
      callback = callback && once(callback);
      jws.createSign({
        header,
        privateKey: secretOrPrivateKey,
        payload,
        encoding
      }).once("error", callback).once("done", function(signature) {
        if (!options.allowInsecureKeySizes && /^(?:RS|PS)/.test(header.alg) && signature.length < 256) {
          return callback(new Error(`secretOrPrivateKey has a minimum key size of 2048 bits for ${header.alg}`));
        }
        callback(null, signature);
      });
    } else {
      let signature = jws.sign({ header, payload, secret: secretOrPrivateKey, encoding });
      if (!options.allowInsecureKeySizes && /^(?:RS|PS)/.test(header.alg) && signature.length < 256) {
        throw new Error(`secretOrPrivateKey has a minimum key size of 2048 bits for ${header.alg}`);
      }
      return signature;
    }
  };
});

// node_modules/jsonwebtoken/index.js
var require_jsonwebtoken = __commonJS((exports, module) => {
  module.exports = {
    decode: require_decode(),
    verify: require_verify(),
    sign: require_sign(),
    JsonWebTokenError: require_JsonWebTokenError(),
    NotBeforeError: require_NotBeforeError(),
    TokenExpiredError: require_TokenExpiredError()
  };
});

// node_modules/hono/dist/utils/html.js
var HtmlEscapedCallbackPhase = {
  Stringify: 1,
  BeforeStream: 2,
  Stream: 3
};
var raw2 = (value, callbacks) => {
  const escapedString = new String(value);
  escapedString.isEscaped = true;
  escapedString.callbacks = callbacks;
  return escapedString;
};
var resolveCallback = async (str, phase, preserveCallbacks, context, buffer) => {
  const callbacks = str.callbacks;
  if (!callbacks?.length) {
    return Promise.resolve(str);
  }
  if (buffer) {
    buffer[0] += str;
  } else {
    buffer = [str];
  }
  const resStr = Promise.all(callbacks.map((c) => c({ phase, buffer, context }))).then((res) => Promise.all(res.filter(Boolean).map((str2) => resolveCallback(str2, phase, false, context, buffer))).then(() => buffer[0]));
  if (preserveCallbacks) {
    return raw2(await resStr, callbacks);
  } else {
    return resStr;
  }
};

// node_modules/hono/dist/context.js
var __accessCheck = (obj, member, msg) => {
  if (!member.has(obj))
    throw TypeError("Cannot " + msg);
};
var __privateGet = (obj, member, getter) => {
  __accessCheck(obj, member, "read from private field");
  return getter ? getter.call(obj) : member.get(obj);
};
var __privateAdd = (obj, member, value) => {
  if (member.has(obj))
    throw TypeError("Cannot add the same private member more than once");
  member instanceof WeakSet ? member.add(obj) : member.set(obj, value);
};
var __privateSet = (obj, member, value, setter) => {
  __accessCheck(obj, member, "write to private field");
  setter ? setter.call(obj, value) : member.set(obj, value);
  return value;
};
var TEXT_PLAIN = "text/plain; charset=UTF-8";
var setHeaders = (headers, map = {}) => {
  Object.entries(map).forEach(([key, value]) => headers.set(key, value));
  return headers;
};
var _status;
var _executionCtx;
var _headers;
var _preparedHeaders;
var _res;
var _isFresh;
var Context = class {
  constructor(req, options) {
    this.env = {};
    this._var = {};
    this.finalized = false;
    this.error = undefined;
    __privateAdd(this, _status, 200);
    __privateAdd(this, _executionCtx, undefined);
    __privateAdd(this, _headers, undefined);
    __privateAdd(this, _preparedHeaders, undefined);
    __privateAdd(this, _res, undefined);
    __privateAdd(this, _isFresh, true);
    this.layout = undefined;
    this.renderer = (content) => this.html(content);
    this.notFoundHandler = () => new Response;
    this.render = (...args) => this.renderer(...args);
    this.setLayout = (layout) => this.layout = layout;
    this.getLayout = () => this.layout;
    this.setRenderer = (renderer) => {
      this.renderer = renderer;
    };
    this.header = (name, value, options2) => {
      if (value === undefined) {
        if (__privateGet(this, _headers)) {
          __privateGet(this, _headers).delete(name);
        } else if (__privateGet(this, _preparedHeaders)) {
          delete __privateGet(this, _preparedHeaders)[name.toLocaleLowerCase()];
        }
        if (this.finalized) {
          this.res.headers.delete(name);
        }
        return;
      }
      if (options2?.append) {
        if (!__privateGet(this, _headers)) {
          __privateSet(this, _isFresh, false);
          __privateSet(this, _headers, new Headers(__privateGet(this, _preparedHeaders)));
          __privateSet(this, _preparedHeaders, {});
        }
        __privateGet(this, _headers).append(name, value);
      } else {
        if (__privateGet(this, _headers)) {
          __privateGet(this, _headers).set(name, value);
        } else {
          __privateGet(this, _preparedHeaders) ?? __privateSet(this, _preparedHeaders, {});
          __privateGet(this, _preparedHeaders)[name.toLowerCase()] = value;
        }
      }
      if (this.finalized) {
        if (options2?.append) {
          this.res.headers.append(name, value);
        } else {
          this.res.headers.set(name, value);
        }
      }
    };
    this.status = (status) => {
      __privateSet(this, _isFresh, false);
      __privateSet(this, _status, status);
    };
    this.set = (key, value) => {
      this._var ?? (this._var = {});
      this._var[key] = value;
    };
    this.get = (key) => {
      return this._var ? this._var[key] : undefined;
    };
    this.newResponse = (data, arg, headers) => {
      if (__privateGet(this, _isFresh) && !headers && !arg && __privateGet(this, _status) === 200) {
        return new Response(data, {
          headers: __privateGet(this, _preparedHeaders)
        });
      }
      if (arg && typeof arg !== "number") {
        const headers2 = setHeaders(new Headers(arg.headers), __privateGet(this, _preparedHeaders));
        return new Response(data, {
          headers: headers2,
          status: arg.status ?? __privateGet(this, _status)
        });
      }
      const status = typeof arg === "number" ? arg : __privateGet(this, _status);
      __privateGet(this, _preparedHeaders) ?? __privateSet(this, _preparedHeaders, {});
      __privateGet(this, _headers) ?? __privateSet(this, _headers, new Headers);
      setHeaders(__privateGet(this, _headers), __privateGet(this, _preparedHeaders));
      if (__privateGet(this, _res)) {
        __privateGet(this, _res).headers.forEach((v, k) => {
          __privateGet(this, _headers)?.set(k, v);
        });
        setHeaders(__privateGet(this, _headers), __privateGet(this, _preparedHeaders));
      }
      headers ?? (headers = {});
      for (const [k, v] of Object.entries(headers)) {
        if (typeof v === "string") {
          __privateGet(this, _headers).set(k, v);
        } else {
          __privateGet(this, _headers).delete(k);
          for (const v2 of v) {
            __privateGet(this, _headers).append(k, v2);
          }
        }
      }
      return new Response(data, {
        status,
        headers: __privateGet(this, _headers)
      });
    };
    this.body = (data, arg, headers) => {
      return typeof arg === "number" ? this.newResponse(data, arg, headers) : this.newResponse(data, arg);
    };
    this.text = (text, arg, headers) => {
      if (!__privateGet(this, _preparedHeaders)) {
        if (__privateGet(this, _isFresh) && !headers && !arg) {
          return new Response(text);
        }
        __privateSet(this, _preparedHeaders, {});
      }
      __privateGet(this, _preparedHeaders)["content-type"] = TEXT_PLAIN;
      return typeof arg === "number" ? this.newResponse(text, arg, headers) : this.newResponse(text, arg);
    };
    this.json = (object, arg, headers) => {
      const body = JSON.stringify(object);
      __privateGet(this, _preparedHeaders) ?? __privateSet(this, _preparedHeaders, {});
      __privateGet(this, _preparedHeaders)["content-type"] = "application/json; charset=UTF-8";
      return typeof arg === "number" ? this.newResponse(body, arg, headers) : this.newResponse(body, arg);
    };
    this.html = (html2, arg, headers) => {
      __privateGet(this, _preparedHeaders) ?? __privateSet(this, _preparedHeaders, {});
      __privateGet(this, _preparedHeaders)["content-type"] = "text/html; charset=UTF-8";
      if (typeof html2 === "object") {
        if (!(html2 instanceof Promise)) {
          html2 = html2.toString();
        }
        if (html2 instanceof Promise) {
          return html2.then((html22) => resolveCallback(html22, HtmlEscapedCallbackPhase.Stringify, false, {})).then((html22) => {
            return typeof arg === "number" ? this.newResponse(html22, arg, headers) : this.newResponse(html22, arg);
          });
        }
      }
      return typeof arg === "number" ? this.newResponse(html2, arg, headers) : this.newResponse(html2, arg);
    };
    this.redirect = (location, status = 302) => {
      __privateGet(this, _headers) ?? __privateSet(this, _headers, new Headers);
      __privateGet(this, _headers).set("Location", location);
      return this.newResponse(null, status);
    };
    this.notFound = () => {
      return this.notFoundHandler(this);
    };
    this.req = req;
    if (options) {
      __privateSet(this, _executionCtx, options.executionCtx);
      this.env = options.env;
      if (options.notFoundHandler) {
        this.notFoundHandler = options.notFoundHandler;
      }
    }
  }
  get event() {
    if (__privateGet(this, _executionCtx) && "respondWith" in __privateGet(this, _executionCtx)) {
      return __privateGet(this, _executionCtx);
    } else {
      throw Error("This context has no FetchEvent");
    }
  }
  get executionCtx() {
    if (__privateGet(this, _executionCtx)) {
      return __privateGet(this, _executionCtx);
    } else {
      throw Error("This context has no ExecutionContext");
    }
  }
  get res() {
    __privateSet(this, _isFresh, false);
    return __privateGet(this, _res) || __privateSet(this, _res, new Response("404 Not Found", { status: 404 }));
  }
  set res(_res2) {
    __privateSet(this, _isFresh, false);
    if (__privateGet(this, _res) && _res2) {
      __privateGet(this, _res).headers.delete("content-type");
      for (const [k, v] of __privateGet(this, _res).headers.entries()) {
        if (k === "set-cookie") {
          const cookies = __privateGet(this, _res).headers.getSetCookie();
          _res2.headers.delete("set-cookie");
          for (const cookie of cookies) {
            _res2.headers.append("set-cookie", cookie);
          }
        } else {
          _res2.headers.set(k, v);
        }
      }
    }
    __privateSet(this, _res, _res2);
    this.finalized = true;
  }
  get var() {
    return { ...this._var };
  }
};
_status = new WeakMap;
_executionCtx = new WeakMap;
_headers = new WeakMap;
_preparedHeaders = new WeakMap;
_res = new WeakMap;
_isFresh = new WeakMap;

// node_modules/hono/dist/compose.js
var compose = (middleware, onError, onNotFound) => {
  return (context2, next) => {
    let index = -1;
    return dispatch(0);
    async function dispatch(i) {
      if (i <= index) {
        throw new Error("next() called multiple times");
      }
      index = i;
      let res;
      let isError = false;
      let handler;
      if (middleware[i]) {
        handler = middleware[i][0][0];
        if (context2 instanceof Context) {
          context2.req.routeIndex = i;
        }
      } else {
        handler = i === middleware.length && next || undefined;
      }
      if (!handler) {
        if (context2 instanceof Context && context2.finalized === false && onNotFound) {
          res = await onNotFound(context2);
        }
      } else {
        try {
          res = await handler(context2, () => {
            return dispatch(i + 1);
          });
        } catch (err) {
          if (err instanceof Error && context2 instanceof Context && onError) {
            context2.error = err;
            res = await onError(err, context2);
            isError = true;
          } else {
            throw err;
          }
        }
      }
      if (res && (context2.finalized === false || isError)) {
        context2.res = res;
      }
      return context2;
    }
  };
};

// node_modules/hono/dist/http-exception.js
var HTTPException = class extends Error {
  constructor(status = 500, options) {
    super(options?.message);
    this.res = options?.res;
    this.status = status;
  }
  getResponse() {
    if (this.res) {
      return this.res;
    }
    return new Response(this.message, {
      status: this.status
    });
  }
};

// node_modules/hono/dist/utils/body.js
var isFormDataContent = function(contentType) {
  if (contentType === null) {
    return false;
  }
  return contentType.startsWith("multipart/form-data") || contentType.startsWith("application/x-www-form-urlencoded");
};
async function parseFormData(request2, options) {
  const formData = await request2.formData();
  if (formData) {
    return convertFormDataToBodyData(formData, options);
  }
  return {};
}
var convertFormDataToBodyData = function(formData, options) {
  const form = {};
  formData.forEach((value, key) => {
    const shouldParseAllValues = options.all || key.endsWith("[]");
    if (!shouldParseAllValues) {
      form[key] = value;
    } else {
      handleParsingAllValues(form, key, value);
    }
  });
  return form;
};
var isArrayField = function(field) {
  return Array.isArray(field);
};
var parseBody = async (request2, options = { all: false }) => {
  const headers = request2 instanceof HonoRequest ? request2.raw.headers : request2.headers;
  const contentType = headers.get("Content-Type");
  if (isFormDataContent(contentType)) {
    return parseFormData(request2, options);
  }
  return {};
};
var handleParsingAllValues = (form, key, value) => {
  if (form[key] && isArrayField(form[key])) {
    appendToExistingArray(form[key], value);
  } else if (form[key]) {
    convertToNewArray(form, key, value);
  } else {
    form[key] = value;
  }
};
var appendToExistingArray = (arr, value) => {
  arr.push(value);
};
var convertToNewArray = (form, key, value) => {
  form[key] = [form[key], value];
};

// node_modules/hono/dist/utils/url.js
var splitPath = (path) => {
  const paths = path.split("/");
  if (paths[0] === "") {
    paths.shift();
  }
  return paths;
};
var splitRoutingPath = (routePath) => {
  const { groups, path } = extractGroupsFromPath(routePath);
  const paths = splitPath(path);
  return replaceGroupMarks(paths, groups);
};
var extractGroupsFromPath = (path) => {
  const groups = [];
  path = path.replace(/\{[^}]+\}/g, (match, index) => {
    const mark = `@${index}`;
    groups.push([mark, match]);
    return mark;
  });
  return { groups, path };
};
var replaceGroupMarks = (paths, groups) => {
  for (let i = groups.length - 1;i >= 0; i--) {
    const [mark] = groups[i];
    for (let j = paths.length - 1;j >= 0; j--) {
      if (paths[j].includes(mark)) {
        paths[j] = paths[j].replace(mark, groups[i][1]);
        break;
      }
    }
  }
  return paths;
};
var patternCache = {};
var getPattern = (label) => {
  if (label === "*") {
    return "*";
  }
  const match = label.match(/^\:([^\{\}]+)(?:\{(.+)\})?$/);
  if (match) {
    if (!patternCache[label]) {
      if (match[2]) {
        patternCache[label] = [label, match[1], new RegExp("^" + match[2] + "$")];
      } else {
        patternCache[label] = [label, match[1], true];
      }
    }
    return patternCache[label];
  }
  return null;
};
var getPath = (request2) => {
  const match = request2.url.match(/^https?:\/\/[^/]+(\/[^?]*)/);
  return match ? match[1] : "";
};
var getQueryStrings = (url) => {
  const queryIndex = url.indexOf("?", 8);
  return queryIndex === -1 ? "" : "?" + url.slice(queryIndex + 1);
};
var getPathNoStrict = (request2) => {
  const result = getPath(request2);
  return result.length > 1 && result[result.length - 1] === "/" ? result.slice(0, -1) : result;
};
var mergePath = (...paths) => {
  let p = "";
  let endsWithSlash = false;
  for (let path of paths) {
    if (p[p.length - 1] === "/") {
      p = p.slice(0, -1);
      endsWithSlash = true;
    }
    if (path[0] !== "/") {
      path = `/${path}`;
    }
    if (path === "/" && endsWithSlash) {
      p = `${p}/`;
    } else if (path !== "/") {
      p = `${p}${path}`;
    }
    if (path === "/" && p === "") {
      p = "/";
    }
  }
  return p;
};
var checkOptionalParameter = (path) => {
  if (!path.match(/\:.+\?$/)) {
    return null;
  }
  const segments = path.split("/");
  const results = [];
  let basePath = "";
  segments.forEach((segment) => {
    if (segment !== "" && !/\:/.test(segment)) {
      basePath += "/" + segment;
    } else if (/\:/.test(segment)) {
      if (/\?/.test(segment)) {
        if (results.length === 0 && basePath === "") {
          results.push("/");
        } else {
          results.push(basePath);
        }
        const optionalSegment = segment.replace("?", "");
        basePath += "/" + optionalSegment;
        results.push(basePath);
      } else {
        basePath += "/" + segment;
      }
    }
  });
  return results.filter((v, i, a) => a.indexOf(v) === i);
};
var _decodeURI = (value) => {
  if (!/[%+]/.test(value)) {
    return value;
  }
  if (value.indexOf("+") !== -1) {
    value = value.replace(/\+/g, " ");
  }
  return /%/.test(value) ? decodeURIComponent_(value) : value;
};
var _getQueryParam = (url, key, multiple) => {
  let encoded;
  if (!multiple && key && !/[%+]/.test(key)) {
    let keyIndex2 = url.indexOf(`?${key}`, 8);
    if (keyIndex2 === -1) {
      keyIndex2 = url.indexOf(`&${key}`, 8);
    }
    while (keyIndex2 !== -1) {
      const trailingKeyCode = url.charCodeAt(keyIndex2 + key.length + 1);
      if (trailingKeyCode === 61) {
        const valueIndex = keyIndex2 + key.length + 2;
        const endIndex = url.indexOf("&", valueIndex);
        return _decodeURI(url.slice(valueIndex, endIndex === -1 ? undefined : endIndex));
      } else if (trailingKeyCode == 38 || isNaN(trailingKeyCode)) {
        return "";
      }
      keyIndex2 = url.indexOf(`&${key}`, keyIndex2 + 1);
    }
    encoded = /[%+]/.test(url);
    if (!encoded) {
      return;
    }
  }
  const results = {};
  encoded ?? (encoded = /[%+]/.test(url));
  let keyIndex = url.indexOf("?", 8);
  while (keyIndex !== -1) {
    const nextKeyIndex = url.indexOf("&", keyIndex + 1);
    let valueIndex = url.indexOf("=", keyIndex);
    if (valueIndex > nextKeyIndex && nextKeyIndex !== -1) {
      valueIndex = -1;
    }
    let name = url.slice(keyIndex + 1, valueIndex === -1 ? nextKeyIndex === -1 ? undefined : nextKeyIndex : valueIndex);
    if (encoded) {
      name = _decodeURI(name);
    }
    keyIndex = nextKeyIndex;
    if (name === "") {
      continue;
    }
    let value;
    if (valueIndex === -1) {
      value = "";
    } else {
      value = url.slice(valueIndex + 1, nextKeyIndex === -1 ? undefined : nextKeyIndex);
      if (encoded) {
        value = _decodeURI(value);
      }
    }
    if (multiple) {
      if (!(results[name] && Array.isArray(results[name]))) {
        results[name] = [];
      }
      results[name].push(value);
    } else {
      results[name] ?? (results[name] = value);
    }
  }
  return key ? results[key] : results;
};
var getQueryParam = _getQueryParam;
var getQueryParams = (url, key) => {
  return _getQueryParam(url, key, true);
};
var decodeURIComponent_ = decodeURIComponent;

// node_modules/hono/dist/request.js
var __accessCheck2 = (obj, member, msg) => {
  if (!member.has(obj))
    throw TypeError("Cannot " + msg);
};
var __privateGet2 = (obj, member, getter) => {
  __accessCheck2(obj, member, "read from private field");
  return getter ? getter.call(obj) : member.get(obj);
};
var __privateAdd2 = (obj, member, value) => {
  if (member.has(obj))
    throw TypeError("Cannot add the same private member more than once");
  member instanceof WeakSet ? member.add(obj) : member.set(obj, value);
};
var __privateSet2 = (obj, member, value, setter) => {
  __accessCheck2(obj, member, "write to private field");
  setter ? setter.call(obj, value) : member.set(obj, value);
  return value;
};
var _validatedData;
var _matchResult;
var HonoRequest = class {
  constructor(request2, path = "/", matchResult = [[]]) {
    __privateAdd2(this, _validatedData, undefined);
    __privateAdd2(this, _matchResult, undefined);
    this.routeIndex = 0;
    this.bodyCache = {};
    this.cachedBody = (key) => {
      const { bodyCache, raw: raw3 } = this;
      const cachedBody = bodyCache[key];
      if (cachedBody) {
        return cachedBody;
      }
      if (bodyCache.arrayBuffer) {
        return (async () => {
          return await new Response(bodyCache.arrayBuffer)[key]();
        })();
      }
      return bodyCache[key] = raw3[key]();
    };
    this.raw = request2;
    this.path = path;
    __privateSet2(this, _matchResult, matchResult);
    __privateSet2(this, _validatedData, {});
  }
  param(key) {
    return key ? this.getDecodedParam(key) : this.getAllDecodedParams();
  }
  getDecodedParam(key) {
    const paramKey = __privateGet2(this, _matchResult)[0][this.routeIndex][1][key];
    const param = this.getParamValue(paramKey);
    return param ? /\%/.test(param) ? decodeURIComponent_(param) : param : undefined;
  }
  getAllDecodedParams() {
    const decoded = {};
    const keys = Object.keys(__privateGet2(this, _matchResult)[0][this.routeIndex][1]);
    for (const key of keys) {
      const value = this.getParamValue(__privateGet2(this, _matchResult)[0][this.routeIndex][1][key]);
      if (value && typeof value === "string") {
        decoded[key] = /\%/.test(value) ? decodeURIComponent_(value) : value;
      }
    }
    return decoded;
  }
  getParamValue(paramKey) {
    return __privateGet2(this, _matchResult)[1] ? __privateGet2(this, _matchResult)[1][paramKey] : paramKey;
  }
  query(key) {
    return getQueryParam(this.url, key);
  }
  queries(key) {
    return getQueryParams(this.url, key);
  }
  header(name) {
    if (name) {
      return this.raw.headers.get(name.toLowerCase()) ?? undefined;
    }
    const headerData = {};
    this.raw.headers.forEach((value, key) => {
      headerData[key] = value;
    });
    return headerData;
  }
  async parseBody(options) {
    if (this.bodyCache.parsedBody) {
      return this.bodyCache.parsedBody;
    }
    const parsedBody = await parseBody(this, options);
    this.bodyCache.parsedBody = parsedBody;
    return parsedBody;
  }
  json() {
    return this.cachedBody("json");
  }
  text() {
    return this.cachedBody("text");
  }
  arrayBuffer() {
    return this.cachedBody("arrayBuffer");
  }
  blob() {
    return this.cachedBody("blob");
  }
  formData() {
    return this.cachedBody("formData");
  }
  addValidatedData(target, data) {
    __privateGet2(this, _validatedData)[target] = data;
  }
  valid(target) {
    return __privateGet2(this, _validatedData)[target];
  }
  get url() {
    return this.raw.url;
  }
  get method() {
    return this.raw.method;
  }
  get matchedRoutes() {
    return __privateGet2(this, _matchResult)[0].map(([[, route]]) => route);
  }
  get routePath() {
    return __privateGet2(this, _matchResult)[0].map(([[, route]]) => route)[this.routeIndex].path;
  }
};
_validatedData = new WeakMap;
_matchResult = new WeakMap;

// node_modules/hono/dist/router.js
var METHOD_NAME_ALL = "ALL";
var METHOD_NAME_ALL_LOWERCASE = "all";
var METHODS = ["get", "post", "put", "delete", "options", "patch"];
var MESSAGE_MATCHER_IS_ALREADY_BUILT = "Can not add a route since the matcher is already built.";
var UnsupportedPathError = class extends Error {
};

// node_modules/hono/dist/hono-base.js
var defineDynamicClass = function() {
  return class {
  };
};
var __accessCheck3 = (obj, member, msg) => {
  if (!member.has(obj))
    throw TypeError("Cannot " + msg);
};
var __privateGet3 = (obj, member, getter) => {
  __accessCheck3(obj, member, "read from private field");
  return getter ? getter.call(obj) : member.get(obj);
};
var __privateAdd3 = (obj, member, value) => {
  if (member.has(obj))
    throw TypeError("Cannot add the same private member more than once");
  member instanceof WeakSet ? member.add(obj) : member.set(obj, value);
};
var __privateSet3 = (obj, member, value, setter) => {
  __accessCheck3(obj, member, "write to private field");
  setter ? setter.call(obj, value) : member.set(obj, value);
  return value;
};
var COMPOSED_HANDLER = Symbol("composedHandler");
var notFoundHandler = (c) => {
  return c.text("404 Not Found", 404);
};
var errorHandler = (err, c) => {
  if (err instanceof HTTPException) {
    return err.getResponse();
  }
  console.error(err);
  return c.text("Internal Server Error", 500);
};
var _path;
var _Hono = class extends defineDynamicClass() {
  constructor(options = {}) {
    super();
    this._basePath = "/";
    __privateAdd3(this, _path, "/");
    this.routes = [];
    this.notFoundHandler = notFoundHandler;
    this.errorHandler = errorHandler;
    this.onError = (handler) => {
      this.errorHandler = handler;
      return this;
    };
    this.notFound = (handler) => {
      this.notFoundHandler = handler;
      return this;
    };
    this.fetch = (request3, Env, executionCtx) => {
      return this.dispatch(request3, executionCtx, Env, request3.method);
    };
    this.request = (input, requestInit, Env, executionCtx) => {
      if (input instanceof Request) {
        if (requestInit !== undefined) {
          input = new Request(input, requestInit);
        }
        return this.fetch(input, Env, executionCtx);
      }
      input = input.toString();
      const path = /^https?:\/\//.test(input) ? input : `http://localhost${mergePath("/", input)}`;
      const req = new Request(path, requestInit);
      return this.fetch(req, Env, executionCtx);
    };
    this.fire = () => {
      addEventListener("fetch", (event) => {
        event.respondWith(this.dispatch(event.request, event, undefined, event.request.method));
      });
    };
    const allMethods = [...METHODS, METHOD_NAME_ALL_LOWERCASE];
    allMethods.map((method) => {
      this[method] = (args1, ...args) => {
        if (typeof args1 === "string") {
          __privateSet3(this, _path, args1);
        } else {
          this.addRoute(method, __privateGet3(this, _path), args1);
        }
        args.map((handler) => {
          if (typeof handler !== "string") {
            this.addRoute(method, __privateGet3(this, _path), handler);
          }
        });
        return this;
      };
    });
    this.on = (method, path, ...handlers) => {
      if (!method) {
        return this;
      }
      for (const p of [path].flat()) {
        __privateSet3(this, _path, p);
        for (const m of [method].flat()) {
          handlers.map((handler) => {
            this.addRoute(m.toUpperCase(), __privateGet3(this, _path), handler);
          });
        }
      }
      return this;
    };
    this.use = (arg1, ...handlers) => {
      if (typeof arg1 === "string") {
        __privateSet3(this, _path, arg1);
      } else {
        __privateSet3(this, _path, "*");
        handlers.unshift(arg1);
      }
      handlers.map((handler) => {
        this.addRoute(METHOD_NAME_ALL, __privateGet3(this, _path), handler);
      });
      return this;
    };
    const strict = options.strict ?? true;
    delete options.strict;
    Object.assign(this, options);
    this.getPath = strict ? options.getPath ?? getPath : getPathNoStrict;
  }
  clone() {
    const clone = new _Hono({
      router: this.router,
      getPath: this.getPath
    });
    clone.routes = this.routes;
    return clone;
  }
  route(path, app) {
    const subApp = this.basePath(path);
    if (!app) {
      return subApp;
    }
    app.routes.map((r) => {
      let handler;
      if (app.errorHandler === errorHandler) {
        handler = r.handler;
      } else {
        handler = async (c, next) => (await compose([], app.errorHandler)(c, () => r.handler(c, next))).res;
        handler[COMPOSED_HANDLER] = r.handler;
      }
      subApp.addRoute(r.method, r.path, handler);
    });
    return this;
  }
  basePath(path) {
    const subApp = this.clone();
    subApp._basePath = mergePath(this._basePath, path);
    return subApp;
  }
  mount(path, applicationHandler, optionHandler) {
    const mergedPath = mergePath(this._basePath, path);
    const pathPrefixLength = mergedPath === "/" ? 0 : mergedPath.length;
    const handler = async (c, next) => {
      let executionContext = undefined;
      try {
        executionContext = c.executionCtx;
      } catch {
      }
      const options = optionHandler ? optionHandler(c) : [c.env, executionContext];
      const optionsArray = Array.isArray(options) ? options : [options];
      const queryStrings = getQueryStrings(c.req.url);
      const res = await applicationHandler(new Request(new URL((c.req.path.slice(pathPrefixLength) || "/") + queryStrings, c.req.url), c.req.raw), ...optionsArray);
      if (res) {
        return res;
      }
      await next();
    };
    this.addRoute(METHOD_NAME_ALL, mergePath(path, "*"), handler);
    return this;
  }
  addRoute(method, path, handler) {
    method = method.toUpperCase();
    path = mergePath(this._basePath, path);
    const r = { path, method, handler };
    this.router.add(method, path, [handler, r]);
    this.routes.push(r);
  }
  matchRoute(method, path) {
    return this.router.match(method, path);
  }
  handleError(err, c) {
    if (err instanceof Error) {
      return this.errorHandler(err, c);
    }
    throw err;
  }
  dispatch(request3, executionCtx, env, method) {
    if (method === "HEAD") {
      return (async () => new Response(null, await this.dispatch(request3, executionCtx, env, "GET")))();
    }
    const path = this.getPath(request3, { env });
    const matchResult = this.matchRoute(method, path);
    const c = new Context(new HonoRequest(request3, path, matchResult), {
      env,
      executionCtx,
      notFoundHandler: this.notFoundHandler
    });
    if (matchResult[0].length === 1) {
      let res;
      try {
        res = matchResult[0][0][0][0](c, async () => {
          c.res = await this.notFoundHandler(c);
        });
      } catch (err) {
        return this.handleError(err, c);
      }
      return res instanceof Promise ? res.then((resolved) => resolved || (c.finalized ? c.res : this.notFoundHandler(c))).catch((err) => this.handleError(err, c)) : res;
    }
    const composed = compose(matchResult[0], this.errorHandler, this.notFoundHandler);
    return (async () => {
      try {
        const context3 = await composed(c);
        if (!context3.finalized) {
          throw new Error("Context is not finalized. You may forget returning Response object or `await next()`");
        }
        return context3.res;
      } catch (err) {
        return this.handleError(err, c);
      }
    })();
  }
};
var Hono = _Hono;
_path = new WeakMap;

// node_modules/hono/dist/router/reg-exp-router/node.js
var compareKey = function(a, b) {
  if (a.length === 1) {
    return b.length === 1 ? a < b ? -1 : 1 : -1;
  }
  if (b.length === 1) {
    return 1;
  }
  if (a === ONLY_WILDCARD_REG_EXP_STR || a === TAIL_WILDCARD_REG_EXP_STR) {
    return 1;
  } else if (b === ONLY_WILDCARD_REG_EXP_STR || b === TAIL_WILDCARD_REG_EXP_STR) {
    return -1;
  }
  if (a === LABEL_REG_EXP_STR) {
    return 1;
  } else if (b === LABEL_REG_EXP_STR) {
    return -1;
  }
  return a.length === b.length ? a < b ? -1 : 1 : b.length - a.length;
};
var LABEL_REG_EXP_STR = "[^/]+";
var ONLY_WILDCARD_REG_EXP_STR = ".*";
var TAIL_WILDCARD_REG_EXP_STR = "(?:|/.*)";
var PATH_ERROR = Symbol();
var Node = class {
  constructor() {
    this.children = {};
  }
  insert(tokens, index, paramMap, context3, pathErrorCheckOnly) {
    if (tokens.length === 0) {
      if (this.index !== undefined) {
        throw PATH_ERROR;
      }
      if (pathErrorCheckOnly) {
        return;
      }
      this.index = index;
      return;
    }
    const [token, ...restTokens] = tokens;
    const pattern = token === "*" ? restTokens.length === 0 ? ["", "", ONLY_WILDCARD_REG_EXP_STR] : ["", "", LABEL_REG_EXP_STR] : token === "/*" ? ["", "", TAIL_WILDCARD_REG_EXP_STR] : token.match(/^\:([^\{\}]+)(?:\{(.+)\})?$/);
    let node;
    if (pattern) {
      const name = pattern[1];
      let regexpStr = pattern[2] || LABEL_REG_EXP_STR;
      if (name && pattern[2]) {
        regexpStr = regexpStr.replace(/^\((?!\?:)(?=[^)]+\)$)/, "(?:");
        if (/\((?!\?:)/.test(regexpStr)) {
          throw PATH_ERROR;
        }
      }
      node = this.children[regexpStr];
      if (!node) {
        if (Object.keys(this.children).some((k) => k !== ONLY_WILDCARD_REG_EXP_STR && k !== TAIL_WILDCARD_REG_EXP_STR)) {
          throw PATH_ERROR;
        }
        if (pathErrorCheckOnly) {
          return;
        }
        node = this.children[regexpStr] = new Node;
        if (name !== "") {
          node.varIndex = context3.varIndex++;
        }
      }
      if (!pathErrorCheckOnly && name !== "") {
        paramMap.push([name, node.varIndex]);
      }
    } else {
      node = this.children[token];
      if (!node) {
        if (Object.keys(this.children).some((k) => k.length > 1 && k !== ONLY_WILDCARD_REG_EXP_STR && k !== TAIL_WILDCARD_REG_EXP_STR)) {
          throw PATH_ERROR;
        }
        if (pathErrorCheckOnly) {
          return;
        }
        node = this.children[token] = new Node;
      }
    }
    node.insert(restTokens, index, paramMap, context3, pathErrorCheckOnly);
  }
  buildRegExpStr() {
    const childKeys = Object.keys(this.children).sort(compareKey);
    const strList = childKeys.map((k) => {
      const c = this.children[k];
      return (typeof c.varIndex === "number" ? `(${k})@${c.varIndex}` : k) + c.buildRegExpStr();
    });
    if (typeof this.index === "number") {
      strList.unshift(`#${this.index}`);
    }
    if (strList.length === 0) {
      return "";
    }
    if (strList.length === 1) {
      return strList[0];
    }
    return "(?:" + strList.join("|") + ")";
  }
};

// node_modules/hono/dist/router/reg-exp-router/trie.js
var Trie = class {
  constructor() {
    this.context = { varIndex: 0 };
    this.root = new Node;
  }
  insert(path, index, pathErrorCheckOnly) {
    const paramAssoc = [];
    const groups = [];
    for (let i = 0;; ) {
      let replaced = false;
      path = path.replace(/\{[^}]+\}/g, (m) => {
        const mark = `@\\${i}`;
        groups[i] = [mark, m];
        i++;
        replaced = true;
        return mark;
      });
      if (!replaced) {
        break;
      }
    }
    const tokens = path.match(/(?::[^\/]+)|(?:\/\*$)|./g) || [];
    for (let i = groups.length - 1;i >= 0; i--) {
      const [mark] = groups[i];
      for (let j = tokens.length - 1;j >= 0; j--) {
        if (tokens[j].indexOf(mark) !== -1) {
          tokens[j] = tokens[j].replace(mark, groups[i][1]);
          break;
        }
      }
    }
    this.root.insert(tokens, index, paramAssoc, this.context, pathErrorCheckOnly);
    return paramAssoc;
  }
  buildRegExp() {
    let regexp = this.root.buildRegExpStr();
    if (regexp === "") {
      return [/^$/, [], []];
    }
    let captureIndex = 0;
    const indexReplacementMap = [];
    const paramReplacementMap = [];
    regexp = regexp.replace(/#(\d+)|@(\d+)|\.\*\$/g, (_, handlerIndex, paramIndex) => {
      if (typeof handlerIndex !== "undefined") {
        indexReplacementMap[++captureIndex] = Number(handlerIndex);
        return "$()";
      }
      if (typeof paramIndex !== "undefined") {
        paramReplacementMap[Number(paramIndex)] = ++captureIndex;
        return "";
      }
      return "";
    });
    return [new RegExp(`^${regexp}`), indexReplacementMap, paramReplacementMap];
  }
};

// node_modules/hono/dist/router/reg-exp-router/router.js
var buildWildcardRegExp = function(path) {
  return wildcardRegExpCache[path] ?? (wildcardRegExpCache[path] = new RegExp(path === "*" ? "" : `^${path.replace(/\/\*/, "(?:|/.*)")}\$`));
};
var clearWildcardRegExpCache = function() {
  wildcardRegExpCache = {};
};
var buildMatcherFromPreprocessedRoutes = function(routes) {
  const trie2 = new Trie;
  const handlerData = [];
  if (routes.length === 0) {
    return nullMatcher;
  }
  const routesWithStaticPathFlag = routes.map((route) => [!/\*|\/:/.test(route[0]), ...route]).sort(([isStaticA, pathA], [isStaticB, pathB]) => isStaticA ? 1 : isStaticB ? -1 : pathA.length - pathB.length);
  const staticMap = {};
  for (let i = 0, j = -1, len = routesWithStaticPathFlag.length;i < len; i++) {
    const [pathErrorCheckOnly, path, handlers] = routesWithStaticPathFlag[i];
    if (pathErrorCheckOnly) {
      staticMap[path] = [handlers.map(([h]) => [h, {}]), emptyParam];
    } else {
      j++;
    }
    let paramAssoc;
    try {
      paramAssoc = trie2.insert(path, j, pathErrorCheckOnly);
    } catch (e) {
      throw e === PATH_ERROR ? new UnsupportedPathError(path) : e;
    }
    if (pathErrorCheckOnly) {
      continue;
    }
    handlerData[j] = handlers.map(([h, paramCount]) => {
      const paramIndexMap = {};
      paramCount -= 1;
      for (;paramCount >= 0; paramCount--) {
        const [key, value] = paramAssoc[paramCount];
        paramIndexMap[key] = value;
      }
      return [h, paramIndexMap];
    });
  }
  const [regexp, indexReplacementMap, paramReplacementMap] = trie2.buildRegExp();
  for (let i = 0, len = handlerData.length;i < len; i++) {
    for (let j = 0, len2 = handlerData[i].length;j < len2; j++) {
      const map = handlerData[i][j]?.[1];
      if (!map) {
        continue;
      }
      const keys = Object.keys(map);
      for (let k = 0, len3 = keys.length;k < len3; k++) {
        map[keys[k]] = paramReplacementMap[map[keys[k]]];
      }
    }
  }
  const handlerMap = [];
  for (const i in indexReplacementMap) {
    handlerMap[i] = handlerData[indexReplacementMap[i]];
  }
  return [regexp, handlerMap, staticMap];
};
var findMiddleware = function(middleware, path) {
  if (!middleware) {
    return;
  }
  for (const k of Object.keys(middleware).sort((a, b) => b.length - a.length)) {
    if (buildWildcardRegExp(k).test(path)) {
      return [...middleware[k]];
    }
  }
  return;
};
var emptyParam = [];
var nullMatcher = [/^$/, [], {}];
var wildcardRegExpCache = {};
var RegExpRouter = class {
  constructor() {
    this.name = "RegExpRouter";
    this.middleware = { [METHOD_NAME_ALL]: {} };
    this.routes = { [METHOD_NAME_ALL]: {} };
  }
  add(method, path, handler) {
    var _a;
    const { middleware, routes } = this;
    if (!middleware || !routes) {
      throw new Error(MESSAGE_MATCHER_IS_ALREADY_BUILT);
    }
    if (!middleware[method]) {
      [middleware, routes].forEach((handlerMap) => {
        handlerMap[method] = {};
        Object.keys(handlerMap[METHOD_NAME_ALL]).forEach((p) => {
          handlerMap[method][p] = [...handlerMap[METHOD_NAME_ALL][p]];
        });
      });
    }
    if (path === "/*") {
      path = "*";
    }
    const paramCount = (path.match(/\/:/g) || []).length;
    if (/\*$/.test(path)) {
      const re = buildWildcardRegExp(path);
      if (method === METHOD_NAME_ALL) {
        Object.keys(middleware).forEach((m) => {
          var _a2;
          (_a2 = middleware[m])[path] || (_a2[path] = findMiddleware(middleware[m], path) || findMiddleware(middleware[METHOD_NAME_ALL], path) || []);
        });
      } else {
        (_a = middleware[method])[path] || (_a[path] = findMiddleware(middleware[method], path) || findMiddleware(middleware[METHOD_NAME_ALL], path) || []);
      }
      Object.keys(middleware).forEach((m) => {
        if (method === METHOD_NAME_ALL || method === m) {
          Object.keys(middleware[m]).forEach((p) => {
            re.test(p) && middleware[m][p].push([handler, paramCount]);
          });
        }
      });
      Object.keys(routes).forEach((m) => {
        if (method === METHOD_NAME_ALL || method === m) {
          Object.keys(routes[m]).forEach((p) => re.test(p) && routes[m][p].push([handler, paramCount]));
        }
      });
      return;
    }
    const paths = checkOptionalParameter(path) || [path];
    for (let i = 0, len = paths.length;i < len; i++) {
      const path2 = paths[i];
      Object.keys(routes).forEach((m) => {
        var _a2;
        if (method === METHOD_NAME_ALL || method === m) {
          (_a2 = routes[m])[path2] || (_a2[path2] = [
            ...findMiddleware(middleware[m], path2) || findMiddleware(middleware[METHOD_NAME_ALL], path2) || []
          ]);
          routes[m][path2].push([handler, paramCount - len + i + 1]);
        }
      });
    }
  }
  match(method, path) {
    clearWildcardRegExpCache();
    const matchers = this.buildAllMatchers();
    this.match = (method2, path2) => {
      const matcher = matchers[method2] || matchers[METHOD_NAME_ALL];
      const staticMatch = matcher[2][path2];
      if (staticMatch) {
        return staticMatch;
      }
      const match = path2.match(matcher[0]);
      if (!match) {
        return [[], emptyParam];
      }
      const index = match.indexOf("", 1);
      return [matcher[1][index], match];
    };
    return this.match(method, path);
  }
  buildAllMatchers() {
    const matchers = {};
    [...Object.keys(this.routes), ...Object.keys(this.middleware)].forEach((method) => {
      matchers[method] || (matchers[method] = this.buildMatcher(method));
    });
    this.middleware = this.routes = undefined;
    return matchers;
  }
  buildMatcher(method) {
    const routes = [];
    let hasOwnRoute = method === METHOD_NAME_ALL;
    [this.middleware, this.routes].forEach((r) => {
      const ownRoute = r[method] ? Object.keys(r[method]).map((path) => [path, r[method][path]]) : [];
      if (ownRoute.length !== 0) {
        hasOwnRoute || (hasOwnRoute = true);
        routes.push(...ownRoute);
      } else if (method !== METHOD_NAME_ALL) {
        routes.push(...Object.keys(r[METHOD_NAME_ALL]).map((path) => [path, r[METHOD_NAME_ALL][path]]));
      }
    });
    if (!hasOwnRoute) {
      return null;
    } else {
      return buildMatcherFromPreprocessedRoutes(routes);
    }
  }
};

// node_modules/hono/dist/router/smart-router/router.js
var SmartRouter = class {
  constructor(init) {
    this.name = "SmartRouter";
    this.routers = [];
    this.routes = [];
    Object.assign(this, init);
  }
  add(method, path, handler) {
    if (!this.routes) {
      throw new Error(MESSAGE_MATCHER_IS_ALREADY_BUILT);
    }
    this.routes.push([method, path, handler]);
  }
  match(method, path) {
    if (!this.routes) {
      throw new Error("Fatal error");
    }
    const { routers, routes } = this;
    const len = routers.length;
    let i = 0;
    let res;
    for (;i < len; i++) {
      const router5 = routers[i];
      try {
        routes.forEach((args) => {
          router5.add(...args);
        });
        res = router5.match(method, path);
      } catch (e) {
        if (e instanceof UnsupportedPathError) {
          continue;
        }
        throw e;
      }
      this.match = router5.match.bind(router5);
      this.routers = [router5];
      this.routes = undefined;
      break;
    }
    if (i === len) {
      throw new Error("Fatal error");
    }
    this.name = `SmartRouter + ${this.activeRouter.name}`;
    return res;
  }
  get activeRouter() {
    if (this.routes || this.routers.length !== 1) {
      throw new Error("No active router has been determined yet.");
    }
    return this.routers[0];
  }
};

// node_modules/hono/dist/router/trie-router/node.js
var Node2 = class {
  constructor(method, handler, children) {
    this.order = 0;
    this.params = {};
    this.children = children || {};
    this.methods = [];
    this.name = "";
    if (method && handler) {
      const m = {};
      m[method] = { handler, possibleKeys: [], score: 0, name: this.name };
      this.methods = [m];
    }
    this.patterns = [];
  }
  insert(method, path, handler) {
    this.name = `${method} ${path}`;
    this.order = ++this.order;
    let curNode = this;
    const parts = splitRoutingPath(path);
    const possibleKeys = [];
    const parentPatterns = [];
    for (let i = 0, len = parts.length;i < len; i++) {
      const p = parts[i];
      if (Object.keys(curNode.children).includes(p)) {
        parentPatterns.push(...curNode.patterns);
        curNode = curNode.children[p];
        const pattern2 = getPattern(p);
        if (pattern2) {
          possibleKeys.push(pattern2[1]);
        }
        continue;
      }
      curNode.children[p] = new Node2;
      const pattern = getPattern(p);
      if (pattern) {
        curNode.patterns.push(pattern);
        parentPatterns.push(...curNode.patterns);
        possibleKeys.push(pattern[1]);
      }
      parentPatterns.push(...curNode.patterns);
      curNode = curNode.children[p];
    }
    if (!curNode.methods.length) {
      curNode.methods = [];
    }
    const m = {};
    const handlerSet = {
      handler,
      possibleKeys: possibleKeys.filter((v, i, a) => a.indexOf(v) === i),
      name: this.name,
      score: this.order
    };
    m[method] = handlerSet;
    curNode.methods.push(m);
    return curNode;
  }
  gHSets(node3, method, nodeParams, params) {
    const handlerSets = [];
    for (let i = 0, len = node3.methods.length;i < len; i++) {
      const m = node3.methods[i];
      const handlerSet = m[method] || m[METHOD_NAME_ALL];
      const processedSet = {};
      if (handlerSet !== undefined) {
        handlerSet.params = {};
        handlerSet.possibleKeys.forEach((key) => {
          const processed = processedSet[handlerSet.name];
          handlerSet.params[key] = params[key] && !processed ? params[key] : nodeParams[key] ?? params[key];
          processedSet[handlerSet.name] = true;
        });
        handlerSets.push(handlerSet);
      }
    }
    return handlerSets;
  }
  search(method, path) {
    const handlerSets = [];
    this.params = {};
    const curNode = this;
    let curNodes = [curNode];
    const parts = splitPath(path);
    for (let i = 0, len = parts.length;i < len; i++) {
      const part = parts[i];
      const isLast = i === len - 1;
      const tempNodes = [];
      for (let j = 0, len2 = curNodes.length;j < len2; j++) {
        const node3 = curNodes[j];
        const nextNode = node3.children[part];
        if (nextNode) {
          nextNode.params = node3.params;
          if (isLast === true) {
            if (nextNode.children["*"]) {
              handlerSets.push(...this.gHSets(nextNode.children["*"], method, node3.params, {}));
            }
            handlerSets.push(...this.gHSets(nextNode, method, node3.params, {}));
          } else {
            tempNodes.push(nextNode);
          }
        }
        for (let k = 0, len3 = node3.patterns.length;k < len3; k++) {
          const pattern = node3.patterns[k];
          const params = { ...node3.params };
          if (pattern === "*") {
            const astNode = node3.children["*"];
            if (astNode) {
              handlerSets.push(...this.gHSets(astNode, method, node3.params, {}));
              tempNodes.push(astNode);
            }
            continue;
          }
          if (part === "") {
            continue;
          }
          const [key, name, matcher] = pattern;
          const child = node3.children[key];
          const restPathString = parts.slice(i).join("/");
          if (matcher instanceof RegExp && matcher.test(restPathString)) {
            params[name] = restPathString;
            handlerSets.push(...this.gHSets(child, method, node3.params, params));
            continue;
          }
          if (matcher === true || matcher instanceof RegExp && matcher.test(part)) {
            if (typeof key === "string") {
              params[name] = part;
              if (isLast === true) {
                handlerSets.push(...this.gHSets(child, method, params, node3.params));
                if (child.children["*"]) {
                  handlerSets.push(...this.gHSets(child.children["*"], method, params, node3.params));
                }
              } else {
                child.params = params;
                tempNodes.push(child);
              }
            }
          }
        }
      }
      curNodes = tempNodes;
    }
    const results = handlerSets.sort((a, b) => {
      return a.score - b.score;
    });
    return [results.map(({ handler, params }) => [handler, params])];
  }
};

// node_modules/hono/dist/router/trie-router/router.js
var TrieRouter = class {
  constructor() {
    this.name = "TrieRouter";
    this.node = new Node2;
  }
  add(method, path, handler) {
    const results = checkOptionalParameter(path);
    if (results) {
      for (const p of results) {
        this.node.insert(method, p, handler);
      }
      return;
    }
    this.node.insert(method, path, handler);
  }
  match(method, path) {
    return this.node.search(method, path);
  }
};

// node_modules/hono/dist/hono.js
var Hono2 = class extends Hono {
  constructor(options = {}) {
    super(options);
    this.router = options.router ?? new SmartRouter({
      routers: [new RegExpRouter, new TrieRouter]
    });
  }
};

// src/common/logger.ts
var import_pino = __toESM(require_pino(), 1);
var import_kafkajs = __toESM(require_kafkajs(), 1);
var logger = import_pino.default({
  transport: {
    target: "pino-pretty",
    options: {
      colorize: true,
      translateTime: "HH:MM:ss"
    }
  }
});
var logLevelMap = {
  [import_kafkajs.logLevel.ERROR]: "error",
  [import_kafkajs.logLevel.WARN]: "warn",
  [import_kafkajs.logLevel.INFO]: "info",
  [import_kafkajs.logLevel.DEBUG]: "debug",
  [import_kafkajs.logLevel.NOTHING]: "silent"
};
var pinoKafkaLogger = () => {
  return ({ namespace, level, label, log }) => {
    const { message, retryCount, retryTime } = log || {};
    const pinoLevel = logLevelMap[level];
    const retryInfo = retryCount && retryTime ? ` | RetryCount: ${retryCount}, RetryTime: ${retryTime}` : "";
    if (pinoLevel) {
      logger[pinoLevel]({
        msg: "KafkaJS: " + message + retryInfo
      });
    }
  };
};
var logger_default = logger;

// src/common/environment.ts
var config2 = __toESM(require_config(), 1);
var accessPublicKey = await Bun.file("jwt_public_key.pem").text();
var accessPrivateKey = await Bun.file("jwt_key.pem").text();
var refreshPublicKey = await Bun.file("jwt_public_key2.pem").text();
var refreshPrivateKey = await Bun.file("jwt_key2.pem").text();
var config3 = {
  origin: "http://localhost:5000",
  port: "3000",
  kafkaBroker: "devdive.tech:9092"
};

// src/controllers/authController.ts
var config4 = __toESM(require_config(), 1);

// node_modules/hono/dist/utils/cookie.js
var _serialize = (name, value, opt = {}) => {
  let cookie = `${name}=${value}`;
  if (opt && typeof opt.maxAge === "number" && opt.maxAge >= 0) {
    cookie += `; Max-Age=${Math.floor(opt.maxAge)}`;
  }
  if (opt.domain) {
    cookie += `; Domain=${opt.domain}`;
  }
  if (opt.path) {
    cookie += `; Path=${opt.path}`;
  }
  if (opt.expires) {
    cookie += `; Expires=${opt.expires.toUTCString()}`;
  }
  if (opt.httpOnly) {
    cookie += "; HttpOnly";
  }
  if (opt.secure) {
    cookie += "; Secure";
  }
  if (opt.sameSite) {
    cookie += `; SameSite=${opt.sameSite}`;
  }
  if (opt.partitioned) {
    cookie += "; Partitioned";
  }
  return cookie;
};
var serialize = (name, value, opt = {}) => {
  value = encodeURIComponent(value);
  return _serialize(name, value, opt);
};

// node_modules/hono/dist/helper/cookie/index.js
var setCookie = (c, name, value, opt) => {
  const cookie2 = serialize(name, value, { path: "/", ...opt });
  c.header("set-cookie", cookie2, { append: true });
};

// src/kafka/kafkaProducer.ts
var import_kafkajs2 = __toESM(require_kafkajs(), 1);
var kafkaConfig = {
  clientId: "auth-service",
  brokers: [config3.kafkaBroker],
  logCreator: pinoKafkaLogger
};
var kafka = new import_kafkajs2.Kafka(kafkaConfig);
var producer = kafka.producer({ createPartitioner: import_kafkajs2.Partitioners.DefaultPartitioner });
try {
  await producer.connect();
} catch (error) {
  if (error instanceof import_kafkajs2.KafkaJSNumberOfRetriesExceeded) {
    logger_default.error("KafkaJS: Unable to connect to broker: Number of retries exceeded");
    process.exit(1);
  } else {
    logger_default.error(error);
    process.exit(1);
  }
}
var kafkaProducer_default = producer;

// src/controllers/authController.ts
var client = __toESM(require_default3(), 1);

// src/common/jwtWorkers.ts
var jwt = __toESM(require_jsonwebtoken(), 1);
var getAccessToken = async (user) => {
  return jwt.sign(user, accessPrivateKey, { algorithm: "RS256", expiresIn: "1h" });
};
var getRefreshToken = async (user) => {
  return jwt.sign(user, refreshPrivateKey, { algorithm: "RS256", expiresIn: "30d" });
};

// src/controllers/authController.ts
var prisma = new client.PrismaClient;
var loginUser = async (c) => {
  try {
    const body2 = await c.req.json();
    const user = await prisma.user.findUnique({
      where: { username: body2.username }
    });
    if (!user)
      return c.json({ message: "User Not Found" }, 404);
    const passwordValid = await Bun.password.verify(body2.password, user.password);
    if (passwordValid) {
      const accessToken = await getAccessToken({ id: user.id, username: user.username });
      const refreshToken = await getRefreshToken({ id: user.id, username: user.username });
      const token = await prisma.refreshToken.findUnique({
        where: {
          device: body2.deviceIdentifier
        }
      });
      if (token) {
        const userTokenRecord = await prisma.refreshToken.findUnique({
          where: {
            device: body2.deviceIdentifier,
            userId: user.id
          }
        });
        if (!userTokenRecord) {
          await prisma.refreshToken.delete({
            where: {
              device: body2.deviceIdentifier
            }
          });
          await prisma.refreshToken.create({
            data: {
              userId: user.id,
              token: refreshToken,
              device: body2.deviceIdentifier,
              expiresAt: new Date(new Date().setDate(new Date().getDate() + 30))
            }
          });
        } else {
          await prisma.refreshToken.update({
            where: {
              device: body2.deviceIdentifier,
              userId: user.id
            },
            data: {
              token: refreshToken
            }
          });
        }
      } else {
        await prisma.refreshToken.create({
          data: {
            userId: user.id,
            token: refreshToken,
            device: body2.deviceIdentifier,
            expiresAt: new Date(new Date().setDate(new Date().getDate() + 30))
          }
        });
      }
      setCookie(c, "refreshToken", refreshToken, {
        httpOnly: true,
        path: "/",
        maxAge: 2592000,
        domain: "localhost"
      });
      setCookie(c, "accessToken", accessToken, {
        httpOnly: true,
        path: "/",
        maxAge: 3600,
        domain: "localhost"
      });
      return c.json({
        message: "Success!"
      }, 200);
    } else {
      return c.json({ message: "Invalid Password" }, 401);
    }
  } catch (error) {
    console.error(error);
    return c.json({ message: "An error occurred during login" }, 500);
  }
};
var registerUser = async (c) => {
  try {
    const body2 = await c.req.json();
    const hashedPassword = await Bun.password.hash(body2.password, {
      algorithm: "argon2id",
      memoryCost: 4,
      timeCost: 3
    });
    const newUser = await prisma.user.create({
      data: {
        username: body2.username,
        password: hashedPassword
      },
      select: {
        id: true,
        username: true
      }
    });
    const accessToken = await getAccessToken(newUser);
    const refreshToken = await getRefreshToken(newUser);
    await prisma.refreshToken.create({
      data: {
        userId: newUser.id,
        token: refreshToken,
        device: body2.deviceIdentifier,
        expiresAt: new Date(new Date().setDate(new Date().getDate() + 30))
      }
    });
    await kafkaProducer_default.send({
      topic: "init-user",
      messages: [{ value: JSON.stringify(newUser) }]
    });
    setCookie(c, "refreshToken", refreshToken, {
      httpOnly: true,
      path: "/",
      maxAge: 2592000,
      domain: "localhost"
    });
    setCookie(c, "accessToken", accessToken, {
      httpOnly: true,
      path: "/",
      maxAge: 3600,
      domain: "localhost"
    });
    return c.json({
      message: "Success!"
    }, 200);
  } catch (error) {
    if (error instanceof client.Prisma.PrismaClientKnownRequestError && error.code === "P2002") {
      return c.json({ message: "Username already exists" }, 400);
    } else if (error instanceof Error) {
      console.error(error.message);
      return c.json({ message: "An error occurred during registration" }, 500);
    }
    console.error("An unknown error occurred during registration.");
    return c.json({ message: "An unknown error occurred during registration" }, 500);
  }
};

// src/routes/authRoutes.ts
var authRoutes = new Hono2;
authRoutes.post("/login", async (c) => {
  return await loginUser(c);
});
authRoutes.post("/signup", async (c) => {
  return await registerUser(c);
});
var authRoutes_default = authRoutes;

// src/controllers/tokenController.ts
var client2 = __toESM(require_default3(), 1);
var import_jsonwebtoken = __toESM(require_jsonwebtoken(), 1);
var prisma2 = new client2.PrismaClient;
var refreshTokens = async (c) => {
  const refreshToken = c.req.header("Authorization")?.split(" ")[1];
  if (!refreshToken)
    return c.json({ message: "Access Denied" }, 401);
  const refreshTokenRecord = await prisma2.refreshToken.findUnique({
    where: { token: refreshToken },
    select: {
      user: {
        select: {
          id: true,
          username: true
        }
      }
    }
  });
  const user = refreshTokenRecord ? refreshTokenRecord.user : null;
  if (!user)
    return c.json({ message: "Invalid Refresh Token" }, 401);
  try {
    const verified = import_jsonwebtoken.verify(refreshToken, refreshPublicKey, { algorithms: ["RS256"] });
    const accessToken = await getAccessToken(user);
    const newRefreshToken = await getRefreshToken(user);
    await prisma2.refreshToken.update({
      where: {
        token: refreshToken
      },
      data: {
        token: newRefreshToken
      }
    });
    return c.json({
      message: "Tokens successfully refreshed.",
      accessToken,
      refreshToken: newRefreshToken
    }, 200);
  } catch (error) {
    return c.json({ message: "Invalid Refresh Token" }, 403);
  }
};
var verifyAccessToken = async (c) => {
  const accessToken = c.req.header("Authorization")?.split(" ")[1];
  if (!accessToken)
    return c.json({ message: "Access Denied" }, 401);
  try {
    const verified = import_jsonwebtoken.verify(accessToken, accessPublicKey, { algorithms: ["RS256"] });
    return c.json({ message: "Authorised" }, 200);
  } catch (error) {
    if (error instanceof import_jsonwebtoken.TokenExpiredError)
      return c.json({ message: "Token Expired" }, 403);
    return c.json({ message: "Invalid Token" }, 401);
  }
};
var getPublicKey = async (c) => {
  return c.text(refreshPublicKey);
};

// src/routes/tokenRoutes.ts
var tokenRoutes = new Hono2;
tokenRoutes.post("/refresh", async (c) => {
  return await refreshTokens(c);
});
tokenRoutes.post("/verify", async (c) => {
  return await verifyAccessToken(c);
});
tokenRoutes.get("/key", async (c) => {
  return await getPublicKey(c);
});
var tokenRoutes_default = tokenRoutes;

// src/common/initUserProfile.ts
var client3 = __toESM(require_default3(), 1);
var prisma3 = new client3.PrismaClient;
var initUserProfile = async function(userJSON) {
  const user = JSON.parse(userJSON);
  if (typeof user.id == "number") {
    await prisma3.user.update({
      where: {
        id: user.id
      },
      data: {
        isUserProfileInitialised: true
      }
    });
  }
};
var initGameProfile = async function(userJSON) {
  const user = JSON.parse(userJSON);
  if (typeof user.id == "number") {
    await prisma3.user.update({
      where: {
        id: user.id
      },
      data: {
        isGameProfileInitialised: true
      }
    });
  }
};

// src/kafka/kafkaConsumer.ts
var import_kafkajs3 = __toESM(require_kafkajs(), 1);
async function runConsumer() {
  await consumer.run({
    eachMessage: async ({ topic, partition, message }) => {
      if (topic == "user-profile-created" && message.value) {
        await initUserProfile(message.value.toString());
      }
      if (topic == "game-profile-created" && message.value) {
        await initGameProfile(message.value.toString());
      }
    }
  });
  process.on("SIGTERM", async () => {
    await consumer.disconnect();
  });
}
var kafkaConfig2 = {
  clientId: "auth-service",
  brokers: [config3.kafkaBroker],
  logCreator: pinoKafkaLogger
};
var kafka2 = new import_kafkajs3.Kafka(kafkaConfig2);
var consumer = kafka2.consumer({ groupId: "auth-service-group" });
try {
  await consumer.connect();
} catch (error) {
  if (error instanceof import_kafkajs3.KafkaJSNumberOfRetriesExceeded) {
    logger_default.error("KafkaJS: Unable to connect to broker: Number of retries exceeded");
    process.exit(1);
  } else {
    logger_default.error(error);
    process.exit(1);
  }
}
await consumer.subscribe({ topic: "user-profile-created", fromBeginning: true });
await consumer.subscribe({ topic: "game-profile-created", fromBeginning: true });
var kafkaConsumer_default = runConsumer;

// node_modules/hono/dist/middleware/cors/index.js
var cors = (options) => {
  const defaults = {
    origin: "*",
    allowMethods: ["GET", "HEAD", "PUT", "POST", "DELETE", "PATCH"],
    allowHeaders: [],
    exposeHeaders: []
  };
  const opts = {
    ...defaults,
    ...options
  };
  const findAllowOrigin = ((optsOrigin) => {
    if (typeof optsOrigin === "string") {
      return () => optsOrigin;
    } else if (typeof optsOrigin === "function") {
      return optsOrigin;
    } else {
      return (origin) => optsOrigin.includes(origin) ? origin : optsOrigin[0];
    }
  })(opts.origin);
  return async function cors2(c, next) {
    function set(key, value) {
      c.res.headers.set(key, value);
    }
    const allowOrigin = findAllowOrigin(c.req.header("origin") || "");
    if (allowOrigin) {
      set("Access-Control-Allow-Origin", allowOrigin);
    }
    if (opts.origin !== "*") {
      set("Vary", "Origin");
    }
    if (opts.credentials) {
      set("Access-Control-Allow-Credentials", "true");
    }
    if (opts.exposeHeaders?.length) {
      set("Access-Control-Expose-Headers", opts.exposeHeaders.join(","));
    }
    if (c.req.method === "OPTIONS") {
      if (opts.maxAge != null) {
        set("Access-Control-Max-Age", opts.maxAge.toString());
      }
      if (opts.allowMethods?.length) {
        set("Access-Control-Allow-Methods", opts.allowMethods.join(","));
      }
      let headers = opts.allowHeaders;
      if (!headers?.length) {
        const requestHeaders = c.req.header("Access-Control-Request-Headers");
        if (requestHeaders) {
          headers = requestHeaders.split(/\s*,\s*/);
        }
      }
      if (headers?.length) {
        set("Access-Control-Allow-Headers", headers.join(","));
        c.res.headers.append("Vary", "Access-Control-Request-Headers");
      }
      c.res.headers.delete("Content-Length");
      c.res.headers.delete("Content-Type");
      return new Response(null, {
        headers: c.res.headers,
        status: 204,
        statusText: c.res.statusText
      });
    }
    await next();
  };
};

// src/middleware/corsMiddleware.ts
var corsMiddleware = cors({
  origin: config3.origin,
  credentials: true
});
var corsMiddleware_default = corsMiddleware;

// src/middleware/errorMiddleware.ts
var errorMiddleware = async (c, next) => {
  try {
    await next();
  } catch (err) {
    const status = err.status || 500;
    const message = err.message || "Internal Server Error";
    logger_default.error(err);
    return c.json({ message }, status);
  }
};
var errorMiddleware_default = errorMiddleware;

// src/app.ts
var app = new Hono2;
app.use(corsMiddleware_default);
app.use("*", errorMiddleware_default);
app.route("/auth", authRoutes_default);
app.route("/token", tokenRoutes_default);
app.get("/health", (c) => c.text("OK"));
var startServer = async () => {
  try {
    await kafkaConsumer_default();
    logger_default.info("Kafka consumer initialized successfully");
    Bun.serve({
      port: config3.port,
      fetch: app.fetch
    });
    logger_default.info(`Server listening on port ${config3.port}`);
  } catch (error) {
    logger_default.error("Failed to initialize Kafka consumer:", error);
  }
};
await startServer();
